{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f533ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27dc182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "# parent_path =  path.home + \"/Workspace/WPI/Summer2023/ws/duc_repo/src/panda_test/\" + \"data/sim_marker/\"\n",
    "parent_path = path.home + \"/lama/results/09_04_2023/\"\n",
    "\n",
    "# root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "root_dir = parent_path + \"rcnn\" + \"/\"\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fucntion tranforms an input image for diverseifying data for training\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba3144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to split the dataset into train, test and validation folder.\n",
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "#         self.imgs_files = [file for file in sorted(os.listdir(root)) if file.endswith(\".jpg\")]\n",
    "#         self.annotations_files = [file for file in sorted(os.listdir(root)) if file.endswith(\".json\")]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         img_path = os.path.join(self.root, self.imgs_files[idx])\n",
    "#         annotations_path = os.path.join(self.root, self.annotations_files[idx])\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "#             bboxes_original = data['bboxes'][:6]\n",
    "            bboxes_original = data['bboxes']\n",
    "#             print(\"bounding boxes\", bboxes_original)\n",
    "#             keypoints_original = data['keypoints'][:6]\n",
    "            keypoints_original = data['keypoints']\n",
    "#             print(\"original keypoints\", np.array(keypoints_original))\n",
    "#             print(\"original keypoints shape\", (np.array(keypoints_original)).shape)\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            # bboxes_labels_original.append('joint5')\n",
    "            # bboxes_labels_original.append('joint6') \n",
    "#             bboxes_labels_original.append('joint7')\n",
    "#             bboxes_labels_original.append('joint8')\n",
    "#             bboxes_labels_original.append('panda_finger_1')\n",
    "#             bboxes_labels_original.append('panda_finger_2')\n",
    "            \n",
    "#         print(bboxes_original)\n",
    "#         print(bboxes_labels_original)\n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "#                     print(\"kp index\", k_idx)\n",
    "#                     print(\"key points\",kp)\n",
    "#                     print(\"keypoints original second iter\", [keypoints_original[0][o_idx][k_idx]],\n",
    "#                           [keypoints_original[o_idx][k_idx][0]], [keypoints_original[o_idx][k_idx][1]], \\\n",
    "#                          [keypoints_original[o_idx][k_idx][2]], [keypoints_original[o_idx][k_idx][3]])\n",
    "                    # kp - coordinates of keypoint\n",
    "                    # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "#             print(keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4]\n",
    "        # labels = [1, 2, 3, 4, 5, 6]\n",
    "#         labels = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "#         labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) \n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" \n",
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "# print(batch[2])\n",
    "\n",
    "# print(\"Original targets:\\n\", batch[3], \"\\n\\n\")\n",
    "# print(\"Transformed targets:\\n\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaeafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize how the transformed data looks \n",
    "\n",
    "keypoints_classes_ids2names = {0: 'base_joint', 1: 'joint2', 2: 'joint3', 3: 'joint4', 4: 'joint5', 5: 'joint6',\\\n",
    "                              6:'joint7', 7:'joint8', 8:'panda_finger_1', 9:'panda_finger_2'}\n",
    "\n",
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):\n",
    "    fontsize = 18\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "    \n",
    "    for idx, kps in enumerate(keypoints):\n",
    "        for kp in kps:\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 2, (255,0,0), 10)\n",
    "#         image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(40,40))\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "    else:\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)\n",
    "        \n",
    "        print(keypoints_original)\n",
    "        for idx, kps in enumerate(keypoints_original):\n",
    "            print(idx)\n",
    "            print(kps)\n",
    "            for kp in kps:\n",
    "                print(kp)\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 5, (255,0,0), 2)\n",
    "#             image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "\n",
    "        ax[0].imshow(image_original)\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "\n",
    "        ax[1].imshow(image)\n",
    "        ax[1].set_title('Transformed image', fontsize=fontsize)\n",
    "        \n",
    "        return None\n",
    "        \n",
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "# for kps in batch1[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "#     keypoints.append([kp[:2] for kp in [kps]])\n",
    "    \n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "# for kps in batch1[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "#     keypoints_original.append([kp[:2] for kp in [kps]])\n",
    "    \n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])\n",
    "\n",
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ad130",
   "metadata": {},
   "outputs": [],
   "source": [
    "### made changes in the path ../.local/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\n",
    "\n",
    "'''def normalize_bbox(bbox: TBox, rows: int, cols: int) -> TBox:\n",
    "    \"\"\"Normalize coordinates of a bounding box. Divide x-coordinates by image width and y-coordinates\n",
    "    by image height.\n",
    "\n",
    "    Args:\n",
    "        bbox: Denormalized bounding box `(x_min, y_min, x_max, y_max)`.\n",
    "        rows: Image height.\n",
    "        cols: Image width.\n",
    "\n",
    "    Returns:\n",
    "        Normalized bounding box `(x_min, y_min, x_max, y_max)`.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If rows or cols is less or equal zero\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if rows <= 0:\n",
    "        raise ValueError(\"Argument rows must be positive integer\")\n",
    "    if cols <= 0:\n",
    "        raise ValueError(\"Argument cols must be positive integer\")\n",
    "\n",
    "    tail: Tuple[Any, ...]\n",
    "    (x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])\n",
    "    \n",
    "    #x_min, x_max = x_min / cols, x_max / cols\n",
    "    #y_min, y_max = y_min / rows, y_max / rows\n",
    "    # next lines added by Jani\n",
    "    x_min = max(0, x_min)\n",
    "    y_min = max(0, y_min)\n",
    "    x_max = min(cols - 1, x_max)\n",
    "    y_max = min(rows - 1, y_max)\n",
    "    \n",
    "    return cast(BoxType, (x_min/cols, y_min/rows, x_max/cols, y_max/rows) + tail)  # type: ignore\n",
    "'''\n",
    "\n",
    "### made changes in the path ../.local/lib/python3.8/site-packages/albumentations/core/keypoint_utils.py\n",
    "'''def check_keypoint(kp, rows, cols):\n",
    "    for name, value, size in zip([\"x\", \"y\"], kp[:2], [cols, rows]):\n",
    "        value = min(max(0.0, value), size - 1)\n",
    "        kp = tuple([value if i == idx else x for i, x in enumerate(kp)])\n",
    "        if not 0 <= value < size:\n",
    "            raise ValueError(\n",
    "                \"Expected {name} for keypoint {kp} \"\n",
    "                \"to be in the range [0.0, {size}], got {value}.\".format(kp=kp, name=name, value=value, size=size)\n",
    "            )\n",
    "    angle = kp[2]\n",
    "    if not (0 <= angle < 2 * math.pi):\n",
    "        raise ValueError(\n",
    "            \"Expected angle for keypoint {kp} to be in the range [0.0, 2 * pi], got {angle}.\".format(kp=kp, angle=angle)\n",
    "        )\n",
    "    return kp\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(weights=False,\n",
    "                                                                   weights_backbone=True,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 5, # Background is the first class, object is the second class\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_keypoints = 4\n",
    "model = get_model(num_keypoints, weights_path=None)\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bff46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_keypoints = 4\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "# dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = ClassDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "batch_sizes = [1,2,3]\n",
    "epochs_lst = [25,30,50]\n",
    "\n",
    "# batch_sizes = [3, 2, 1]\n",
    "# epochs_lst = [25, 30]\n",
    "# batch_sizes = [3]\n",
    "# epochs_lst = [50]\n",
    "\n",
    "v = 1\n",
    "\n",
    "for b_size in batch_sizes:\n",
    "    for epochs in epochs_lst:\n",
    "        data_loader_train = DataLoader(dataset_train, batch_size=b_size, shuffle=True, collate_fn=collate_fn)\n",
    "        data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "        data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        model = get_model(num_keypoints = total_keypoints)\n",
    "        model.to(device)\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.9, weight_decay=0.0005)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "        num_epochs = epochs\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=1000)\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "#             if epoch == 25 or epoch == 30 :\n",
    "#                 PATH = f\"/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_ld_b{b_size}_e{epoch}_v{v}.pth\"        \n",
    "#                 torch.save(model, PATH)\n",
    "#                 v+=1 \n",
    "            \n",
    "        \n",
    "        PATH = f\"/home/jc-merlab/lama/results/09_04_2023/trained_models/keypointsrcnn_weights_origami_b{b_size}_e{epochs}_v{v}.pth\"\n",
    "            \n",
    "            \n",
    "        torch.save(model, PATH)\n",
    "        \n",
    "           \n",
    "        \n",
    "\n",
    "\n",
    "#     evaluate(model, data_loader_val, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights after training\n",
    "# torch.save(model.state_dict(), 'keypointsrcnn_weights_120.pth')\n",
    "# torch.save(model, '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_ld_b1_e30_v3.pth')\n",
    "            \n",
    "# torch.save(model, PATH)\n",
    "PATH = os.path.join(parent_path, f\"trained_models/keypointsrcnn_weights_ld_b{b_size}_e{epochs}_v{v}.pth\")\n",
    "# PATH = os.path.join(parent_path, f\"trained_models/keypointsrcnn_weights_ld_b{1}_e{25}_v{'origami'}.pth\")            \n",
    "            \n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e5f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_VAL = parent_path + \"split_folder_output-2023-07-14\" +\"/val\"\n",
    "dataset_val = ClassDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "iterator = iter(data_loader_val)\n",
    "len(data_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, data_loader_val, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fad317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_path = '/home/user/Workspace/WPI/Summer2023/ws/duc_repo/src/panda_test/data/trained_models/keypointsrcnn_weights_ld_b1_e25_v9.pth'\n",
    "weights_path = PATH\n",
    "model = torch.load(weights_path).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = iter(data_loader_val)\n",
    "i = 1\n",
    "while True:\n",
    "    try:\n",
    "        images, targets = next(data_iterator)\n",
    "        images = list(img.to(device) for img in images)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            images = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "            scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "            high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "            post_nms_idxs = torchvision.ops.nms(outputs[0]['boxes'][high_scores_idxs], outputs[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "            keypoints = []\n",
    "            for kps in outputs[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "                keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "            bboxes = []\n",
    "            for bbox in outputs[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "                bboxes.append(list(map(int, bbox.tolist())))\n",
    "            img = visualize(images, bboxes, keypoints)\n",
    "#             cv2.imwrite(\"/home/jc-merlab/Pictures/Data/video_results_01/out_image_\" + str(i) + \".jpg\", img)\n",
    "            cv2.imshow(f'image{i}', img)\n",
    "\n",
    "            cv2.waitKey(2000)\n",
    "            cv2.destroyWindow(f'image{i}')\n",
    "\n",
    "            \n",
    "            i = i+1\n",
    "\n",
    "            # Calculate loss and metrics for evaluation here.\n",
    "    except StopIteration:\n",
    "        break         \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_path = '/home/user/Workspace/WPI/Summer2023/ws/duc_repo/src/panda_test/data/trained_models/keypointsrcnn_weights_ld_b1_e25_v1.pth'\n",
    "weights_path = PATH\n",
    "model = torch.load(weights_path).to(device)\n",
    "\n",
    "images1, targets1 = next(iterator)\n",
    "images2, targets2 = next(iterator)\n",
    "images3, targets3 = next(iterator)\n",
    "images4, targets4 = next(iterator)\n",
    "# images, targets = next(iterator)\n",
    "\n",
    "print(type(images1))\n",
    "\n",
    "\n",
    "images1 = list(image1.to(device) for image1 in images1)\n",
    "images2 = list(image2.to(device) for image2 in images2)\n",
    "images3 = list(image3.to(device) for image3 in images3)\n",
    "images4 = list(image4.to(device) for image4 in images4)\n",
    "# images = list(image.to(device) for image in images)\n",
    "\n",
    "print(type(images1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output1 = model(images1)\n",
    "    output2 = model(images2)\n",
    "    output3 = model(images3)\n",
    "    output4 = model(images4)\n",
    "#     output = model(image)\n",
    "\n",
    "# print(\"Predictions: \\n\", output1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2300e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions: \\n\", output1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca000731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, targets) in enumerate(data_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_ld_b1_e25_v2.pth'\n",
    "model = torch.load(weights_path).to(device)\n",
    "# model = get_model(num_keypoints=6, weights_path=weights_path)\n",
    "# model.load_state_dict(torch.load('keypointsrcnn_weights.pth'))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# print(type(model))\n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "cap = cv2.VideoCapture('/home/jc-merlab/Pictures/Data/inference_data/test_video_3d.avi')\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "i = 0\n",
    "print(type(i))\n",
    "while(cap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "    print(i)\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:        \n",
    "#         img = cv2.imread(frame)\n",
    "        image = Image.fromarray(frame)\n",
    "\n",
    "        image = F.to_tensor(image).to(device)\n",
    "        image.unsqueeze_(0)\n",
    "        image = list(image)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            start = time.time(now)\n",
    "            output = model(image)\n",
    "            stop = time.time(now)\n",
    "            print(\"time\", (stop - start))\n",
    "\n",
    "        image = (image[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "        # Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "        # Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "        # Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "        keypoints = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "        bboxes = []\n",
    "        for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            bboxes.append(list(map(int, bbox.tolist())))\n",
    "        img = visualize(image, bboxes, keypoints)\n",
    "        \n",
    "        cv2.imwrite(\"/home/jc-merlab/Pictures/Data/video_results_01/out_image_\" + str(i) + \".jpg\", img)\n",
    "    \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "    i = i+1\n",
    "    \n",
    "cap.release()\n",
    " \n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc57ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = (images1[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores1 = output1[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores1 > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output1[0]['boxes'][high_scores_idxs], output1[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output1[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "    \n",
    "print(keypoints)\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output1[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "image = visualize(image1, bboxes, keypoints)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b19363",
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = (images2[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores = output2[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output2[0]['boxes'][high_scores_idxs], output2[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output2[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output2[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image2, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image3 = (images3[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores3 = output3[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output3[0]['boxes'][high_scores_idxs], output3[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output3[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output3[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image3, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image4 = (images4[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores4 = output4[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output4[0]['boxes'][high_scores_idxs], output4[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output4[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output4[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image4, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"/home/jc-merlab/lama/results/08_14_2023/test_set/2023-08-14_13_55_39/13.jpg\")\n",
    "print(type(image))\n",
    "\n",
    "image = F.to_tensor(image).to(device)\n",
    "image.unsqueeze_(0)\n",
    "print(image.shape)\n",
    "image = list(image)\n",
    "# print(type(images))\n",
    "# images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(image)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (image[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.1)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077377c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "weights_path = 'keypointsrcnn_weights.pth'\n",
    "model = get_model(num_keypoints=6, weights_path=weights_path)\n",
    "model.load_state_dict(torch.load('keypointsrcnn_weights.pth'))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# print(type(model))\n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "cap = cv2.VideoCapture('/home/jc-merlab/nov1_v1.avi')\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "i = 0\n",
    "print(type(i))\n",
    "while(cap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "    print(i)\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:        \n",
    "#         img = cv2.imread(frame)\n",
    "        image = Image.fromarray(frame)\n",
    "\n",
    "        image = F.to_tensor(image).to(device)\n",
    "        image.unsqueeze_(0)\n",
    "        image = list(image)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            output = model(image)\n",
    "\n",
    "        image = (image[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "        # Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "        # Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "        # Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "        keypoints = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "        bboxes = []\n",
    "        for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            bboxes.append(list(map(int, bbox.tolist())))\n",
    "        img = visualize(image, bboxes, keypoints)\n",
    "        \n",
    "        cv2.imwrite(\"/home/jc-merlab/Pictures/Data/video_results/out_image_\" + str(i) + \".jpg\", img)\n",
    "        \n",
    "    i = i+1\n",
    "    \n",
    "cap.release()\n",
    " \n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99134e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import moviepy.video.io.ImageSequenceClip\n",
    "image_folder=\"/home/jc-merlab/Pictures/Data/video_results/\"\n",
    "\n",
    "fps=1\n",
    "\n",
    "image_files = [os.path.join(image_folder,img)\n",
    "               for img in os.listdir(image_folder)\n",
    "               if img.endswith(\".jpg\")]\n",
    "clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(image_files, fps=fps)\n",
    "clip.write_videofile('my_video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4af23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "2500//72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1070c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

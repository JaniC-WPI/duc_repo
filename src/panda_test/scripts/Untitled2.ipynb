{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df872746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch_geometric.nn as pyg\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "_EPS = 1e-10\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Two-layer fully-connected ELU net with batch norm.\"\"\"\n",
    "    def __init__(self, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        self.dropout_prob = do_prob\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "    def batch_norm(self, inputs):\n",
    "        x = inputs.view(inputs.size(0) * inputs.size(1), -1)\n",
    "        x = self.bn(x)\n",
    "        return x.view(inputs.size(0), inputs.size(1), -1)\n",
    "    def forward(self, inputs):\n",
    "        # Flatten the last two dimensions for the linear layer input\n",
    "        x = func.elu(self.fc1(inputs))\n",
    "        x = func.dropout(x, self.dropout_prob, training=self.training)\n",
    "        x = func.elu(self.fc2(x))        \n",
    "        return self.batch_norm(x)\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_out=4, do_prob=0., factor=True):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.mlp1 = MLP(n_in, n_hid, n_hid, do_prob)\n",
    "        self.mlp2 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "        self.mlp3 = MLP(n_hid, n_hid, n_hid, do_prob)\n",
    "        if self.factor:\n",
    "            self.mlp4 = MLP(n_hid * 3, n_hid, n_hid, do_prob)\n",
    "            print(\"Using factor graph MLP encoder.\")\n",
    "        else:\n",
    "            self.mlp4 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "            print(\"mlp4\", self.mlp4)\n",
    "            print(\"Using MLP graph encoder.\")\n",
    "        self.fc_out = nn.Linear(n_hid, n_out)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "    def edge2node(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        incoming = torch.matmul(rel_rec.t(), x)\n",
    "        return incoming / incoming.size(1)\n",
    "    def node2edge(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        receivers = torch.matmul(rel_rec, x)\n",
    "        senders = torch.matmul(rel_send, x)\n",
    "        edges = torch.cat([receivers, senders], dim=2)\n",
    "        return edges\n",
    "    def forward(self, inputs, rel_rec, rel_send):\n",
    "        # Input shape: [num_sims, num_atoms, num_timesteps, num_dims]\n",
    "        x = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "        # New shape: [num_sims, num_atoms, num_timesteps*num_dims]\n",
    "        x = self.mlp1(x)  # 2-layer ELU net per node\n",
    "        x = self.node2edge(x, rel_rec, rel_send)\n",
    "        x = self.mlp2(x)\n",
    "        x_skip = x            \n",
    "        if self.factor:\n",
    "            x = self.edge2node(x, rel_rec, rel_send)\n",
    "            x = self.mlp3(x)\n",
    "            x = self.node2edge(x, rel_rec, rel_send)\n",
    "            x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "            x = self.mlp4(x)\n",
    "        else:\n",
    "            x = self.mlp3(x)\n",
    "            x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "            x = self.mlp4(x)\n",
    "        return self.fc_out(x)    \n",
    "Part2 of the Code:  \n",
    "class GraphDecoder(nn.Module):\n",
    "    def __init__(self, n_in_node, edge_types, msg_hid, msg_out, n_hid,\n",
    "                 do_prob=0., skip_first=False):\n",
    "        super(GraphDecoder, self).__init__()\n",
    "        self.msg_fc1 = nn.ModuleList(\n",
    "            [nn.Linear(2 * n_in_node, msg_hid) for _ in range(edge_types)])\n",
    "        self.msg_fc2 = nn.ModuleList(\n",
    "            [nn.Linear(msg_hid, msg_out) for _ in range(edge_types)])\n",
    "        self.msg_out_shape = msg_out\n",
    "        self.skip_first_edge_type = skip_first\n",
    "\n",
    "        self.out_fc1 = nn.Linear(n_in_node + msg_out, n_hid)\n",
    "        self.out_fc2 = nn.Linear(n_hid, n_hid)\n",
    "        self.out_fc3 = nn.Linear(n_hid, n_in_node)\n",
    "        print('Using learned graph decoder.')\n",
    "        self.dropout_prob = do_prob\n",
    "    def single_step_forward(self, single_timestep_inputs, rel_rec, rel_send,\n",
    "                            single_timestep_rel_type):\n",
    "        # Node2edge\n",
    "        receivers = torch.matmul(rel_rec, single_timestep_inputs)\n",
    "        senders = torch.matmul(rel_send, single_timestep_inputs)\n",
    "        pre_msg = torch.cat([receivers, senders], dim=-1)\n",
    "\n",
    "        all_msgs = Variable(torch.zeros(pre_msg.size(0), pre_msg.size(1),self.msg_out_shape))\n",
    "        if single_timestep_inputs.is_cuda:\n",
    "            all_msgs = all_msgs.cuda()\n",
    "\n",
    "        if self.skip_first_edge_type:\n",
    "            start_idx = 1\n",
    "        else:\n",
    "            start_idx = 0\n",
    "        # Run separate MLP for every edge type\n",
    "        # NOTE: To exlude one edge type, simply offset range by 1\n",
    "        for i in range(start_idx, len(self.msg_fc2)):\n",
    "            msg = func.relu(self.msg_fc1[i](pre_msg))\n",
    "            msg = func.dropout(msg, p=self.dropout_prob)\n",
    "            msg = func.relu(self.msg_fc2[i](msg))\n",
    "            msg = msg * single_timestep_rel_type[:, :, i:i + 1]\n",
    "            all_msgs += msg\n",
    "        # Aggregate all msgs to receiver\n",
    "        agg_msgs = all_msgs.transpose(-2, -1).matmul(rel_rec).transpose(-2, -1)\n",
    "        agg_msgs = agg_msgs.contiguous()\n",
    "        # Skip connection\n",
    "        aug_inputs = torch.cat([single_timestep_inputs, agg_msgs], dim=-1)\n",
    "        # Output MLP\n",
    "        pred = func.dropout(func.relu(self.out_fc1(aug_inputs)), p=self.dropout_prob)\n",
    "        pred = func.dropout(func.relu(self.out_fc2(pred)), p=self.dropout_prob)\n",
    "        pred = self.out_fc3(pred)\n",
    "        # Predict position/velocity difference\n",
    "        return single_timestep_inputs + pred\n",
    "    def forward(self, inputs, rel_type, rel_rec, rel_send, pred_steps=1):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        # Only take n-th timesteps as starting points (n: pred_steps)\n",
    "        last_pred = inputs[:, :, :]\n",
    "        curr_rel_type = rel_type[:, :, :]\n",
    "        preds=[]\n",
    "        # NOTE: Assumes rel_type is constant (i.e. same across all time steps).\n",
    "        # Run n prediction steps\n",
    "        #for step in range(0, pred_steps):\n",
    "        last_pred = self.single_step_forward(last_pred, rel_rec, rel_send,\n",
    "                                                 curr_rel_type)\n",
    "        preds.append(last_pred)\n",
    "        sizes = [preds[0].size(0), preds[0].size(1),\n",
    "                 preds[0].size(2)]\n",
    "        output = Variable(torch.zeros(sizes))\n",
    "        if inputs.is_cuda:\n",
    "            output = output.cuda()\n",
    "        # Re-assemble correct timeline\n",
    "        for i in range(len(preds)):\n",
    "            output[:, :, :] = preds[i]\n",
    "        pred_all = output[:, :, :]\n",
    "        # NOTE: We potentially over-predicted (stored in future_pred). Unused.\n",
    "        # future_pred = output[:, (inputs.size(1) - 1):, :, :]\n",
    "        return pred_all#.transpose(1, 2).contiguous()  \n",
    "    \n",
    "def my_softmax(input, axis=1):\n",
    "    trans_input = input.transpose(axis, 0).contiguous()\n",
    "    soft_max_1d = func.softmax(trans_input,dim=0)\n",
    "    return soft_max_1d.transpose(axis, 0)\n",
    "\n",
    "Part3 of the code:\n",
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path):\n",
    "        super(KeypointPipeline, self).__init__()  \n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.encoder = GraphEncoder(4,512,4,0.5,False)\n",
    "        self.decoder = GraphDecoder(n_in_node=4,\n",
    "                                 edge_types=2,\n",
    "                                 msg_hid=512,\n",
    "                                 msg_out=512,\n",
    "                                 n_hid=512,\n",
    "                                 do_prob=0.5,\n",
    "                                 skip_first=False)\n",
    "        \n",
    "        self.off_diag = np.ones([6,6]) - np.eye(6)\n",
    "        self.rel_rec = np.array(encode_onehot(np.where(self.off_diag)[1]), dtype=np.float32)\n",
    "        self.rel_send = np.array(encode_onehot(np.where(self.off_diag)[0]), dtype=np.float32)\n",
    "        self.rel_rec = torch.FloatTensor(self.rel_rec)\n",
    "        self.rel_send = torch.FloatTensor(self.rel_send)\n",
    "self.rel_rec = np.array(encode_onehot(np.where(self.off_diag)[1]), dtype=np.float32)\n",
    "        self.rel_send = np.array(encode_onehot(np.where(self.off_diag)[0]), dtype=np.float32)\n",
    "        self.rel_rec = torch.FloatTensor(self.rel_rec).to(device)\n",
    "        self.rel_send = torch.FloatTensor(self.rel_send).to(device)\n",
    "        self.encoder= self.encoder.cuda()\n",
    "        self.decoder = self.decoder.cuda()\n",
    "        self.rel_rec = self.rel_rec.cuda()\n",
    "        self.rel_send = self.rel_send.cuda()    \n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \n",
    "                                            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "        confidence = output[0]['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0,0:2])) + [confidence[idx]] + [labels[idx]])        \n",
    "        # Sort keypoints based on label\n",
    "        keypoints.sort(key=lambda x: x[-1])\n",
    "        return keypoints    \n",
    "    def keypoints_to_graph(self, keypoints, image_width, image_height):\n",
    "        # keypoints is expected to be a tensor with shape (num_keypoints, 4),\n",
    "        # where each keypoint is (x, y, score, label).\n",
    "        # Convert all elements in keypoints to tensors if they are not already\n",
    "        keypoints = [torch.tensor(kp, dtype=torch.float32).to(device) if not isinstance(kp, torch.Tensor) else kp for kp in keypoints]\n",
    "        # Then stack them\n",
    "        keypoints = torch.stack(keypoints).to(device)       \n",
    "        # Remove duplicates: Only keep the keypoint with the highest score for each label\n",
    "        unique_labels, best_keypoint_indices = torch.unique(keypoints[:, 3], return_inverse=True)\n",
    "        best_scores, best_indices = torch.max(keypoints[:, 2].unsqueeze(0) * (best_keypoint_indices == torch.arange(len(unique_labels)).unsqueeze(1).cuda()), dim=1)\n",
    "        keypoints = keypoints[best_indices]        \n",
    "        print(\"init keypoints in graph features\", keypoints)\n",
    "        # Normalize x and y to be in the range [-1, 1]\n",
    "        keypoints[:, 0] = (keypoints[:, 0] - image_width / 2) / (image_width / 2)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] - image_height / 2) / (image_height / 2)\n",
    "        # Use only x, y, and score for the graph features\n",
    "        graph_features = keypoints[:, :4]  # Now shape is (num_keypoints, 3)        \n",
    "        # Ensure the shape is [num_keypoints, 3] before returning\n",
    "        graph_features = graph_features.view(-1, 4)  # Reshape to ensure it's [num_keypoints, 3]\n",
    "#         print(\"graph features\", graph_features)\n",
    "        print(\"graph features shape\", graph_features.shape)\n",
    "        return graph_features   \n",
    "Part4 Continued from KeypointPipeline\n",
    "    def forward(self, imgs):\n",
    "        # Temporarily set the keypoint model to evaluation mode\n",
    "        keypoint_model_training = self.keypoint_model.training\n",
    "        self.keypoint_model.eval()\n",
    "        # Process each image in the batch\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = [self.keypoint_model(img.unsqueeze(0).to(device)) for img in imgs]\n",
    "        # Set the keypoint model back to its original training mode\n",
    "        self.keypoint_model.train(mode=keypoint_model_training)\n",
    "        # Process model outputs to get labeled keypoints\n",
    "        batch_labeled_keypoints = [self.process_model_output(output) for output in batch_outputs]\n",
    "        # Generate graph input tensor for each image and handle varying number of keypoints\n",
    "        batch_x = []\n",
    "        for labeled_keypoints in batch_labeled_keypoints:\n",
    "            keypoints = self.keypoints_to_graph(labeled_keypoints, 640, 480)\n",
    "            # Initialize x with zeros for 6 nodes with 4 features each\n",
    "            x = torch.zeros(1, 6, 4, device=device)\n",
    "            # Ensure that keypoints are on the correct device and fill in x\n",
    "            num_keypoints_detected = keypoints.size(0)\n",
    "            if num_keypoints_detected <= 6:\n",
    "                x[0, :num_keypoints_detected, :] = keypoints\n",
    "            else:\n",
    "                raise ValueError(\"Number of keypoints detected exceeds the maximum of 6.\")\n",
    "            batch_x.append(x)\n",
    "        # Stack the batch of x tensors for batch processing\n",
    "        batch_x = torch.cat(batch_x, dim=0)\n",
    "        # Forward pass through the encoder and decoder\n",
    "        logits = self.encoder(batch_x, self.rel_rec, self.rel_send)\n",
    "        edges = my_softmax(logits, -1)\n",
    "        KGNN2D = self.decoder(batch_x, edges, self.rel_rec, self.rel_send)\n",
    "        return logits, KGNN2D, batch_labeled_keypoints\n",
    "    \n",
    "def loss_edges(valid_points, edges):\n",
    "    off_diag = np.ones([6, 6]) - np.eye(6)\n",
    "    idx =  torch.LongTensor(np.where(off_diag)[1].reshape(6,5)).cuda()\n",
    "    if valid_points.ndim == 1:\n",
    "        valid_points = valid_points.unsqueeze(0)  # Reshape to 2D if necessary\n",
    "    relations = torch.zeros(valid_points.shape[0],valid_points.shape[1]*(valid_points.shape[1]-1)).cuda()\n",
    "    for count,vis in enumerate(valid_points):\n",
    "        vis = vis.view(-1,1) \n",
    "        vis = vis*vis.t()\n",
    "        vis = torch.gather(vis,1,idx)\n",
    "        relations[count] = vis.view(-1)\n",
    "    relations = relations.type(torch.LongTensor).cuda() \n",
    "    relations_expanded = relations.repeat_interleave(2)\n",
    "    loss_edges = func.cross_entropy(edges.view(-1, 2), relations_expanded.view(-1))\n",
    "    return loss_edges\n",
    "def nll_gaussian(preds, target, variance, add_const=False):\n",
    "    neg_log_p = ((preds - target) ** 2 / (2 * variance))\n",
    "    if add_const:\n",
    "        const = 0.5 * np.log(2 * np.pi * variance)\n",
    "        neg_log_p += const\n",
    "    return neg_log_p.sum() / (target.size(0) * target.size(1))\n",
    "def kgnn2d_loss(keypoints_gt, valid_points, keypoints_logits):\n",
    "    # Ensure data types are consistent and move tensors to the appropriate device\n",
    "    keypoints_gt = keypoints_gt.type(torch.FloatTensor).cuda()\n",
    "    keypoints_logits = keypoints_logits.type(torch.FloatTensor).cuda()\n",
    "    valid_points = valid_points.type(torch.FloatTensor).cuda()\n",
    "    keypoints_gt = keypoints_gt.type(torch.FloatTensor)*valid_points.unsqueeze(2).type(torch.FloatTensor)\n",
    "    keypoints_logits = keypoints_logits.type(torch.FloatTensor)*valid_points.unsqueeze(2).type(torch.FloatTensor)\n",
    "    keypoints_gt = keypoints_gt.cuda()\n",
    "    keypoints_logits = keypoints_logits.cuda()\n",
    "    loss_occ = nll_gaussian(keypoints_gt[:,:,0:2], keypoints_logits[:,:,0:2] , 0.1)\n",
    "    return loss_occ\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "Part5 of the code\n",
    "def process_batch_keypoints(target_dicts):\n",
    "    # This function now expects target_dicts, a list of dictionaries containing keypoints information\n",
    "    batch_size = len(target_dicts)\n",
    "    # Initialize lists to store keypoints and visibilities for each image in the batch\n",
    "    keypoints_list = []\n",
    "    visibilities_list = []\n",
    "    for dict_ in target_dicts:\n",
    "        # Each keypoints tensor in the dict is expected to have a shape [num_keypoints, 3]\n",
    "        keypoints = dict_['keypoints'].squeeze(1).to(device)\n",
    "        # Extract x, y coordinates and visibility flags\n",
    "        xy_coords = keypoints[:, :2]  # Keep only x, y coordinates\n",
    "        visibilities = keypoints[:, 2]  # Extract visibility flags\n",
    "        keypoints_list.append(xy_coords)\n",
    "        visibilities_list.append(visibilities)\n",
    "    # Concatenate keypoints and visibilities for the entire batch\n",
    "    # The final shape of keypoints_gt should be [batch_size, num_keypoints, 2]\n",
    "    keypoints_gt = torch.stack(keypoints_list).float().cuda()\n",
    "    visibilities = torch.stack(visibilities_list).cuda()\n",
    "    # Create valid visibility masks\n",
    "    valid_vis_all = (visibilities == 1).long().cuda()\n",
    "    valid_invis_all = (visibilities == 0).long().cuda()\n",
    "    return keypoints_gt, valid_vis_all, valid_invis_all\n",
    "def reorder_batch_keypoints(batch_keypoints):\n",
    "    # Assuming batch_keypoints is a tensor of shape [batch_size, num_keypoints, num_features]\n",
    "    batch_size, num_keypoints, num_features = batch_keypoints.shape\n",
    "    reordered_keypoints_batch = []\n",
    "    for i in range(batch_size):\n",
    "        # Directly use the normalized keypoints\n",
    "        normalized_keypoints = batch_keypoints[i]\n",
    "        # Initialize a tensor for reordered keypoints with only x, y coordinates\n",
    "        reordered_normalized_keypoints = torch.zeros(num_keypoints, 2, device=batch_keypoints.device)\n",
    "        # Reordering logic\n",
    "        rounded_labels = torch.round(normalized_keypoints[:, -1]).int()\n",
    "        used_indices = []\n",
    "        for label in range(1, 7):\n",
    "            valid_idx = (rounded_labels == label).nonzero(as_tuple=True)[0]\n",
    "            if valid_idx.numel() > 0:\n",
    "                reordered_normalized_keypoints[label - 1] = normalized_keypoints[valid_idx[0], :2]\n",
    "            else:\n",
    "                invalid_idx = ((rounded_labels < 1) | (rounded_labels > 6)).nonzero(as_tuple=True)[0]\n",
    "                invalid_idx = [idx for idx in invalid_idx if idx not in used_indices]\n",
    "                if invalid_idx:\n",
    "                    reordered_normalized_keypoints[label - 1] = normalized_keypoints[invalid_idx[0], :2]\n",
    "                    used_indices.append(invalid_idx[0])\n",
    "        reordered_keypoints_batch.append(reordered_normalized_keypoints)\n",
    "    return torch.stack(reordered_keypoints_batch)\n",
    "def denormalize_keypoints(batch_keypoints, width=640, height=480):\n",
    "    # Assuming batch_keypoints is a batch of normalized keypoints tensors\n",
    "    # Denormalize each keypoint in the batch\n",
    "    denormalized_keypoints = []\n",
    "    for kp in batch_keypoints:\n",
    "        denormalized_x = (kp[:, 0] * (width / 2)) + (width / 2)\n",
    "        denormalized_y = (kp[:, 1] * (height / 2)) + (height / 2)\n",
    "        denormalized_kp = torch.stack((denormalized_x, denormalized_y), dim=1)\n",
    "        denormalized_keypoints.append(denormalized_kp)        \n",
    "    denormalized_keypoints = torch.stack(denormalized_keypoints)\n",
    "    return denormalized_keypoints\n",
    "\n",
    "Part 6 of the code:\n",
    "# Define the model\n",
    "model = KeypointPipeline(weights_path)\n",
    "model = model.to(device)\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 2 # Define your number of epochs\n",
    "batch_size = 2\n",
    "split_folder_path = train_test_split(root_dir)\n",
    "KEYPOINTS_FOLDER_TRAIN = split_folder_path +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = split_folder_path +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = split_folder_path +\"/test\"\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "v = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, target_dicts, _ in data_loader_train:\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass for batch\n",
    "        logits, KGNN2D, batch_labeled_keypoints = model(imgs)\n",
    "        # Process keypoints for the entire batch\n",
    "        keypoints_gt, valid_vis_all, valid_invis_all = process_batch_keypoints(target_dicts)           \n",
    "        # Normalize and reorder keypoints as per your existing logic\n",
    "        # Ensure this logic works on the batch level        \n",
    "        reordered_normalized_keypoints = reorder_batch_keypoints(KGNN2D)\n",
    "        # Denormalize the reordered keypoints for the entire batch\n",
    "        denormalized_keypoints = denormalize_keypoints(reordered_normalized_keypoints)        \n",
    "        loss_kgnn2d = kgnn2d_loss(keypoints_gt, valid_invis_all, denormalized_keypoints)\n",
    "        # Compute batch losses\n",
    "        edge_loss = loss_edges(valid_vis_all, logits)\n",
    "        # Combine the losses\n",
    "        total_batch_loss = edge_loss + loss_kgnn2d\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += total_batch_loss.item()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59bdaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path):\n",
    "        super(KeypointPipeline, self).__init__()  \n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        # probably the autoencoder model will go in here \n",
    "       \n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \n",
    "                                            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "        confidence = output[0]['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0,0:2])) + [confidence[idx]] + [labels[idx]])        \n",
    "        # Sort keypoints based on label\n",
    "        keypoints.sort(key=lambda x: x[-1])\n",
    "        return keypoints    \n",
    "    def keypoints_to_graph(self, keypoints, image_width, image_height):\n",
    "        # keypoints is expected to be a tensor with shape (num_keypoints, 4),\n",
    "        # where each keypoint is (x, y, score, label).\n",
    "        # Convert all elements in keypoints to tensors if they are not already\n",
    "        keypoints = [torch.tensor(kp, dtype=torch.float32).to(device) if not isinstance(kp, torch.Tensor) else kp for kp in keypoints]\n",
    "        # Then stack them\n",
    "        keypoints = torch.stack(keypoints).to(device)       \n",
    "        # Remove duplicates: Only keep the keypoint with the highest score for each label\n",
    "        unique_labels, best_keypoint_indices = torch.unique(keypoints[:, 3], return_inverse=True)\n",
    "        best_scores, best_indices = torch.max(keypoints[:, 2].unsqueeze(0) * (best_keypoint_indices == torch.arange(len(unique_labels)).unsqueeze(1).cuda()), dim=1)\n",
    "        keypoints = keypoints[best_indices]        \n",
    "        print(\"init keypoints in graph features\", keypoints)\n",
    "        # Normalize x and y to be in the range [-1, 1]\n",
    "        keypoints[:, 0] = (keypoints[:, 0] - image_width / 2) / (image_width / 2)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] - image_height / 2) / (image_height / 2)\n",
    "        # Use only x, y, and score for the graph features\n",
    "        graph_features = keypoints[:, :4]  # Now shape is (num_keypoints, 3)        \n",
    "        # Ensure the shape is [num_keypoints, 3] before returning\n",
    "        graph_features = graph_features.view(-1, 4)  # Reshape to ensure it's [num_keypoints, 3]\n",
    "#         print(\"graph features\", graph_features)\n",
    "        print(\"graph features shape\", graph_features.shape)\n",
    "        return graph_features   \n",
    "    def forward(self, imgs):\n",
    "        # Temporarily set the keypoint model to evaluation mode\n",
    "        keypoint_model_training = self.keypoint_model.training\n",
    "        self.keypoint_model.eval()\n",
    "        # Process each image in the batch\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = [self.keypoint_model(img.unsqueeze(0).to(device)) for img in imgs]\n",
    "        # Set the keypoint model back to its original training mode\n",
    "        self.keypoint_model.train(mode=keypoint_model_training)\n",
    "        # Process model outputs to get labeled keypoints\n",
    "        batch_labeled_keypoints = [self.process_model_output(output) for output in batch_outputs]\n",
    "        # Generate graph input tensor for each image and handle varying number of keypoints\n",
    "        batch_x = []\n",
    "        for labeled_keypoints in batch_labeled_keypoints:\n",
    "            keypoints = self.keypoints_to_graph(labeled_keypoints, 640, 480)\n",
    "            # Initialize x with zeros for 6 nodes with 4 features each\n",
    "            x = torch.zeros(1, 6, 4, device=device)\n",
    "            # Ensure that keypoints are on the correct device and fill in x\n",
    "            num_keypoints_detected = keypoints.size(0)\n",
    "            if num_keypoints_detected <= 6:\n",
    "                x[0, :num_keypoints_detected, :] = keypoints\n",
    "            else:\n",
    "                raise ValueError(\"Number of keypoints detected exceeds the maximum of 6.\")\n",
    "            batch_x.append(x)\n",
    "        # Stack the batch of x tensors for batch processing\n",
    "        batch_x = torch.cat(batch_x, dim=0)\n",
    "        # Forward pass through the encoder and decoder\n",
    "        logits = self.encoder(batch_x, self.rel_rec, self.rel_send)\n",
    "        edges = my_softmax(logits, -1)\n",
    "        KGNN2D = self.decoder(batch_x, edges, self.rel_rec, self.rel_send)\n",
    "        return logits, KGNN2D, batch_labeled_keypoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

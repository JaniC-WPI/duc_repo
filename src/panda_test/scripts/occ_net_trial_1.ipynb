{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bedf1768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16908615680\n",
      "1006632960\n",
      "976208896\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b355d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "237a5cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2189cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aaae8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    print(type(output))\n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9395a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        print(img_original.shape)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd956dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model(num_keypoints, weights_path=None):\n",
    "    \n",
    "#     anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "#     model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "#                                                                    pretrained_backbone=True,\n",
    "#                                                                    num_keypoints=num_keypoints,\n",
    "#                                                                    num_classes = 7, # Background is the first class, object is the second class\n",
    "#                                                                    rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "#     if weights_path:\n",
    "# #         state_dict = torch.load(weights_path)\n",
    "# #         model.load_state_dict(state_dict)      \n",
    "#         model = torch.load(weights_path)\n",
    "        \n",
    "        \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1df0e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "# class GNNEncoder(nn.Module):\n",
    "#     def __init__(self, graph_edges, vertices_dim=3, edges_dim=1, hidden_dim=128):\n",
    "#         super(GNNEncoder, self).__init__()\n",
    "#         # Define your layers here, e.g., \n",
    "#         self.f_enc = nn.Linear(3, 128)  # Assuming 3 features per vertex as per the paper\n",
    "#         self.f_e1 = nn.Linear(256, 128)  # Concatenate two vertices\n",
    "#         self.f_v = nn.Linear(128, 128)  # Hidden layer for vertices\n",
    "#         self.f_e2 = nn.Linear(256, 128)  # Concatenate updated vertices\n",
    "\n",
    "#         # Define the graph edges\n",
    "#         self.graph_edges = torch.tensor([[1,2], [2,3], [3,4], [4,5], [5,6]]) - 1  # Subtract 1 for zero-based indexing\n",
    "#         self.graph_edges = torch.cat((self.graph_edges, torch.flip(self.graph_edges, [1])), dim=0)  # Make graph bidirectional\n",
    "\n",
    "#     def forward(self, vertices):\n",
    "#         h1 = self.f_enc(vertices)\n",
    "#         h1_expand_1 = h1[self.graph_edges[:,0]]\n",
    "#         h1_expand_2 = h1[self.graph_edges[:,1]]\n",
    "#         h_e1 = self.f_e1(torch.cat((h1_expand_1, h1_expand_2), dim=-1))  # Concatenate along the last dimension\n",
    "#         h2 = self.f_v(h_e1.sum(dim=0))  # e->v\n",
    "#         print(\"h2 shape:\", h2.shape)\n",
    "#         h2_prob = functional.log_softmax(h2, dim=-1) # vertices probability\n",
    "#         h2_expand_1 = h2[self.graph_edges[:,0]]\n",
    "#         h2_expand_2 = h2[self.graph_edges[:,1]]\n",
    "#         print(\"h1 shape:\", h1.shape)\n",
    "#         print(\"h1_expand_1 shape:\", h1_expand_1.shape)\n",
    "#         print(\"h1_expand_2 shape:\", h1_expand_2.shape)\n",
    "#         print(\"h_e1 shape:\", h_e1.shape)        \n",
    "#         print(\"h2_expand_1 shape:\", h2_expand_1.shape)\n",
    "#         print(\"h2_expand_2 shape:\", h2_expand_2.shape)\n",
    "#         h_e2 = self.f_e2(torch.cat((h2_expand_1, h2_expand_2), dim=-1))  # v->e\n",
    "        \n",
    "#         return h2_prob, h_e2\n",
    "\n",
    "\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, graph_edges, vertices_dim=3, edges_dim=1, hidden_dim=128):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        # Define your layers here, e.g., \n",
    "        self.f_enc = nn.Linear(vertices_dim, hidden_dim)  # Assuming 3 features per vertex as per the paper\n",
    "        self.f_e1 = nn.Linear(hidden_dim * 2, hidden_dim)  # Concatenate two vertices features\n",
    "        self.f_v = nn.Linear(hidden_dim, hidden_dim)  # Hidden layer for vertices\n",
    "        self.f_e2 = nn.Linear(hidden_dim * 2, hidden_dim)  # Concatenate updated vertices features\n",
    "\n",
    "        # Define the graph edges\n",
    "        self.graph_edges = graph_edges.to(device)\n",
    "\n",
    "    def forward(self, vertices):\n",
    "        h1 = self.f_enc(vertices)\n",
    "        print(\"h1 shape:\", h1.shape)\n",
    "        h1_expand_1 = h1[self.graph_edges[:,0]]\n",
    "        h1_expand_2 = h1[self.graph_edges[:,1]]\n",
    "        h_e1 = self.f_e1(torch.cat((h1_expand_1, h1_expand_2), dim=-1))  # Concatenate along the last dimension\n",
    "        print(\"h1_expand_1 shape:\", h1_expand_1.shape)\n",
    "        print(\"h1_expand_2 shape:\", h1_expand_2.shape)\n",
    "        print(\"h_e1 shape:\", h_e1.shape)\n",
    "        h2 = torch.tanh(h1)\n",
    "        print(\"h2 shape:\", h2)\n",
    "        h2_prob = torch.sigmoid(h2)  # vertices probability\n",
    "        h2_expand_1 = h2[self.graph_edges[:,0]]\n",
    "        h2_expand_2 = h2[self.graph_edges[:,1]]\n",
    "        print(\"h2_expand_1 shape:\", h2_expand_1.shape)\n",
    "        print(\"h2_expand_2 shape:\", h2_expand_2.shape)\n",
    "        h_e2 = self.f_e2(torch.cat((h2_expand_1, h2_expand_2), dim=-1))  # v->e\n",
    "        print(\"h_e2 shape:\", h_e2.shape)\n",
    "        \n",
    "        return h2_prob, h_e2\n",
    "\n",
    "# class GNNDecoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # Define your layers here, e.g., \n",
    "#         self.f_ep = nn.ModuleList([nn.Linear(6, 128) for _ in range(3)])  # Assuming 3 features per vertex as per the paper\n",
    "#         self.f_v = nn.Linear(128, 3)  # To update vertices\n",
    "\n",
    "#     def forward(self, vertices, edges):\n",
    "#         h = sum(edge*f_ep(torch.cat((vertices[None, :], vertices[:, None]), dim=-1)) for edge, f_ep in zip(edges, self.f_ep))  # v->e\n",
    "#         updated_vertices = vertices + self.f_v(h.sum(dim=1))  # e->v\n",
    "\n",
    "class GNNDecoder(nn.Module):\n",
    "    def __init__(self, graph_edges, hidden_dim=128, vertices_dim=3, num_classes=6):\n",
    "        super().__init__()\n",
    "        # Define your layers here\n",
    "        self.f_ep = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.f_v = nn.Linear(hidden_dim, vertices_dim + num_classes - 1)  # To update vertices + classes\n",
    "\n",
    "        # Define the graph edges\n",
    "        self.graph_edges = torch.tensor(graph_edges) - 1  # Subtract 1 for zero-based indexing\n",
    "        self.graph_edges = torch.cat((self.graph_edges, torch.flip(self.graph_edges, [1])), dim=0)  # Make graph bidirectional\n",
    "\n",
    "    def forward(self, vertices, edges):\n",
    "        vertices_expand_1 = vertices[self.graph_edges[:,0]]\n",
    "        vertices_expand_2 = vertices[self.graph_edges[:,1]]\n",
    "        print(\"Edges shape\", edges.shape)\n",
    "        h = self.f_ep(torch.cat((vertices_expand_1, vertices_expand_2), dim=-1))  # v->e\n",
    "        print(\"h shape\", h.shape)\n",
    "        edges_expand = edges.unsqueeze(-1).unsqueeze(-1).expand_as(h)  # Expand edges to match the shape of h\n",
    "        h = h * edges_expand  # apply edge weights\n",
    "#         h = h * edges.unsqueeze(-1)  # apply edge weights\n",
    "\n",
    "        h = h.view(-1, edges.size(1), h.size(-1))  # Reshape h to (num_edges, num_vertices, hidden_dim)\n",
    "        \n",
    "        h_summed = h.sum(dim=0)  # Sum over the edges\n",
    "        updated_vertices_classes = self.f_v(h_summed)  # e->v\n",
    "\n",
    "        updated_vertices = torch.tanh(updated_vertices_classes[:, :2])  # For x, y coordinates\n",
    "        updated_classes = F.softmax(updated_vertices_classes[:, 2:], dim=1)  # For class probabilities\n",
    "\n",
    "        updated_vertices = torch.cat((updated_vertices, updated_classes), dim=1)  # Concatenate along the last dimension\n",
    "\n",
    "        return updated_vertices\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90ea42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as functional\n",
    "\n",
    "# class GNNEncoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # Define your layers here, e.g., \n",
    "#         self.f_enc = nn.Linear(3, 128)  # Assuming 3 features per vertex as per the paper\n",
    "#         self.f_e1 = nn.Linear(256, 128)  # Concatenate two vertices\n",
    "#         self.f_v = nn.Linear(128, 128)  # Hidden layer for vertices\n",
    "#         self.f_e2 = nn.Linear(256, 128)  # Concatenate updated vertices\n",
    "\n",
    "#     def forward(self, vertices):\n",
    "#         h1 = self.f_enc(vertices)\n",
    "#         N = h1.shape[0]\n",
    "#         h1_expand_1 = h1.unsqueeze(1).expand(-1, h1.size(0), -1)  # Now the size is (N, N, F)\n",
    "#         h1_expand_2 = h1.unsqueeze(0).expand(h1.size(0), -1, -1)  # Now the size is (N, N, F)\n",
    "#         h_e1 = self.f_e1(torch.cat((h1_expand_1, h1_expand_2), dim=-1))  # Concatenate along the last dimension\n",
    "# #         h_e1 = self.f_e1(torch.cat((h1[None, :], h1[:, None]), dim=-1))  # v->e\n",
    "#         h2 = self.f_v(h_e1.sum(dim=1))  # e->v\n",
    "#         h2_prob = functional.log_softmax(h2, dim=-1) # vertices probability\n",
    "#         h2_expand_1 = h2.repeat(1, N).view(N, N, -1)\n",
    "#         h2_expand_2 = h2.repeat(N, 1).view(N, N, -1)\n",
    "#         h_e2 = self.f_e2(torch.cat((h2_expand_1, h2_expand_2), dim=-1))  # v->e\n",
    "#         return h2_prob, h_e2 \n",
    "\n",
    "# class GNNDecoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # Define your layers here, e.g.,\n",
    "#         self.f_ep = nn.ModuleList([nn.Linear(256, 128) for _ in range(3)])  # Assuming 3 features per vertex as per the paper\n",
    "#         self.f_v = nn.Linear(128, 3)  # To update vertices\n",
    "\n",
    "#     def forward(self, vertices, edges):\n",
    "#         edges = edges.long()  # Convert edges to integer tensor\n",
    "#         N = vertices.size(0)\n",
    "#         expanded_vertices = vertices.unsqueeze(1).expand(-1, N, -1)  # Expand vertices tensor to match the shape of edges\n",
    "#         h = sum(edge * f_ep(torch.cat((expanded_vertices, expanded_vertices.transpose(0, 1)), dim=-1)) for edge, f_ep in zip(edges, self.f_ep))  # v->e\n",
    "#         updated_vertices = vertices + self.f_v(h.sum(dim=1).reshape(vertices.shape))  # e->v (reshape h)\n",
    "#         return updated_vertices\n",
    "\n",
    "# class GNNDecoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.f_ep = nn.ModuleList([nn.Linear(256, 128) for _ in range(3)])  # Assuming 3 features per vertex as per the paper\n",
    "#         self.f_v = nn.Linear(128, 3)  # To update vertices\n",
    "\n",
    "#     def forward(self, vertices, edges):\n",
    "#         N = vertices.size(0)\n",
    "#         expanded_vertices = vertices.unsqueeze(1).expand(-1, N, -1)  # Expand vertices tensor to match the shape of edges\n",
    "#         h = sum(edge * f_ep(torch.cat((expanded_vertices, expanded_vertices.transpose(0, 1)), dim=-1)) for edge, f_ep in zip(edges, self.f_ep))  # v->e\n",
    "#         updated_vertices = vertices + self.f_v(h.view(-1, 128)).view(vertices.size())  # e->v\n",
    "#         return updated_vertices\n",
    "\n",
    "# class GNNDecoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.f_ep = nn.ModuleList([nn.Linear(256, 128) for _ in range(3)])  # Assuming 3 features per vertex as per the paper\n",
    "#         self.f_v = nn.Linear(128, 3)  # To update vertices\n",
    "\n",
    "#     def forward(self, vertices, edges):\n",
    "#         N = vertices.size(0)\n",
    "#         expanded_vertices = vertices.unsqueeze(1).expand(-1, N, -1)  # Expand vertices tensor to match the shape of edges\n",
    "#         h = sum(edge * f_ep(torch.cat((expanded_vertices, expanded_vertices.transpose(0, 1)), dim=-1)) for edge, f_ep in zip(edges, self.f_ep))  # v->e\n",
    "#         updated_vertices = vertices + self.f_v(h.view(-1, 128)).view(vertices.size(0), -1)  # e->v\n",
    "#         return updated_vertices\n",
    "\n",
    "# class GNNDecoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.f_ep = nn.ModuleList([nn.Linear(256, 128) for _ in range(3)])  # Assuming 3 features per vertex as per the paper\n",
    "#         self.f_v = nn.Linear(128, 18)  # To update vertices (assuming 6 keypoints with 3 dimensions each)\n",
    "\n",
    "#     def forward(self, vertices, edges):\n",
    "#         N = vertices.size(0)\n",
    "#         expanded_vertices = vertices.unsqueeze(1).expand(-1, N, -1)  # Expand vertices tensor to match the shape of edges\n",
    "#         h = sum(edge * f_ep(torch.cat((expanded_vertices, expanded_vertices.transpose(0, 1)), dim=-1)) for edge, f_ep in zip(edges, self.f_ep))  # v->e\n",
    "#         updated_vertices = vertices + self.f_v(h.view(vertices.size(0), -1))  # e->v\n",
    "#         return updated_vertices\n",
    "\n",
    "# class GNNDecoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.f_ep = nn.ModuleList([nn.Linear(256, 128) for _ in range(3)])  # Assuming 3 features per vertex as per the paper\n",
    "#         self.f_v = nn.Linear(128, 18)  # To update vertices (assuming 6 keypoints with 3 dimensions each)\n",
    "\n",
    "#     def forward(self, vertices, edges):\n",
    "#         N = vertices.size(0)\n",
    "#         expanded_vertices = vertices.unsqueeze(1).expand(-1, N, -1)  # Expand vertices tensor to match the shape of edges\n",
    "#         h = sum(edge * f_ep(torch.cat((expanded_vertices, expanded_vertices.transpose(0, 1)), dim=-1)) for edge, f_ep in zip(edges, self.f_ep))  # v->e\n",
    "#         updated_vertices = vertices + self.f_v(h.view(vertices.size(0), -1)).view(vertices.size())  # e->v\n",
    "#         return updated_vertices\n",
    "\n",
    "# class GNNDecoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.f_ep = nn.ModuleList([nn.Linear(256, 128) for _ in range(3)])  # Assuming 3 features per vertex as per the paper\n",
    "#         self.f_v = nn.Linear(128, 18)  # To update vertices (assuming 6 keypoints with 3 dimensions each)\n",
    "\n",
    "#     def forward(self, vertices, edges):\n",
    "#         N = vertices.size(0)\n",
    "#         expanded_vertices = vertices.unsqueeze(1).expand(-1, N, -1)  # Expand vertices tensor to match the shape of edges\n",
    "#         h = sum(edge * f_ep(torch.cat((expanded_vertices, expanded_vertices.transpose(0, 1)), dim=-1)) for edge, f_ep in zip(edges, self.f_ep))  # v->e\n",
    "#         updated_vertices = vertices + self.f_v(h.view(vertices.size(0), -1)).view(vertices.size(0), -1)  # e->v\n",
    "#         return updated_vertices\n",
    "\n",
    "class TrifocalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, vertices_pred, vertices_gt):\n",
    "        loss = (vertices_gt - vertices_pred).pow(2).mean()  # Changed from sum() to mean()\n",
    "        return loss\n",
    "    \n",
    "def cross_entropy_loss_func(vertices_prob, vertices_gt):\n",
    "    return -torch.sum(vertices_gt * vertices_prob)\n",
    "\n",
    "def one_hot_encode(vertices_gt, num_classes):\n",
    "    \"\"\"\n",
    "    One-hot encode the ground truth vertices.\n",
    "    :param vertices_gt: tensor of shape (N, K, D), where N is the number of samples, K is the number of keypoints, and D is the number of dimensions per keypoint.\n",
    "    :param num_classes: the total number of keypoints types (classes).\n",
    "    :return: one-hot encoded vertices of shape (N, K, num_classes).\n",
    "    \"\"\"\n",
    "    # Subtract 1 for zero-based indexing\n",
    "    vertices_gt = vertices_gt.long() - 1\n",
    "\n",
    "    # Create a tensor of zeros of size (N, K, num_classes)\n",
    "    one_hot = torch.zeros(vertices_gt.size(0), vertices_gt.size(1), num_classes).to(device)\n",
    "\n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot.scatter_(2, vertices_gt.unsqueeze(-1), 1)\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a63300d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "#         self.graph_edges = torch.tensor([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]], dtype=torch.long)\n",
    "        # Define edges \n",
    "        edges = [(1,2), (2,3), (3,4), (4,5), (5,6)]\n",
    "        # Convert to tensor and subtract 1 for zero-based indexing\n",
    "        self.graph_edges = torch.tensor(edges, dtype=torch.long) - 1\n",
    "        self.gnn_encoder = GNNEncoder(self.graph_edges.to(device))\n",
    "        self.gnn_decoder = GNNDecoder(self.graph_edges.to(device))\n",
    "\n",
    "    def forward(self, img):\n",
    "        print(\"image in keypoints eval phase\", img.shape)\n",
    "#         img = F.to_tensor(img).to(device)\n",
    "        img.unsqueeze_(0)\n",
    "        img = list(img)\n",
    "        with torch.no_grad():\n",
    "            self.keypoint_model.to(device)\n",
    "            self.keypoint_model.eval()\n",
    "            output = self.keypoint_model(img)\n",
    "            \n",
    "        img = (img[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \\\n",
    "            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "        \n",
    "        keypoints = []\n",
    "        key_points = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append(list(map(int, kps[0,0:2])))\n",
    "            key_points.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "        labels = []\n",
    "        for label in output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            labels.append(label)\n",
    "#         keypoints_ = [(x,_) for _,x in sorted(zip(labels,keypoints))]\n",
    "#         keypoints_ = [list(x,_) for (x, _) in sorted(zip(keypoints, labels))]\n",
    "        keypoints_ = [list(x) + [y] for (x, y) in sorted(zip(keypoints, labels))]\n",
    "        \n",
    "        print(\"keypoints_\",keypoints_)\n",
    "        \n",
    "        keypoints = torch.stack([torch.tensor(kp) for kp in keypoints_]).float().to(device)\n",
    "\n",
    "        print(\"keypoints\", keypoints)\n",
    "        print(keypoints.shape)\n",
    "#         keypoints = keypoints_\n",
    "        vertices_prob, edges = self.gnn_encoder(keypoints)\n",
    "#         print(vertices_prob)\n",
    "        print(vertices_prob.shape, edges.shape)\n",
    "        vertices = self.gnn_decoder(vertices_prob, edges)\n",
    "        \n",
    "        return vertices, vertices_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "945ce7b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4610/2276094537.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.graph_edges = torch.tensor(graph_edges) - 1  # Subtract 1 for zero-based indexing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2662 files [00:00, 21202.74 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2662 files [00:00, 20683.99 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2662 files [00:00, 20460.73 files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n",
      "torch.Size([3, 640, 480])\n",
      "ground_truth_vertex tensor([[[112.0801, 257.9522,   1.0000]],\n",
      "\n",
      "        [[195.9869, 257.9597,   2.0000]],\n",
      "\n",
      "        [[271.7243, 282.4390,   3.0000]],\n",
      "\n",
      "        [[265.3522, 302.1998,   4.0000]],\n",
      "\n",
      "        [[330.3802, 376.7305,   5.0000]],\n",
      "\n",
      "        [[338.7143, 397.0408,   6.0000]]], device='cuda:0')\n",
      "image in keypoints eval phase torch.Size([3, 640, 480])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints_ [[112, 258, 1], [196, 258, 2], [265, 302, 4], [271, 282, 3], [330, 377, 5], [339, 397, 6]]\n",
      "keypoints tensor([[112., 258.,   1.],\n",
      "        [196., 258.,   2.],\n",
      "        [265., 302.,   4.],\n",
      "        [271., 282.,   3.],\n",
      "        [330., 377.,   5.],\n",
      "        [339., 397.,   6.]], device='cuda:0')\n",
      "torch.Size([6, 3])\n",
      "h1 shape: torch.Size([6, 128])\n",
      "h1_expand_1 shape: torch.Size([5, 128])\n",
      "h1_expand_2 shape: torch.Size([5, 128])\n",
      "h_e1 shape: torch.Size([5, 128])\n",
      "h2 shape: tensor([[ 1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  1.0000,  0.9999, -1.0000, -1.0000,  1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n",
      "          1.0000, -1.0000, -0.6751,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -0.9769, -1.0000,  1.0000, -1.0000,  0.9999,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n",
      "          1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000,  0.9265,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000, -0.9999,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n",
      "          1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n",
      "         -1.0000,  1.0000,  0.9804,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3977, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n",
      "          1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  0.5693,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -0.8971,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n",
      "          1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -0.9364,\n",
      "         -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -1.0000,  0.9807, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n",
      "          1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  0.9995,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  0.9973, -1.0000, -1.0000,  1.0000,  1.0000,\n",
      "          0.9551,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n",
      "          1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000, -1.0000,  1.0000,  0.1697, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.9996,\n",
      "         -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000,  0.7710,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n",
      "          1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  0.6411,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -0.8937,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n",
      "          1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -0.9853,\n",
      "         -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -1.0000,  0.9853, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n",
      "          1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000, -0.2849,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  0.9834,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n",
      "          1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,\n",
      "         -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -0.9998,\n",
      "         -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "         -1.0000, -0.8672, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n",
      "          1.0000,  1.0000, -1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,\n",
      "          1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000]],\n",
      "       device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "h2_expand_1 shape: torch.Size([5, 128])\n",
      "h2_expand_2 shape: torch.Size([5, 128])\n",
      "h_e2 shape: torch.Size([5, 128])\n",
      "torch.Size([6, 128]) torch.Size([5, 128])\n",
      "Edges shape torch.Size([5, 128])\n",
      "h shape torch.Size([10, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.cuda.FloatTensor{[5, 128, 1, 1]}, size=[10, 128]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth_vertex\u001b[39m\u001b[38;5;124m\"\u001b[39m, vertices_gt)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m vertices_pred, vertices_prob \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(vertices_prob\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Compute the losses\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36mKeypointPipeline.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#         print(vertices_prob)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28mprint\u001b[39m(vertices_prob\u001b[38;5;241m.\u001b[39mshape, edges\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 52\u001b[0m         vertices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m vertices, vertices_prob\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36mGNNDecoder.forward\u001b[0;34m(self, vertices, edges)\u001b[0m\n\u001b[1;32m     98\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_ep(torch\u001b[38;5;241m.\u001b[39mcat((vertices_expand_1, vertices_expand_2), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# v->e\u001b[39;00m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 100\u001b[0m         edges_expand \u001b[38;5;241m=\u001b[39m \u001b[43medges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Expand edges to match the shape of h\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m*\u001b[39m edges_expand  \u001b[38;5;66;03m# apply edge weights\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m#         h = h * edges.unsqueeze(-1)  # apply edge weights\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.cuda.FloatTensor{[5, 128, 1, 1]}, size=[10, 128]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (4)"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = KeypointPipeline()\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss\n",
    "criterion = TrifocalLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Load the trained model\n",
    "model.keypoint_model = torch.load(weights_path).to(device)\n",
    "\n",
    "num_epochs = 10  # Define your number of epochs\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "model.train()\n",
    "# Epoch loop\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in your training data\n",
    "    for batch in data_loader_train:\n",
    "        img_tuple, target_dict_tuple = batch\n",
    "        img = img_tuple[0]\n",
    "        print(img.shape)\n",
    "        target = target_dict_tuple[0]\n",
    "\n",
    "        img = img.to(device)\n",
    "        vertices_gt = target['keypoints'].to(device)\n",
    "        num_vertices = vertices_gt.shape[0]\n",
    "        vertices_gt[:, :, 2] = torch.arange(1, num_vertices+1).unsqueeze(1).to(device)\n",
    "\n",
    "#         vertices_gt = one_hot_encode(target['keypoints'].to(device), num_classes=6)\n",
    "\n",
    "        \n",
    "        print(\"ground_truth_vertex\", vertices_gt)\n",
    "\n",
    "        # Forward pass\n",
    "        vertices_pred, vertices_prob = model(img)\n",
    "        \n",
    "        print(vertices_prob.shape)\n",
    "\n",
    "        # Compute the losses\n",
    "        trifocal_loss = criterion(vertices_pred, vertices_gt)\n",
    "        ce_loss = cross_entropy_loss_func(vertices_prob, vertices_gt)\n",
    "\n",
    "        # Combined loss\n",
    "        loss = trifocal_loss + ce_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for each epoch\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Please go through the following notebook and tell where I am going wrong . I am going to submit the code in two parts                                                                                                                                                  Part1:                                                                                                                                                                                      weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_ld_b1_e25_v2.pth'                                                      # to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"                                                                                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )                                                                                                                                                                                                                  def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    print(type(output))\n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output                                                                                                                                                                                                          class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "#         self.imgs_files = [file for file in sorted(os.listdir(root)) if file.endswith(\".jpg\")]\n",
    "#         self.annotations_files = [file for file in sorted(os.listdir(root)) if file.endswith(\".json\")]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         img_path = os.path.join(self.root, self.imgs_files[idx])\n",
    "#         annotations_path = os.path.join(self.root, self.annotations_files[idx])\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes'][:6]\n",
    "#             print(\"bounding boxes\", bboxes_original)\n",
    "            keypoints_original = data['keypoints'][:6]\n",
    "#             print(\"original keypoints\", np.array(keypoints_original))\n",
    "#             print(\"original keypoints shape\", (np.array(keypoints_original)).shape)\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "#                     print(\"kp index\", k_idx)\n",
    "#                     print(\"key points\",kp)\n",
    "#                     print(\"keypoints original second iter\", [keypoints_original[0][o_idx][k_idx]],\n",
    "#                           [keypoints_original[o_idx][k_idx][0]], [keypoints_original[o_idx][k_idx][1]], \\\n",
    "#                          [keypoints_original[o_idx][k_idx][2]], [keypoints_original[o_idx][k_idx][3]])\n",
    "                    # kp - coordinates of keypoint\n",
    "                    # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "#             print(keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)                                                                                                                                                          def get_model(num_keypoints, weights_path=None):\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 7, # Background is the first class, object is the second class\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4397900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740f228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6214a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

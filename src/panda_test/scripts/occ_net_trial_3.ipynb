{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedf1768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16908615680\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "# weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b355d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "237a5cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2189cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "#         A.Resize(640, 480)  # Resize all images to be 640x480\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaae8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9395a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f915be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model(num_keypoints, weights_path=None):\n",
    "    \n",
    "#     anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "#     model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "#                                                                    pretrained_backbone=True,\n",
    "#                                                                    num_keypoints=num_keypoints,\n",
    "#                                                                    num_classes = 7, # Background is the first class, object is the second class\n",
    "#                                                                    rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "#     if weights_path:\n",
    "#         state_dict = torch.load(weights_path)\n",
    "#         model.load_state_dict(state_dict)        \n",
    "        \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, vertices_dim=2, hidden_dim=128, num_vertices=6, num_edge_features=2):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.f_enc = nn.Linear(vertices_dim, hidden_dim)\n",
    "        self.f_e1 = nn.Linear((hidden_dim * 2), hidden_dim)\n",
    "        self.f_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.f_e2 = nn.Linear((hidden_dim * 2), num_edge_features)\n",
    "        self.num_vertices = num_vertices        \n",
    "    \n",
    "#     def get_node_features(self, vertices):\n",
    "# #         print(\"Vertices in node features\", vertices)\n",
    "#         node_features = []\n",
    "#         for keypoint in vertices:\n",
    "#             x, y, confidence, visibility, label = keypoint\n",
    "#             node_features.append([x, y, confidence, visibility, label])        \n",
    "#         nodes = torch.tensor(node_features, dtype=torch.float).to(device)\n",
    "# #         print(nodes)\n",
    "#         return nodes\n",
    "\n",
    "    def get_edge_features(self, vertices):\n",
    "        edges = [(0,1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)]\n",
    "#         print(edges)\n",
    "#         edge_features = []\n",
    "#         for edge in edges:\n",
    "#             print(edge)\n",
    "#             print(edge[0])\n",
    "#             print(edge[1])\n",
    "#             k1, k2 = vertices[edge[0]][:2], vertices[edge[1]][:2]\n",
    "#             distance = torch.norm(k1 - k2)\n",
    "#             angle = torch.atan2(k2[1] - k1[1], k2[0] - k1[0])\n",
    "#             edge_features.append([distance.item(), angle.item()])\n",
    "            \n",
    "        edges = torch.tensor(edges, dtype=torch.long).to(device)\n",
    "#         edge_features = torch.tensor(edge_features, dtype=torch.float).to(device)\n",
    "#         return edges, edge_features\n",
    "        return edges\n",
    "\n",
    "    def forward(self, vertices):\n",
    "        nodes = self.get_node_features(vertices)\n",
    "        edges, edge_features = self.get_edge_features(vertices)\n",
    "        h1 = self.f_enc(nodes)\n",
    "        h1_source = h1[edges[:, 0]]\n",
    "        h1_target = h1[edges[:, 1]]\n",
    "        h_e1 = self.f_e1(torch.cat((h1_source, h1_target, edge_features), dim=1))  # Include edge feature in the input\n",
    "        h_j_2 = self.f_v(h_e1)\n",
    "        h2_source = h_j_2[edges[:, 0]]\n",
    "        h2_target = h_j_2[edges[:, 1]]\n",
    "        h_e2 = self.f_e2(torch.cat((h1_source, h1_target, edge_features), dim=1))  # Include edge feature in the input\n",
    "        h_e2_prob = torch.sigmoid(h_e2)\n",
    "        return vertices, h_e2_prob, edges\n",
    "\n",
    "class GNNDecoder(nn.Module):\n",
    "    def __init__(self, vertices_dim=2, hidden_dim=128, num_vertices=6, num_edge_features=2):\n",
    "        super(GNNDecoder, self).__init__()\n",
    "        self.f_e = nn.Linear((vertices_dim * 2), num_edge_features)  # Concatenate two vertices features\n",
    "        self.f_h = nn.Linear(num_edge_features, vertices_dim)  # Transform h_ij to the same dimension as vertices\n",
    "        self.f_v = nn.Linear(vertices_dim, vertices_dim)  # Update vertex feature\n",
    "    \n",
    "    def forward(self, vertices, h_e2_prob, edges, edge_features):\n",
    "        h_source = vertices[edges[:, 0]]\n",
    "        h_target = vertices[edges[:, 1]]\n",
    "        h = torch.zeros_like(vertices)\n",
    "\n",
    "        for idx, (i, j) in enumerate(edges):  # Iterate over edges\n",
    "            single_edge_features = edge_features[idx].unsqueeze(0)    \n",
    "            h_ij = h_e2_prob[idx] * self.f_e(torch.cat((h_source[idx].unsqueeze(0), h_target[idx].unsqueeze(0), single_edge_features), dim=1))  # Include edge weights in the input\n",
    "            h_ij_transformed = self.f_h(h_ij.squeeze())  # Transform h_ij to the same dimension as vertices\n",
    "            h[j] += h_ij_transformed  # Accumulate edge features to the target vertex\n",
    "\n",
    "        h_transformed = self.f_v(h.view(-1, vertices.shape[1]))  # Transform h\n",
    "        h_transformed = h_transformed.view(vertices.shape)  # Reshape back to original shape\n",
    "        vertices_g = vertices + h_transformed  # Update vertex features\n",
    "\n",
    "        return vertices_g  # Return vertices_g as the prediction and vertices_g itself as the mean for Gaussian distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccludedKeyPointLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, vertices_pred, vertices_gt):\n",
    "        vertices_pred = vertices_pred[:,:,:2]  # Considering only x, y coordinates, confidence_score\n",
    "        vertices_gt = vertices_gt[:,:,:2]  # Considering only x, y coordinates, confidence_score\n",
    "        # Compute differences\n",
    "        diff = (vertices_gt - vertices_pred).abs()\n",
    "        # Compute Huber loss\n",
    "        huber_loss = torch.where(diff < self.delta, 0.5 * diff**2, self.delta * (diff - 0.5 * self.delta))\n",
    "        return huber_loss.mean()\n",
    "    \n",
    "def visibility_loss (vertices_pred, vertices_gt):    \n",
    "    return nn.functional.cross_entropy(vertices_pred[:,:,:2], vertices_gt[:,:,:2])  # Loss based on visibility of keypoints\n",
    "def edge_loss(edges_prob, edges_gt):\n",
    "    # Compute the cross-entropy loss\n",
    "    loss = -torch.sum(edges_gt.to(device) * torch.log(torch.clamp(edges_prob, min=1e-7)))                      \n",
    "    return loss\n",
    "\n",
    "def temporal_consistency_loss(y_true_sequence, y_pred_sequence):\n",
    "    loss = 0\n",
    "    for t in range(1, len(y_true_sequence)):\n",
    "        # Selecting the x, y coordinates and visibility for true and predicted sequences\n",
    "        true_diff = y_true_sequence[t, :, :] - y_true_sequence[t-1, :, :]\n",
    "        pred_diff = y_pred_sequence[t, :, :] - y_pred_sequence[t-1, :, :]\n",
    "        loss += torch.mean(torch.abs(true_diff - pred_diff))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, num_vertices):\n",
    "        super().__init__()\n",
    "\n",
    "        self.keypoint_rcnn = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=True, num_keypoints=6, num_classes=7)\n",
    "        self.num_vertices = num_vertices\n",
    "        self.gnn_encoder = GNNEncoder()\n",
    "        self.gnn_decoder = GNNDecoder()\n",
    "\n",
    "    def forward(self, images, targets=None, train=False):\n",
    "        if train:\n",
    "            output = self.keypoint_rcnn(images, targets)\n",
    "            return output\n",
    "    \n",
    "        else:\n",
    "            all_keypoints = []\n",
    "            all_edges\n",
    "            with torch.no_grad():\n",
    "                self.keypoint_rcnn.eval()\n",
    "                output = self.keypoint_rcnn(images)\n",
    "                self.keypoint_rcnn.train()\n",
    "\n",
    "                for i in range(len(images)):  # Process each image in the batch\n",
    "                    print(\"image in sample\", i)\n",
    "                    keypoints = output[i]['keypoints'].detach().cpu().numpy()\n",
    "                    kp_score = output[i]['keypoints_scores'].detach().cpu().numpy()\n",
    "                    labels = output[i]['labels'].detach().cpu().numpy()\n",
    "                    unique_labels = list(set(labels))\n",
    "                    scores = output[i]['scores'].detach().cpu().numpy()\n",
    "                    print(\"labels\", unique_labels)\n",
    "\n",
    "                    kps = []\n",
    "                    kp_scores = []\n",
    "                    ulabels = []\n",
    "\n",
    "                    for label in unique_labels:\n",
    "                        indices = [j for j, x in enumerate(labels) if x == label]\n",
    "                        scores_for_label = [scores[j] for j in indices]\n",
    "                        max_score_index = indices[scores_for_label.index(max(scores_for_label))]\n",
    "                        kp_score_label = kp_score[max_score_index].tolist()\n",
    "                        kps.append(keypoints[max_score_index][kp_score_label.index(max(kp_score_label))])\n",
    "                        ulabels.append(label)\n",
    "\n",
    "                    kps = [torch.tensor(kp, dtype=torch.float32) for kp in kps]\n",
    "                    if not kps:\n",
    "                        default_value = torch.tensor([[320, 240, 1]], dtype=torch.float32, device=images[i].device)\n",
    "                        keypoints = default_value.repeat(6, 1)\n",
    "                    else:\n",
    "                        keypoints = torch.stack(kps)\n",
    "                        \n",
    "                    print(\"kp before placeholder\", keypoints)\n",
    "                    keypoints = self.complete_missing_keypoints(keypoints, unique_labels)\n",
    "                    print(\"kp after placeholder\", keypoints)\n",
    "                    vertices, self.enc_e, self.edges = self.gnn_encoder(keypoints, edge_index)\n",
    "                    vertices_pred = self.gnn_decoder(vertices, self.enc_e, self.edges)\n",
    "                    all_keypoints.append(vertices_pred)\n",
    "                    all_pred_edges.append(self.enc_e)\n",
    "                    all_edges.append(self.edges)\n",
    "                    print(\"kp after graph\", vertices_pred)\n",
    "                    print(\"edges after graph\", self.enc_e)\n",
    "\n",
    "            print(\"All keypoints\", all_keypoints)\n",
    "\n",
    "            return torch.stack(all_keypoints), torch.stack(all_pred_edges), torch.stack(all_edges)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592175b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 4\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_val = ClassDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize the GradScaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "top_5_models = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):  # for 50 epochs\n",
    "    for batch_idx, batch in enumerate(data_loader_train):\n",
    "        images, targets = batch  \n",
    "        \n",
    "        if len(images) != batch_size:\n",
    "            continue\n",
    "        print(\"The training is continued\")\n",
    "        \n",
    "        # Move images to GPU\n",
    "        images = torch.stack(images).cuda()\n",
    "        # Move targets to GPU\n",
    "        for target in targets:\n",
    "            for key, val in target.items():\n",
    "                target[key] = val.cuda()\n",
    "        \n",
    "        ground_truth_keypoints = [target['keypoints'] for target in targets]\n",
    "        ground_truth_boxes = [target['boxes'] for target in targets]\n",
    "        \n",
    "        # Assuming you want all images to be of size [3, 640, 480]\n",
    "#         desired_size = (640, 480)  \n",
    "\n",
    "        # Resize all images to the desired size\n",
    "#         resized_images = [F.resize(img, desired_size) for img in images]\n",
    "\n",
    "        ground_truth_keypoints = torch.stack(ground_truth_keypoints).squeeze()[:, :, 0:2]\n",
    "        print(\"ground_truth_keypoints\", ground_truth_keypoints)\n",
    "        ground_truth_boxes = torch.stack(ground_truth_boxes)[:, :, 0:2]\n",
    "        \n",
    "        print(\"ground truth keypoints shape\", ground_truth_keypoints.shape)\n",
    "\n",
    "#         Create a batched adjacency matrix with the same batch size\n",
    "#         batch_adj_matrix = adj_matrix.repeat(batch_size, 1, 1)\n",
    "        batch_adj_matrix = adj_matrix\n",
    "        print(batch_adj_matrix.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward pass for training\n",
    "#         output_train = model(images, adj_matrix=batch_adj_matrix, targets=targets, train=True)\n",
    "#         print(\"Output keypoints shape\", output_train.keys())\n",
    "        \n",
    "#         #Forward pass for loss\n",
    "#         predicted_keypoints = model(images, adj_matrix=batch_adj_matrix, train=False)\n",
    "        \n",
    "        \n",
    "#         print(\"predicted keypoints\", predicted_keypoints.shape)\n",
    "                \n",
    "#         loss_keypoint = output_train['loss_keypoint']\n",
    "        \n",
    "#         # Compute loss and backpropagate\n",
    "#         loss = custom_loss(predicted_keypoints, ground_truth_keypoints, \n",
    "#                            adj_matrix=batch_adj_matrix, loss_keypoint=loss_keypoint)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "        # Automatic mixed precision for forward pass\n",
    "        with autocast():\n",
    "            output_train = model(images, targets=targets, train=True)\n",
    "#             print(\"Output keypoints shape\", output_train.keys())\n",
    "            \n",
    "            predicted_keypoints = model(images, train=False)\n",
    "         \n",
    "#             print(\"predicted keypoints\", predicted_keypoints)\n",
    "\n",
    "            loss_keypoint = output_train['loss_keypoint']\n",
    "\n",
    "            # Compute loss\n",
    "            loss = custom_loss(predicted_keypoints, ground_truth_keypoints, loss_keypoint=loss_keypoint)\n",
    "            \n",
    "            print(loss.device)\n",
    "        \n",
    "        # Scale the loss and backpropagate\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scheduler.step()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        \n",
    "        # Check if the current model should be saved as a top model\n",
    "        if len(top_5_models) < 5 or loss.item() < max(top_5_models, key=lambda x: x[0])[0]:\n",
    "            # Save the model state and loss\n",
    "            model_state = {\n",
    "                'epoch': epoch,\n",
    "                'complete_model': model,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "            }\n",
    "            top_5_models.append((loss.item(), model_state))\n",
    "\n",
    "            # Sort the list based on loss (ascending order)\n",
    "            top_5_models.sort(key=lambda x: x[0])\n",
    "\n",
    "            # If there are more than 5 models, remove the one with the highest loss\n",
    "            if len(top_5_models) > 5:\n",
    "                top_5_models.pop()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx + 1}/{len(data_loader_train)}, Loss: {loss.item()}\")\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "    \n",
    "# After all epochs, save the top 5 models to disk\n",
    "for idx, (_, model_state) in enumerate(top_5_models):\n",
    "    torch.save(model_state, f'/home/jc-merlab/Pictures/Data/trained_models/best_gnn_model_b{batch_size}_e{num_epochs}_{idx+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ce7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = KeypointPipeline(num_vertices=6)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss\n",
    "criterion = OccludedKeyPointLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 1  # Define your number of epochs\n",
    "batch_size = 1\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "v = 1\n",
    "\n",
    "# # Initialize sequences for true and predicted keypoints\n",
    "# y_true_sequence = []\n",
    "# y_pred_sequence = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for i, batch in enumerate(data_loader_train):\n",
    "        img_tuple, target_dict_tuple, img_files = batch\n",
    "        print(f\"Processing batch {i+1} with images:\", img_files)\n",
    "        \n",
    "        imgs = [img.to(device) for img in img_tuple]  # Create list of images\n",
    "\n",
    "        # Process each image individually\n",
    "        losses = []\n",
    "        for i in range(len(imgs)):\n",
    "            img = imgs[i].unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "\n",
    "            # Prepare ground truth vertices for the image\n",
    "            keypoints = target_dict_tuple[i]['keypoints'].to(device)\n",
    "            print(keypoints.shape)\n",
    "            # add visibility to ground truth\n",
    "            visibility = torch.ones((keypoints.shape[0], keypoints.shape[1], 1)).to(device)\n",
    "            print(visibility.shape)\n",
    "            # add labels to ground truth\n",
    "            labels = torch.arange(1, keypoints.shape[0] + 1, device=keypoints.device).view(-1, 1, 1).float()\n",
    "            print(labels.shape)\n",
    "            # update gt with vis and labels\n",
    "            vertices_gt = torch.cat((keypoints, visibility, labels), dim=2).unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "            \n",
    "            vertices_gt = vertices_gt.squeeze()    \n",
    "            print(\"Ground truth keypoints from annotations\", vertices_gt)\n",
    "            y_true_sequence.append(vertices_gt)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(img)\n",
    "            vertices_pred = output[0]\n",
    "            y_pred_sequence.append(vertices_pred)\n",
    "            \n",
    "            edges_prob = model.enc_e\n",
    "            edges = model.edges\n",
    "            edge_features = model.edge_features\n",
    "            edges_gt = torch.cat((edges, edge_features), dim=1) \n",
    "\n",
    "            # Compute loss for the image\n",
    "            huber_loss = criterion(vertices_pred, vertices_gt)\n",
    "            ce_loss = edge_loss(edges_prob, edges_gt)\n",
    "            vis_loss = visibility_loss(vertices_pred, vertices_gt)\n",
    "\n",
    "            loss = huber_loss + ce_loss + vis_loss\n",
    "            losses.append(loss)  # Store loss for the image\n",
    "            \n",
    "        # Convert true and predicted sequences to tensors\n",
    "        y_true_tensor = torch.stack(y_true_sequence)\n",
    "        y_pred_tensor = torch.stack(y_pred_sequence)\n",
    "        \n",
    "        # Compute temporal consistency loss\n",
    "        temporal_loss = temporal_consistency_loss(y_true_tensor, y_pred_tensor)\n",
    "\n",
    "        # Average loss over all images in the batch\n",
    "        other_losses = torch.mean(torch.stack(losses))\n",
    "        \n",
    "        # Combine temporal loss with other losses\n",
    "        total_loss = other_losses + temporal_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Clear the sequences for the next batch\n",
    "        y_true_sequence.clear()\n",
    "        y_pred_sequence.clear()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    eta = epoch_time * (num_epochs - epoch - 1)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, ETA: {eta} seconds')\n",
    "\n",
    "model_save_path = f\"/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_occ_b{batch_size}_e{num_epochs}_v{v}.pth\"\n",
    "\n",
    "torch.save(model, model_save_path)\n",
    "    \n",
    "# Save the state dict of the model, not the entire model\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "torch.save(model, model_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfef6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save(img, vertices, filename):\n",
    "    print(\"type of image befor conversion\",type(img))    \n",
    "    print(\"type of vertices before conversion\", type(vertices))\n",
    "    print(img)\n",
    "    img = (img.permute(1,2,0).cpu().numpy() * 255).astype(np.uint8)\n",
    "#     img = (img * 255).astype(np.uint8)  # Convert back from [0, 1] range to [0, 255]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    vertices = vertices.cpu().numpy()\n",
    "\n",
    "    print(f\"Image shape before saving: {img.shape}\")  # print the image shape\n",
    "    print(\"type of vertices\", type(vertices))\n",
    "#     print(\"entered vertices\", vertices)\n",
    "#     print(\"entered image\", img)\n",
    "\n",
    "    # Convert grayscale to BGR if necessary\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "    for i in range(vertices.shape[0]):\n",
    "        img = cv2.circle(img, (int(vertices[i, 0]), int(vertices[i, 1])), radius=2, color=(0, 0, 255), thickness=-1)\n",
    "        \n",
    "    result = cv2.imwrite(filename, img)\n",
    "    print(f\"Image saved at {filename}: {result}\")  # print if save was successful\n",
    "\n",
    "    # If the image didn't save correctly, save the image data to a text file for examination\n",
    "    if not result:\n",
    "        with open(filename + \".txt\", \"w\") as f:\n",
    "            np.savetxt(f, img.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_save_model(model, data_loader_test):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_trifocal_loss = 0.0\n",
    "    total_ce_loss = 0.0\n",
    "    total_vis_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    y_true_sequence = []\n",
    "    y_pred_sequence = []\n",
    "\n",
    "    # We don't need to track gradients during evaluation\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(data_loader_test):\n",
    "            img_tuple, target_dict_tuple, img_files = batch\n",
    "\n",
    "            total_batch_loss = 0.0\n",
    "            total_batch_trifocal_loss = 0.0\n",
    "            total_batch_ce_loss = 0.0\n",
    "            total_batch_vis_loss = 0.0\n",
    "\n",
    "            # Process each image individually\n",
    "            for i in range(len(img_tuple)):\n",
    "                img = img_tuple[i].to(device)\n",
    "                target = target_dict_tuple[i]\n",
    "\n",
    "                # Prepare ground truth vertices for the image\n",
    "                keypoints = target['keypoints'].to(device)\n",
    "                visibility = torch.ones((keypoints.shape[0], keypoints.shape[1], 1)).to(device)\n",
    "                vertices_gt = torch.cat((keypoints, visibility), dim=2).unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "                vertices_gt = vertices_gt.squeeze()\n",
    "                y_true_sequence.append(vertices_gt)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(img.unsqueeze(0))\n",
    "                vertices_pred = output[0]\n",
    "                y_pred_sequence.append(vertices_pred)\n",
    "\n",
    "                edges_prob = model.enc_e\n",
    "                edges = model.edges\n",
    "                edge_features = model.edge_features\n",
    "                edges_gt = torch.cat((edges, edge_features), dim=1) \n",
    "\n",
    "                trifocal_loss = criterion(vertices_pred, vertices_gt)\n",
    "                ce_loss = edge_loss(edges_prob, edges_gt)\n",
    "                vis_loss = visibility_loss(vertices_pred, vertices_gt)\n",
    "                loss = trifocal_loss + ce_loss + vis_loss\n",
    "\n",
    "                total_batch_loss += loss.item()\n",
    "                total_batch_trifocal_loss += trifocal_loss.item()\n",
    "                total_batch_ce_loss += ce_loss.item()\n",
    "                total_batch_vis_loss += vis_loss.item()\n",
    "\n",
    "                # Visualize and save the prediction\n",
    "                filename = f'/home/jc-merlab/Pictures/Data/occ_vis_data/image_{idx}_{i}.jpg'\n",
    "                visualize_and_save(img, vertices_pred, filename)\n",
    "                print(f\"Image saved at {filename}\")  # Print statement to confirm image save\n",
    "\n",
    "            # Convert true and predicted sequences to tensors\n",
    "            y_true_tensor = torch.stack(y_true_sequence)\n",
    "            y_pred_tensor = torch.stack(y_pred_sequence)\n",
    "\n",
    "            # Compute temporal consistency loss\n",
    "            temporal_loss = temporal_consistency_loss(y_true_tensor, y_pred_tensor)\n",
    "\n",
    "            total_loss += (total_batch_loss + temporal_loss) / len(img_tuple)\n",
    "            total_trifocal_loss += total_batch_trifocal_loss / len(img_tuple)\n",
    "            total_ce_loss += total_batch_ce_loss / len(img_tuple)\n",
    "            num_batches += 1\n",
    "\n",
    "            # Clear the sequences for the next batch\n",
    "            y_true_sequence.clear()\n",
    "            y_pred_sequence.clear()\n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_trifocal_loss = total_trifocal_loss / num_batches\n",
    "    avg_ce_loss = total_ce_loss / num_batches\n",
    "    \n",
    "    print(f'Avg. Test Loss: {avg_loss}, Avg. Trifocal Loss: {avg_trifocal_loss}, Avg. Cross Entropy Loss: {avg_ce_loss}')\n",
    "    return avg_loss, avg_trifocal_loss, avg_ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_loss, avg_trifocal_loss, avg_ce_loss, all_preds = test_and_save_model(model, data_loader_test)\n",
    "\n",
    "avg_loss, avg_trifocal_loss, avg_ce_loss = test_and_save_model(model, data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58949932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Directory containing images\n",
    "dir_path = '/home/jc-merlab/Pictures/Data/occ_vis_data/'\n",
    "images = []\n",
    "\n",
    "# Ensure the images are sorted by name\n",
    "for f in sorted(os.listdir(dir_path)):\n",
    "    if f.endswith('.jpg') or f.endswith('.png'):  # Check for image file extension\n",
    "        images.append(f)\n",
    "\n",
    "# Determine the width and height from the first image\n",
    "image_path = os.path.join(dir_path, images[0])\n",
    "frame = cv2.imread(image_path)\n",
    "cv2.imshow('video',frame)\n",
    "height, width, channels = frame.shape\n",
    "\n",
    "# Define the codec and create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Be sure to use the correct codec\n",
    "video_filename = 'output.mp4'\n",
    "video = cv2.VideoWriter(video_filename, fourcc, 3.0, (width, height))\n",
    "\n",
    "for image in images:\n",
    "    image_path = os.path.join(dir_path, image)\n",
    "    frame = cv2.imread(image_path)\n",
    "    video.write(frame)  # Write out frame to video\n",
    "\n",
    "# Release everything when job is finished\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"The output video is\", video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0007f54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_occ_b16_e25_v6.pth'\n",
    "\n",
    "model = torch.load(model_path).to(device)\n",
    "\n",
    "\n",
    "image = Image.open(\"/home/jc-merlab/Pictures/Data/occluded_results_mi20_ma80_n2/occluded_000041.rgb.jpg\")\n",
    "print(type(image))\n",
    "\n",
    "img = F.to_tensor(image).to(device)\n",
    "img.unsqueeze_(0)\n",
    "# print(image.shape)\n",
    "# image = list(image)\n",
    "# print(type(images))\n",
    "# images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(img)\n",
    "    \n",
    "keypoints = output[0]\n",
    "\n",
    "print(keypoints)\n",
    "plt.imshow(image)\n",
    "\n",
    "# Assuming each keypoint is a tensor representing (x, y)\n",
    "for i, keypoint in enumerate(keypoints):\n",
    "    print(f'Key point {i}: {keypoint}')\n",
    "    keypoint = keypoint.cpu().numpy()\n",
    "    plt.plot(keypoint[0], keypoint[1], 'ro')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the image\n",
    "\n",
    "# plt.imshow(image)\n",
    "\n",
    "# for keypoint in output[0]:\n",
    "#     plt.plot(keypoint[0], keypoint[1], 'ro')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d23048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7eb2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

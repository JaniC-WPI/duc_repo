{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9c735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10504699904\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch_geometric.nn import SAGEConv, GCNConv\n",
    "import torch.nn.functional as func\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_planning_b1_e50_v8.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e61df6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "n_nodes = 9\n",
    "\n",
    "# parent_path =  \"/home/jc-merlab/Pictures/Data/\"\n",
    "\n",
    "parent_path =  \"/home/schatterjee/lama/kprcnn_panda/\"\n",
    "\n",
    "# root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "root_dir = parent_path + \"occ_new_panda_physical_dataset/\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b271bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fucntion tranforms an input image for diverseifying data for training\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), \n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1), \n",
    "        ], p=1),\n",
    "        A.Resize(640, 480),  # Resize every image to 640x480 after all other transformations\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'),\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )\n",
    "\n",
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_occ_sage\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.8, .1, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0d6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')\n",
    "            bboxes_labels_original.append('joint7')\n",
    "            bboxes_labels_original.append('joint8')\n",
    "            bboxes_labels_original.append('joint9')\n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b20935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = x.cuda()\n",
    "        edge_index = edge_index.cuda()\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_angle(kp1, kp2):\n",
    "    dx = kp2[0] - kp1[0]\n",
    "    dy = kp2[1] - kp1[1]\n",
    "    dx, dy = torch.tensor(dx).to(device), torch.tensor(dy).to(device)\n",
    "    distance = torch.sqrt(dx ** 2 + dy ** 2)\n",
    "    angle = torch.atan2(dy, dx)\n",
    "    return distance, angle\n",
    "\n",
    "def calculate_gt_distances_angles(keypoints_gt):\n",
    "#     print(f\"keypoints_gt shape: {keypoints_gt.shape}\")  # Debug print\n",
    "    batch_size, num_keypoints, num_dims = keypoints_gt.shape\n",
    "    assert num_keypoints == n_nodes and num_dims == 2, \"keypoints_gt must have shape (batch_size, 9, 2)\"\n",
    "    distances_angles = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        batch_distances_angles = torch.zeros((num_keypoints, 4), dtype=torch.float32).to(device)  # Initialize with zeros\n",
    "        \n",
    "        for i in range(num_keypoints):\n",
    "            current_kp = keypoints_gt[b, i]\n",
    "            next_i = (i + 1) % num_keypoints\n",
    "            prev_i = (i - 1 + num_keypoints) % num_keypoints\n",
    "\n",
    "            # Calculate distance and angle to the next keypoint\n",
    "            dist, angle = calculate_distance_angle(current_kp, keypoints_gt[b, next_i])\n",
    "            batch_distances_angles[i, 0] = dist\n",
    "            batch_distances_angles[i, 1] = angle\n",
    "\n",
    "            # Calculate distance and angle to the previous keypoint\n",
    "            dist, angle = calculate_distance_angle(current_kp, keypoints_gt[b, prev_i])\n",
    "            batch_distances_angles[i, 2] = dist\n",
    "            batch_distances_angles[i, 3] = angle\n",
    "\n",
    "        distances_angles.append(batch_distances_angles)\n",
    "\n",
    "    distances_angles = torch.stack(distances_angles)\n",
    "#     print(\"ground truth dist and angles\", distances_angles)\n",
    "    return distances_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88763330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path):\n",
    "        super(KeypointPipeline, self).__init__()  \n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.graph_gcn = GraphGCN(8,1024,4)\n",
    "        \n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.01)[0].tolist()\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "        confidence = output[0]['scores'][high_scores_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs].detach().cpu().numpy()\n",
    "        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0, 0:2])) + [confidence[idx]] + [labels[idx]])\n",
    "        \n",
    "        keypoints = [torch.tensor(kp, dtype=torch.float32).to(device) if not isinstance(kp, torch.Tensor) else kp for kp in keypoints]\n",
    "        keypoints = torch.stack(keypoints).to(device)\n",
    "        \n",
    "        unique_labels, best_keypoint_indices = torch.unique(keypoints[:, 3], return_inverse=True)\n",
    "        best_scores, best_indices = torch.max(keypoints[:, 2].unsqueeze(0) * (best_keypoint_indices == torch.arange(len(unique_labels)).unsqueeze(1).cuda()), dim=1)\n",
    "        keypoints = keypoints[best_indices]\n",
    "        \n",
    "#         print(\"initial predicted keypoints\", keypoints)\n",
    "        \n",
    "        return keypoints\n",
    "    \n",
    "    def fill_missing_keypoints(self, keypoints, image_width, image_height):\n",
    "        keypoints_dict = {int(kp[3]): kp for kp in keypoints}\n",
    "        complete_keypoints = []\n",
    "        labels = [int(kp[3]) for kp in keypoints]\n",
    "        \n",
    "        # Identify missing labels\n",
    "        all_labels = set(range(1, 10))\n",
    "        missing_labels = list(all_labels - set(labels))\n",
    "        missing_labels.sort()\n",
    "        \n",
    "        # Handle consecutive missing labels by placing them at the midpoint or image center\n",
    "        for i in range(1, 10):\n",
    "            if i in keypoints_dict:\n",
    "                complete_keypoints.append(keypoints_dict[i].tolist())\n",
    "            else:\n",
    "                prev_label = i - 1 if i > 1 else 9\n",
    "                next_label = i + 1 if i < 9 else 1\n",
    "                prev_kp = keypoints_dict.get(prev_label, [image_width / 2, image_height / 2, 0, prev_label])\n",
    "                next_kp = keypoints_dict.get(next_label, [image_width / 2, image_height / 2, 0, next_label])\n",
    "                \n",
    "                if next_label in missing_labels:\n",
    "                    next_kp = [image_width / 2, image_height / 2, 0, next_label]\n",
    "                avg_x = (prev_kp[0] + next_kp[0]) / 2\n",
    "                avg_y = (prev_kp[1] + next_kp[1]) / 2\n",
    "                complete_keypoints.append([avg_x, avg_y, 0, i])\n",
    "                \n",
    "#         print(\"filled missing keypoints\", complete_keypoints)\n",
    "        \n",
    "        return torch.tensor(complete_keypoints, dtype=torch.float32).to(device)\n",
    "\n",
    "    def normalize_keypoints(self, keypoints, image_width, image_height):\n",
    "        keypoints[:, 0] = (keypoints[:, 0] - image_width / 2) / (image_width / 2)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] - image_height / 2) / (image_height / 2)\n",
    "        return keypoints\n",
    "    \n",
    "\n",
    "    def keypoints_to_graph(self, keypoints, image_width, image_height):\n",
    "        node_features = []\n",
    "        for i, kp in enumerate(keypoints):\n",
    "            x, y, conf, label = kp\n",
    "            prev_kp = keypoints[i - 1]\n",
    "            next_kp = keypoints[(i + 1) % len(keypoints)]\n",
    "            dist_next, angle_next = calculate_distance_angle([x, y], next_kp[:2])\n",
    "            dist_prev, angle_prev = calculate_distance_angle([x, y], prev_kp[:2])\n",
    "            node_features.append([x, y, conf, label, dist_next, angle_next, dist_prev, angle_prev])\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float32).to(device)\n",
    "        edge_index = torch.tensor([[i, (i + 1) % len(keypoints)] for i in range(len(keypoints))] + \n",
    "                                  [[(i + 1) % len(keypoints), i] for i in range(len(keypoints))], dtype=torch.long).t().contiguous().to(device)\n",
    "        return Data(x=node_features, edge_index=edge_index)\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        keypoint_model_training = self.keypoint_model.training\n",
    "        self.keypoint_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = [self.keypoint_model(img.unsqueeze(0).to(device)) for img in imgs]\n",
    "\n",
    "        self.keypoint_model.train(mode=keypoint_model_training)\n",
    "\n",
    "        batch_labeled_keypoints = [self.process_model_output(output) for output in batch_outputs]\n",
    "\n",
    "        # Fill missing keypoints first and then normalize them\n",
    "        for idx, keypoints in enumerate(batch_labeled_keypoints):\n",
    "            image_width, image_height = 640, 480\n",
    "            filled_keypoints = self.fill_missing_keypoints(keypoints, image_width, image_height)\n",
    "            normalized_keypoints = self.normalize_keypoints(filled_keypoints, image_width, image_height)\n",
    "            batch_labeled_keypoints[idx] = normalized_keypoints\n",
    "       \n",
    "        all_graphs = [self.keypoints_to_graph(keypoints, 640, 480) for keypoints in batch_labeled_keypoints]\n",
    "        all_predictions = [self.graph_gcn(graph.x, graph.edge_index) for graph in all_graphs]\n",
    "\n",
    "        final_predictions = torch.stack(all_predictions)\n",
    "\n",
    "        return final_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fa3b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgnn2d_loss(gt_keypoints, pred_keypoints, gt_distances_next, gt_angles_next, gt_distances_prev, gt_angles_prev, pred_distances_next, pred_angles_next, pred_distances_prev, pred_angles_prev):\n",
    "    keypoints_loss = func.mse_loss(pred_keypoints, gt_keypoints)\n",
    "    prev_distances_loss = func.mse_loss(pred_distances_prev, gt_distances_prev)\n",
    "    prev_angles_loss = func.mse_loss(pred_angles_prev, gt_angles_prev)\n",
    "    next_distances_loss = func.mse_loss(pred_distances_next, gt_distances_next)\n",
    "    next_angles_loss = func.mse_loss(pred_angles_next, gt_angles_next)\n",
    "    return keypoints_loss + prev_distances_loss + prev_angles_loss + next_distances_loss + next_angles_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daf282ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_keypoints(target_dicts):\n",
    "    batch_size = len(target_dicts)\n",
    "    keypoints_list = []\n",
    "    for dict_ in target_dicts:\n",
    "        keypoints = dict_['keypoints'].squeeze(1).to(device)\n",
    "        xy_coords = keypoints[:, :2]\n",
    "        keypoints_list.append(xy_coords)\n",
    "    keypoints_gt = torch.stack(keypoints_list).float().cuda()\n",
    "    return keypoints_gt\n",
    "\n",
    "def reorder_batch_keypoints(batch_keypoints):\n",
    "    batch_size, num_keypoints, num_features = batch_keypoints.shape\n",
    "    reordered_keypoints_batch = []\n",
    "    for i in range(batch_size):\n",
    "        normalized_keypoints = batch_keypoints[i]\n",
    "        reordered_normalized_keypoints = torch.zeros(num_keypoints, 2, device=batch_keypoints.device)\n",
    "        rounded_labels = torch.round(normalized_keypoints[:, -1]).int()\n",
    "        used_indices = []\n",
    "        for label in range(1, 10):\n",
    "            valid_idx = (rounded_labels == label).nonzero(as_tuple=True)[0]\n",
    "            if valid_idx.numel() > 0:\n",
    "                reordered_normalized_keypoints[label - 1] = normalized_keypoints[valid_idx[0], :2]\n",
    "            else:\n",
    "                invalid_idx = ((rounded_labels < 1) | (rounded_labels > 6)).nonzero(as_tuple=True)[0]\n",
    "                invalid_idx = [idx for idx in invalid_idx if idx not in used_indices]\n",
    "                if invalid_idx:\n",
    "                    reordered_normalized_keypoints[label - 1] = normalized_keypoints[invalid_idx[0], :2]\n",
    "                    used_indices.append(invalid_idx[0])\n",
    "        reordered_keypoints_batch.append(reordered_normalized_keypoints)\n",
    "    return torch.stack(reordered_keypoints_batch)\n",
    "\n",
    "def denormalize_keypoints(batch_keypoints, width=640, height=480):\n",
    "    denormalized_keypoints = []\n",
    "    for kp in batch_keypoints:\n",
    "        denormalized_x = (kp[:, 0] * (width / 2)) + (width / 2)\n",
    "        denormalized_y = (kp[:, 1] * (height / 2)) + (height / 2)\n",
    "        denormalized_kp = torch.stack((denormalized_x, denormalized_y), dim=1)\n",
    "        denormalized_keypoints.append(denormalized_kp)\n",
    "    denormalized_keypoints = torch.stack(denormalized_keypoints)\n",
    "    return denormalized_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7750a607",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-22 14:55:29,773] A new study created in memory with name: no-name-5024cf5c-edb9-4dca-986f-2fcc686ac072\n",
      "/tmp/ipykernel_5518/310355109.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
      "/tmp/ipykernel_5518/310355109.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "Copying files: 49670 files [00:04, 11893.96 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.004067364023894021  batch_size:  128  optimizer_name:  SGD weight_decay:  3.968603007057578e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5518/1660099778.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dx, dy = torch.tensor(dx).to(device), torch.tensor(dy).to(device)\n",
      "[W 2024-08-22 15:09:57,612] Trial 0 failed with parameters: {'learning_rate': 0.004067364023894021, 'batch_size': 128, 'optimizer': 'SGD', 'weight_decay': 3.968603007057578e-06} because of the following error: TypeError(\"kgnn2d_loss() missing 7 required positional arguments: 'gt_angles_next', 'gt_distances_prev', 'gt_angles_prev', 'pred_distances_next', 'pred_angles_next', 'pred_distances_prev', and 'pred_angles_prev'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jc-merlab/.local/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_5518/310355109.py\", line 91, in objective\n",
      "    val_loss += kgnn2d_loss(keypoints_gt, denormalized_keypoints, ...).item()\n",
      "TypeError: kgnn2d_loss() missing 7 required positional arguments: 'gt_angles_next', 'gt_distances_prev', 'gt_angles_prev', 'pred_distances_next', 'pred_angles_next', 'pred_distances_prev', and 'pred_angles_prev'\n",
      "[W 2024-08-22 15:09:57,612] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "kgnn2d_loss() missing 7 required positional arguments: 'gt_angles_next', 'gt_distances_prev', 'gt_angles_prev', 'pred_distances_next', 'pred_angles_next', 'pred_distances_prev', and 'pred_angles_prev'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 106\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_val_loss\n\u001b[1;32m    105\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6000\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn [23], line 91\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     88\u001b[0m             reordered_normalized_keypoints \u001b[38;5;241m=\u001b[39m reorder_batch_keypoints(KGNN2D)\n\u001b[1;32m     89\u001b[0m             denormalized_keypoints \u001b[38;5;241m=\u001b[39m denormalize_keypoints(reordered_normalized_keypoints)\n\u001b[0;32m---> 91\u001b[0m             val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mkgnn2d_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeypoints_gt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenormalized_keypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     93\u001b[0m val_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "\u001b[0;31mTypeError\u001b[0m: kgnn2d_loss() missing 7 required positional arguments: 'gt_angles_next', 'gt_distances_prev', 'gt_angles_prev', 'pred_distances_next', 'pred_angles_next', 'pred_distances_prev', and 'pred_angles_prev'"
     ]
    }
   ],
   "source": [
    "\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    num_epochs = 1\n",
    "#     num_epochs = trial.suggest_int('num_epochs', 50, 300)\n",
    "    \n",
    "    # Create data loaders with the suggested batch size\n",
    "\n",
    "    split_folder_path = train_test_split(root_dir)\n",
    "    KEYPOINTS_FOLDER_TRAIN = split_folder_path +\"/train\"\n",
    "    KEYPOINTS_FOLDER_VAL = split_folder_path +\"/val\"\n",
    "    KEYPOINTS_FOLDER_TEST = split_folder_path +\"/test\"\n",
    "\n",
    "    dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "    dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "    dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = KeypointPipeline(weights_path).to(device)\n",
    "    \n",
    "    # Select optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        print('learning_rate:', learning_rate, ' batch_size: ', batch_size, ' optimizer_name: ', optimizer_name, \n",
    "              'weight_decay: ', weight_decay)\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for imgs, target_dicts, _ in data_loader_train:\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                KGNN2D = model(imgs)            \n",
    "                keypoints_gt = process_batch_keypoints(target_dicts)\n",
    "                # print(\"Ground truth keypoints\", keypoints_gt)\n",
    "                reordered_normalized_keypoints = reorder_batch_keypoints(KGNN2D)\n",
    "                denormalized_keypoints = denormalize_keypoints(reordered_normalized_keypoints)\n",
    "                # print(\"Final precicted keypoints\", denormalized_keypoints)\n",
    "                gt_distances_angles = calculate_gt_distances_angles(keypoints_gt)\n",
    "                pred_distances_angles = calculate_gt_distances_angles(denormalized_keypoints)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss_kgnn2d = kgnn2d_loss(keypoints_gt, denormalized_keypoints, gt_distances_angles[:, :, 0], \n",
    "                                      gt_distances_angles[:, :, 1], gt_distances_angles[:, :, 2], \n",
    "                                      gt_distances_angles[:, :, 3], pred_distances_angles[:, :, 0], \n",
    "                                      pred_distances_angles[:, :, 1], pred_distances_angles[:, :, 2], \n",
    "                                      pred_distances_angles[:, :, 3])\n",
    "                \n",
    "                # Backpropagation\n",
    "                scaler.scale(loss_kgnn2d).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss_kgnn2d.item()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, target_dicts, _ in data_loader_val:\n",
    "                imgs = [img.to(device) for img in imgs]\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    KGNN2D = model(imgs)\n",
    "                    keypoints_gt = process_batch_keypoints(target_dicts)\n",
    "                    reordered_normalized_keypoints = reorder_batch_keypoints(KGNN2D)\n",
    "                    denormalized_keypoints = denormalize_keypoints(reordered_normalized_keypoints)\n",
    "                    \n",
    "                    val_loss += kgnn2d_loss(keypoints_gt, denormalized_keypoints, ...).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            trial.report(val_loss, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, timeout=6000)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best trial: {study.best_trial.value}\")\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "model = KeypointPipeline(weights_path).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay']) if best_params['optimizer'] == 'Adam' else optim.SGD(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'], momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038672f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c4f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd8ce8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 49670 files [00:06, 8128.09 files/s] \n",
      "/tmp/ipykernel_82262/1660099778.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dx, dy = torch.tensor(dx).to(device), torch.tensor(dy).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 1969.3689341208142\n",
      "Checkpoint saved to /home/jc-merlab/Pictures/Data/trained_models/gcn_ckpt_noopur/kprcnn_occ_gcn_ckpt_noopur_b1e1.pth\n"
     ]
    }
   ],
   "source": [
    "# model = KeypointPipeline(weights_path)\n",
    "# model = model.to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scaler = GradScaler()\n",
    "\n",
    "# num_epochs = 1\n",
    "# batch_size = 1\n",
    "\n",
    "# split_folder_path = train_test_split(root_dir)\n",
    "# KEYPOINTS_FOLDER_TRAIN = split_folder_path +\"/train\"\n",
    "# KEYPOINTS_FOLDER_VAL = split_folder_path +\"/val\"\n",
    "# KEYPOINTS_FOLDER_TEST = split_folder_path +\"/test\"\n",
    "\n",
    "# dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "# dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "# dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "# data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "# data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "# data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# checkpoint_dir = '/home/jc-merlab/Pictures/Data/trained_models/gcn_ckpt_noopur/'\n",
    "# checkpoint_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')\n",
    "\n",
    "# # Create checkpoint directory if it doesn't exist\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# # Load checkpoint if exists\n",
    "# start_epoch = 0\n",
    "# if os.path.isfile(checkpoint_path):\n",
    "#     checkpoint = torch.load(checkpoint_path)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "#     start_epoch = checkpoint['epoch'] + 1\n",
    "#     print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "\n",
    "\n",
    "# for epoch in range(start_epoch, num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for imgs, target_dicts, _ in data_loader_train:\n",
    "#         imgs = [img.to(device) for img in imgs]\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         with autocast():\n",
    "#             KGNN2D = model(imgs)            \n",
    "#             keypoints_gt = process_batch_keypoints(target_dicts)\n",
    "#             # print(\"Ground truth keypoints\", keypoints_gt)\n",
    "#             reordered_normalized_keypoints = reorder_batch_keypoints(KGNN2D)\n",
    "#             denormalized_keypoints = denormalize_keypoints(reordered_normalized_keypoints)\n",
    "#             # print(\"Final precicted keypoints\", denormalized_keypoints)\n",
    "#             gt_distances_angles = calculate_gt_distances_angles(keypoints_gt)\n",
    "#             pred_distances_angles = calculate_gt_distances_angles(denormalized_keypoints)\n",
    "#             loss_kgnn2d = kgnn2d_loss(keypoints_gt, denormalized_keypoints, gt_distances_angles[:, :, 0], \n",
    "#                                       gt_distances_angles[:, :, 1], gt_distances_angles[:, :, 2], \n",
    "#                                       gt_distances_angles[:, :, 3], pred_distances_angles[:, :, 0], \n",
    "#                                       pred_distances_angles[:, :, 1], pred_distances_angles[:, :, 2], \n",
    "#                                       pred_distances_angles[:, :, 3])\n",
    "            \n",
    "#         scaler.scale(loss_kgnn2d).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         total_loss += loss_kgnn2d.item()\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader_train)}')\n",
    "    \n",
    "# # Save checkpoint every epoch\n",
    "#     if (epoch + 1) % 1 == 0:\n",
    "#         checkpoint_path = os.path.join(checkpoint_dir, f'kprcnn_occ_gcn_ckpt_noopur_b{batch_size}e{epoch+1}.pth')\n",
    "#         torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'scaler_state_dict': scaler.state_dict(),\n",
    "#         }, checkpoint_path)\n",
    "#         print(f'Checkpoint saved to {checkpoint_path}')\n",
    "\n",
    "#     # Save latest checkpoint\n",
    "#     torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'scaler_state_dict': scaler.state_dict(),\n",
    "#     }, checkpoint_path)\n",
    "\n",
    "# # end_time = time.time()\n",
    "\n",
    "# # total_time = end_time - start_time\n",
    "# # print(\"total time\", total_time)\n",
    "\n",
    "# # Save final model\n",
    "# model_save_path = f\"/home/jc-merlab/Pictures/Data/trained_models/kprcnn_gcn_noopur_b{batch_size}_e{num_epochs}.pth\"\n",
    "# torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01066d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5570977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba3f83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

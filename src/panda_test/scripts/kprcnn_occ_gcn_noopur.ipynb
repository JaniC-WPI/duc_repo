{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9c735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10504699904\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch_geometric.nn import SAGEConv, GCNConv\n",
    "import torch.nn.functional as func\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
<<<<<<< HEAD
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_planning_b1_e50_v8.pth'"
=======
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_planning_b1_e50_v8.pth'\n",
    "\n",
    "# weights_path = '/home/schatterjee/lama/kprcnn_panda/trained_models/keypointsrcnn_planning_b1_e100_v4.pth'"
>>>>>>> cca2857761a3b84ed89235f4cec9f95e7e7ca81e
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e61df6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "n_nodes = 9\n",
    "\n",
    "parent_path =  \"/home/jc-merlab/Pictures/Data/\"\n",
    "\n",
    "# parent_path =  \"/home/schatterjee/lama/kprcnn_panda/\"\n",
    "\n",
    "# root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "root_dir = parent_path + \"occ_new_panda_physical_dataset/\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b271bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fucntion tranforms an input image for diverseifying data for training\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), \n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1), \n",
    "        ], p=1),\n",
    "        A.Resize(640, 480),  # Resize every image to 640x480 after all other transformations\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'),\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )\n",
    "\n",
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_occ_sage\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.8, .1, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b0d6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')\n",
    "            bboxes_labels_original.append('joint7')\n",
    "            bboxes_labels_original.append('joint8')\n",
    "            bboxes_labels_original.append('joint9')\n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b20935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = x.cuda()\n",
    "        edge_index = edge_index.cuda()\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "d7b8b8a8",
=======
   "execution_count": 6,
   "id": "ec7ca4ec",
>>>>>>> cca2857761a3b84ed89235f4cec9f95e7e7ca81e
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_angle(kp1, kp2):\n",
    "    dx = kp2[0] - kp1[0]\n",
    "    dy = kp2[1] - kp1[1]\n",
    "    dx, dy = torch.tensor(dx).to(device), torch.tensor(dy).to(device)\n",
    "    distance = torch.sqrt(dx ** 2 + dy ** 2)\n",
    "    angle = torch.atan2(dy, dx)\n",
    "    return distance, angle\n",
    "\n",
    "def calculate_gt_distances_angles(keypoints_gt):\n",
    "#     print(f\"keypoints_gt shape: {keypoints_gt.shape}\")  # Debug print\n",
    "    batch_size, num_keypoints, num_dims = keypoints_gt.shape\n",
    "    assert num_keypoints == n_nodes and num_dims == 2, \"keypoints_gt must have shape (batch_size, 9, 2)\"\n",
    "    distances_angles = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        batch_distances_angles = torch.zeros((num_keypoints, 4), dtype=torch.float32).to(device)  # Initialize with zeros\n",
    "        \n",
    "        for i in range(num_keypoints):\n",
    "            current_kp = keypoints_gt[b, i]\n",
    "            next_i = (i + 1) % num_keypoints\n",
    "            prev_i = (i - 1 + num_keypoints) % num_keypoints\n",
    "\n",
    "            # Calculate distance and angle to the next keypoint\n",
    "            dist, angle = calculate_distance_angle(current_kp, keypoints_gt[b, next_i])\n",
    "            batch_distances_angles[i, 0] = dist\n",
    "            batch_distances_angles[i, 1] = angle\n",
    "\n",
    "            # Calculate distance and angle to the previous keypoint\n",
    "            dist, angle = calculate_distance_angle(current_kp, keypoints_gt[b, prev_i])\n",
    "            batch_distances_angles[i, 2] = dist\n",
    "            batch_distances_angles[i, 3] = angle\n",
    "\n",
    "        distances_angles.append(batch_distances_angles)\n",
    "\n",
    "    distances_angles = torch.stack(distances_angles)\n",
    "#     print(\"ground truth dist and angles\", distances_angles)\n",
    "    return distances_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88763330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path):\n",
    "        super(KeypointPipeline, self).__init__()  \n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.graph_gcn = GraphGCN(8,1024,4)\n",
    "        \n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.01)[0].tolist()\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "        confidence = output[0]['scores'][high_scores_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs].detach().cpu().numpy()\n",
    "        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0, 0:2])) + [confidence[idx]] + [labels[idx]])\n",
    "        \n",
    "        keypoints = [torch.tensor(kp, dtype=torch.float32).to(device) if not isinstance(kp, torch.Tensor) else kp for kp in keypoints]\n",
    "        keypoints = torch.stack(keypoints).to(device)\n",
    "        \n",
    "        unique_labels, best_keypoint_indices = torch.unique(keypoints[:, 3], return_inverse=True)\n",
    "        best_scores, best_indices = torch.max(keypoints[:, 2].unsqueeze(0) * (best_keypoint_indices == torch.arange(len(unique_labels)).unsqueeze(1).cuda()), dim=1)\n",
    "        keypoints = keypoints[best_indices]\n",
    "        \n",
    "#         print(\"initial predicted keypoints\", keypoints)\n",
    "        \n",
    "        return keypoints\n",
    "    \n",
    "    def fill_missing_keypoints(self, keypoints, image_width, image_height):\n",
    "        keypoints_dict = {int(kp[3]): kp for kp in keypoints}\n",
    "        complete_keypoints = []\n",
    "        labels = [int(kp[3]) for kp in keypoints]\n",
    "        \n",
    "        # Identify missing labels\n",
    "        all_labels = set(range(1, 10))\n",
    "        missing_labels = list(all_labels - set(labels))\n",
    "        missing_labels.sort()\n",
    "        \n",
    "        # Handle consecutive missing labels by placing them at the midpoint or image center\n",
    "        for i in range(1, 10):\n",
    "            if i in keypoints_dict:\n",
    "                complete_keypoints.append(keypoints_dict[i].tolist())\n",
    "            else:\n",
    "                prev_label = i - 1 if i > 1 else 9\n",
    "                next_label = i + 1 if i < 9 else 1\n",
    "                prev_kp = keypoints_dict.get(prev_label, [image_width / 2, image_height / 2, 0, prev_label])\n",
    "                next_kp = keypoints_dict.get(next_label, [image_width / 2, image_height / 2, 0, next_label])\n",
    "                \n",
    "                if next_label in missing_labels:\n",
    "                    next_kp = [image_width / 2, image_height / 2, 0, next_label]\n",
    "                avg_x = (prev_kp[0] + next_kp[0]) / 2\n",
    "                avg_y = (prev_kp[1] + next_kp[1]) / 2\n",
    "                complete_keypoints.append([avg_x, avg_y, 0, i])\n",
    "                \n",
    "#         print(\"filled missing keypoints\", complete_keypoints)\n",
    "        \n",
    "        return torch.tensor(complete_keypoints, dtype=torch.float32).to(device)\n",
    "\n",
    "    def normalize_keypoints(self, keypoints, image_width, image_height):\n",
    "        keypoints[:, 0] = (keypoints[:, 0] - image_width / 2) / (image_width / 2)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] - image_height / 2) / (image_height / 2)\n",
    "        return keypoints\n",
    "    \n",
    "\n",
    "    def keypoints_to_graph(self, keypoints, image_width, image_height):\n",
    "        node_features = []\n",
    "        for i, kp in enumerate(keypoints):\n",
    "            x, y, conf, label = kp\n",
    "            prev_kp = keypoints[i - 1]\n",
    "            next_kp = keypoints[(i + 1) % len(keypoints)]\n",
    "            dist_next, angle_next = calculate_distance_angle([x, y], next_kp[:2])\n",
    "            dist_prev, angle_prev = calculate_distance_angle([x, y], prev_kp[:2])\n",
    "            node_features.append([x, y, conf, label, dist_next, angle_next, dist_prev, angle_prev])\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float32).to(device)\n",
    "        edge_index = torch.tensor([[i, (i + 1) % len(keypoints)] for i in range(len(keypoints))] + \n",
    "                                  [[(i + 1) % len(keypoints), i] for i in range(len(keypoints))], dtype=torch.long).t().contiguous().to(device)\n",
    "        return Data(x=node_features, edge_index=edge_index)\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        keypoint_model_training = self.keypoint_model.training\n",
    "        self.keypoint_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = [self.keypoint_model(img.unsqueeze(0).to(device)) for img in imgs]\n",
    "\n",
    "        self.keypoint_model.train(mode=keypoint_model_training)\n",
    "\n",
    "        batch_labeled_keypoints = [self.process_model_output(output) for output in batch_outputs]\n",
    "\n",
    "        # Fill missing keypoints first and then normalize them\n",
    "        for idx, keypoints in enumerate(batch_labeled_keypoints):\n",
    "            image_width, image_height = 640, 480\n",
    "            filled_keypoints = self.fill_missing_keypoints(keypoints, image_width, image_height)\n",
    "            normalized_keypoints = self.normalize_keypoints(filled_keypoints, image_width, image_height)\n",
    "            batch_labeled_keypoints[idx] = normalized_keypoints\n",
    "       \n",
    "        all_graphs = [self.keypoints_to_graph(keypoints, 640, 480) for keypoints in batch_labeled_keypoints]\n",
    "        all_predictions = [self.graph_gcn(graph.x, graph.edge_index) for graph in all_graphs]\n",
    "\n",
    "        final_predictions = torch.stack(all_predictions)\n",
    "\n",
    "        return final_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa3b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgnn2d_loss(gt_keypoints, pred_keypoints, gt_distances_next, gt_angles_next, gt_distances_prev, gt_angles_prev, pred_distances_next, pred_angles_next, pred_distances_prev, pred_angles_prev):\n",
    "    keypoints_loss = func.mse_loss(pred_keypoints, gt_keypoints)\n",
    "    prev_distances_loss = func.mse_loss(pred_distances_prev, gt_distances_prev)\n",
    "    prev_angles_loss = func.mse_loss(pred_angles_prev, gt_angles_prev)\n",
    "    next_distances_loss = func.mse_loss(pred_distances_next, gt_distances_next)\n",
    "    next_angles_loss = func.mse_loss(pred_angles_next, gt_angles_next)\n",
    "    return keypoints_loss + prev_distances_loss + prev_angles_loss + next_distances_loss + next_angles_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf282ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_keypoints(target_dicts):\n",
    "    batch_size = len(target_dicts)\n",
    "    keypoints_list = []\n",
    "    for dict_ in target_dicts:\n",
    "        keypoints = dict_['keypoints'].squeeze(1).to(device)\n",
    "        xy_coords = keypoints[:, :2]\n",
    "        keypoints_list.append(xy_coords)\n",
    "    keypoints_gt = torch.stack(keypoints_list).float().cuda()\n",
    "    return keypoints_gt\n",
    "\n",
    "def reorder_batch_keypoints(batch_keypoints):\n",
    "    batch_size, num_keypoints, num_features = batch_keypoints.shape\n",
    "    reordered_keypoints_batch = []\n",
    "    for i in range(batch_size):\n",
    "        normalized_keypoints = batch_keypoints[i]\n",
    "        reordered_normalized_keypoints = torch.zeros(num_keypoints, 2, device=batch_keypoints.device)\n",
    "        rounded_labels = torch.round(normalized_keypoints[:, -1]).int()\n",
    "        used_indices = []\n",
    "        for label in range(1, 10):\n",
    "            valid_idx = (rounded_labels == label).nonzero(as_tuple=True)[0]\n",
    "            if valid_idx.numel() > 0:\n",
    "                reordered_normalized_keypoints[label - 1] = normalized_keypoints[valid_idx[0], :2]\n",
    "            else:\n",
    "                invalid_idx = ((rounded_labels < 1) | (rounded_labels > 6)).nonzero(as_tuple=True)[0]\n",
    "                invalid_idx = [idx for idx in invalid_idx if idx not in used_indices]\n",
    "                if invalid_idx:\n",
    "                    reordered_normalized_keypoints[label - 1] = normalized_keypoints[invalid_idx[0], :2]\n",
    "                    used_indices.append(invalid_idx[0])\n",
    "        reordered_keypoints_batch.append(reordered_normalized_keypoints)\n",
    "    return torch.stack(reordered_keypoints_batch)\n",
    "\n",
    "def denormalize_keypoints(batch_keypoints, width=640, height=480):\n",
    "    denormalized_keypoints = []\n",
    "    for kp in batch_keypoints:\n",
    "        denormalized_x = (kp[:, 0] * (width / 2)) + (width / 2)\n",
    "        denormalized_y = (kp[:, 1] * (height / 2)) + (height / 2)\n",
    "        denormalized_kp = torch.stack((denormalized_x, denormalized_y), dim=1)\n",
    "        denormalized_keypoints.append(denormalized_kp)\n",
    "    denormalized_keypoints = torch.stack(denormalized_keypoints)\n",
    "    return denormalized_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "038672f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jc-merlab/Pictures/Data/occ_new_panda_physical_dataset/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m total_keypoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[1;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m split_folder_path \u001b[38;5;241m=\u001b[39m train_test_split(root_dir)\n\u001b[1;32m     13\u001b[0m KEYPOINTS_FOLDER_TRAIN \u001b[38;5;241m=\u001b[39m split_folder_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m KEYPOINTS_FOLDER_VAL \u001b[38;5;241m=\u001b[39m split_folder_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/val\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn [3], line 21\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(src_dir)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfolders exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst_dir_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(dst_dir_anno)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m jpgfile \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39miglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(src_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jc-merlab/Pictures/Data/occ_new_panda_physical_dataset/images'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define global variables\n",
    "total_keypoints = 9\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "split_folder_path = train_test_split(root_dir)\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = split_folder_path + \"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = split_folder_path + \"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = split_folder_path + \"/test\"\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_val = ClassDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters for tuning\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 20, 100)\n",
    "    momentum = trial.suggest_uniform('momentum', 0.8, 0.99)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    step_size = trial.suggest_int('step_size', 5, 20)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.1, 0.5)\n",
    "\n",
    "    # Create data loaders\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = get_model(num_keypoints=total_keypoints)\n",
    "    model.to(device)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    v = 3  # Versioning for saving model\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=1000)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Validation loop\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for imgs, target_dicts, _ in data_loader_val:\n",
    "                imgs = [img.to(device) for img in imgs]\n",
    "                output = model(imgs)\n",
    "                loss = compute_loss(output, target_dicts)  # Replace with actual loss function\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(data_loader_val)\n",
    "\n",
    "        # Save the best model checkpoint\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            PATH = f\"/home/jc-merlab/Pictures/Data/trained_models/kprcnn_best_b{batch_size}_e{epoch}_v{v}.pth\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            v += 1\n",
    "\n",
    "        # Optuna pruning - if the trial should be pruned\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# Step 3: Run the Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20, timeout=7200)  # Adjust n_trials and timeout as needed\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best trial: {study.best_trial.value}\")\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)\n",
    "\n",
    "# Step 4: Use the best hyperparameters to train and save the final model\n",
    "best_params = study.best_trial.params\n",
    "batch_size = best_params['batch_size']\n",
    "learning_rate = best_params['learning_rate']\n",
    "num_epochs = best_params['num_epochs']\n",
    "momentum = best_params['momentum']\n",
    "weight_decay = best_params['weight_decay']\n",
    "step_size = best_params['step_size']\n",
    "gamma = best_params['gamma']\n",
    "\n",
    "# Re-initialize data loaders with the best batch size\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Re-initialize the model and optimizer with best hyperparameters\n",
    "model = get_model(num_keypoints=total_keypoints)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training the final model with the best hyperparameters\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=1000)\n",
    "    lr_scheduler.step()\n",
    "    PATH = f\"/home/jc-merlab/Pictures/Data/trained_models/kprcnn_final_b{batch_size}_e{epoch}_v{v}.pth\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    v += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c4f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd8ce8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 49670 files [00:06, 8128.09 files/s] \n",
      "/tmp/ipykernel_82262/1660099778.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dx, dy = torch.tensor(dx).to(device), torch.tensor(dy).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 1969.3689341208142\n",
      "Checkpoint saved to /home/jc-merlab/Pictures/Data/trained_models/gcn_ckpt_noopur/kprcnn_occ_gcn_ckpt_noopur_b1e1.pth\n"
     ]
    }
   ],
   "source": [
    "# model = KeypointPipeline(weights_path)\n",
    "# model = model.to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scaler = GradScaler()\n",
    "\n",
    "# num_epochs = 1\n",
    "# batch_size = 1\n",
    "\n",
    "# split_folder_path = train_test_split(root_dir)\n",
    "# KEYPOINTS_FOLDER_TRAIN = split_folder_path +\"/train\"\n",
    "# KEYPOINTS_FOLDER_VAL = split_folder_path +\"/val\"\n",
    "# KEYPOINTS_FOLDER_TEST = split_folder_path +\"/test\"\n",
    "\n",
    "# dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "# dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "# dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "# data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "# data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "# data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# checkpoint_dir = '/home/jc-merlab/Pictures/Data/trained_models/gcn_ckpt_noopur/'\n",
    "# checkpoint_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')\n",
    "\n",
    "# # Create checkpoint directory if it doesn't exist\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# # Load checkpoint if exists\n",
    "# start_epoch = 0\n",
    "# if os.path.isfile(checkpoint_path):\n",
    "#     checkpoint = torch.load(checkpoint_path)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "#     start_epoch = checkpoint['epoch'] + 1\n",
    "#     print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "\n",
    "\n",
    "# for epoch in range(start_epoch, num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for imgs, target_dicts, _ in data_loader_train:\n",
    "#         imgs = [img.to(device) for img in imgs]\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         with autocast():\n",
    "#             KGNN2D = model(imgs)            \n",
    "#             keypoints_gt = process_batch_keypoints(target_dicts)\n",
    "#             # print(\"Ground truth keypoints\", keypoints_gt)\n",
    "#             reordered_normalized_keypoints = reorder_batch_keypoints(KGNN2D)\n",
    "#             denormalized_keypoints = denormalize_keypoints(reordered_normalized_keypoints)\n",
    "#             # print(\"Final precicted keypoints\", denormalized_keypoints)\n",
    "#             gt_distances_angles = calculate_gt_distances_angles(keypoints_gt)\n",
    "#             pred_distances_angles = calculate_gt_distances_angles(denormalized_keypoints)\n",
    "#             loss_kgnn2d = kgnn2d_loss(keypoints_gt, denormalized_keypoints, gt_distances_angles[:, :, 0], \n",
    "#                                       gt_distances_angles[:, :, 1], gt_distances_angles[:, :, 2], \n",
    "#                                       gt_distances_angles[:, :, 3], pred_distances_angles[:, :, 0], \n",
    "#                                       pred_distances_angles[:, :, 1], pred_distances_angles[:, :, 2], \n",
    "#                                       pred_distances_angles[:, :, 3])\n",
    "            \n",
    "#         scaler.scale(loss_kgnn2d).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         total_loss += loss_kgnn2d.item()\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader_train)}')\n",
    "    \n",
    "# # Save checkpoint every epoch\n",
    "#     if (epoch + 1) % 1 == 0:\n",
    "#         checkpoint_path = os.path.join(checkpoint_dir, f'kprcnn_occ_gcn_ckpt_noopur_b{batch_size}e{epoch+1}.pth')\n",
    "#         torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'scaler_state_dict': scaler.state_dict(),\n",
    "#         }, checkpoint_path)\n",
    "#         print(f'Checkpoint saved to {checkpoint_path}')\n",
    "\n",
    "#     # Save latest checkpoint\n",
    "#     torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'scaler_state_dict': scaler.state_dict(),\n",
    "#     }, checkpoint_path)\n",
    "\n",
    "# # end_time = time.time()\n",
    "\n",
    "# # total_time = end_time - start_time\n",
    "# # print(\"total time\", total_time)\n",
    "\n",
    "# # Save final model\n",
    "# model_save_path = f\"/home/jc-merlab/Pictures/Data/trained_models/kprcnn_gcn_noopur_b{batch_size}_e{num_epochs}.pth\"\n",
    "# torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01066d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5570977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba3f83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

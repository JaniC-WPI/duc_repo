{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d908ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d479321",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fucntion tranforms an input image for diverseifying data for training\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), \n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1), \n",
    "        ], p=1),\n",
    "        A.Resize(640, 480),  # Resize every image to 640x480 after all other transformations\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'),\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717193a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to split the dataset into train, test and validation folder.\n",
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200017a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo \n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the arm\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_kp')\n",
    "            bboxes_labels_original.append('joint1')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "\n",
    "        if self.transform:\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "            \n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj):\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original  \n",
    "\n",
    "            # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]   \n",
    "#         labels = [1, 2, 3, 4]\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b8f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" \n",
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "# print(batch[2])\n",
    "\n",
    "# print(\"Original targets:\\n\", batch[3], \"\\n\\n\")\n",
    "# print(\"Transformed targets:\\n\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize how the transformed data looks \n",
    "\n",
    "keypoints_classes_ids2names = {0: 'base_joint', 1: 'joint2', 2: 'joint3', 3: 'joint4', 4: 'joint5', 5: 'joint6',\\\n",
    "                              6:'joint7', 7:'joint8', 8:'panda_finger_1', 9:'panda_finger_2'}\n",
    "\n",
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):\n",
    "    fontsize = 18\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "    \n",
    "    for idx, kps in enumerate(keypoints):\n",
    "        for kp in kps:\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 2, (255,0,0), 10)\n",
    "#         image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(40,40))\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "    else:\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)\n",
    "        \n",
    "        print(keypoints_original)\n",
    "        for idx, kps in enumerate(keypoints_original):\n",
    "            print(idx)\n",
    "            print(kps)\n",
    "            for kp in kps:\n",
    "                print(kp)\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 5, (255,0,0), 2)\n",
    "#             image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "\n",
    "        ax[0].imshow(image_original)\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "\n",
    "        ax[1].imshow(image)\n",
    "        ax[1].set_title('Transformed image', fontsize=fontsize)\n",
    "        \n",
    "        return None\n",
    "        \n",
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "# for kps in batch1[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "#     keypoints.append([kp[:2] for kp in [kps]])\n",
    "    \n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "# for kps in batch1[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "#     keypoints_original.append([kp[:2] for kp in [kps]])\n",
    "    \n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])\n",
    "\n",
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab9f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.fc = nn.Linear(in_channels, out_channels)\n",
    "        nn.init.xavier_uniform_(self.fc.weight) # Adding Xavier Uniform Initialization for weight\n",
    "        nn.init.zeros_(self.fc.bias) # Initializing bias to zeros\n",
    "\n",
    "    def forward(self, x, adj):  \n",
    "        out = torch.matmul(adj.cuda(), x.cuda())\n",
    "#         print(\"bmm output, how does it look\", out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.keypoint_rcnn = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=True, num_keypoints=6, num_classes=7)\n",
    "        self.graph_conv1 = GraphConv(2, 128)\n",
    "#         self.graph_conv2 = GraphConv(128, 128)\n",
    "        self.graph_conv2 = GraphConv(128, 2)\n",
    "    \n",
    "    def complete_missing_keypoints(self, keypoints, labels, num_expected_keypoints=6):\n",
    "\n",
    "        detected_kps = keypoints.shape[0]\n",
    "        # Check if all keypoints are detected\n",
    "        if detected_kps == num_expected_keypoints:\n",
    "            return keypoints\n",
    "        \n",
    "        # Create a placeholder tensor for keypoints with the correct shape\n",
    "        ordered_keypoints = torch.zeros((num_expected_keypoints, 3), device=keypoints.device)\n",
    "\n",
    "        # If some keypoints are detected, compute their average position\n",
    "        average_kp = torch.mean(keypoints, dim=0, keepdim=True)\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            ordered_keypoints[label - 1] = keypoints[i]\n",
    "\n",
    "        # Fill in the missing keypoints with the average position\n",
    "        missing_indices = (torch.sum(ordered_keypoints, dim=1) == 0).nonzero(as_tuple=True)[0]\n",
    "        ordered_keypoints[missing_indices] = average_kp\n",
    "\n",
    "        return ordered_keypoints\n",
    "\n",
    "    def forward(self, images, adj_matrix=None, targets=None, train=False):\n",
    "        if train:\n",
    "            output = self.keypoint_rcnn(images, targets)\n",
    "            return output\n",
    "    \n",
    "        else:\n",
    "            all_keypoints = []\n",
    "            with torch.no_grad():\n",
    "                self.keypoint_rcnn.eval()\n",
    "                output = self.keypoint_rcnn(images)\n",
    "                self.keypoint_rcnn.train()\n",
    "                \n",
    "                keypoints = output[0]['keypoints'].detach().cpu().numpy()\n",
    "                kp_score = output[0]['keypoints_scores'].detach().cpu().numpy()\n",
    "                labels = output[0]['labels'].detach().cpu().numpy()\n",
    "                unique_labels = list(set(labels))\n",
    "                scores = output[0]['scores'].detach().cpu().numpy()\n",
    "                print(\"labels\", unique_labels)\n",
    "                kps = []\n",
    "                kp_scores = []\n",
    "                ulabels = []\n",
    "\n",
    "                for label in unique_labels:\n",
    "                    indices = [j for j, x in enumerate(labels) if x == label]\n",
    "                    scores_for_label = [scores[j] for j in indices]\n",
    "                    max_score_index = indices[scores_for_label.index(max(scores_for_label))]\n",
    "                    kp_score_label = kp_score[max_score_index].tolist()\n",
    "                    kps.append(keypoints[max_score_index][kp_score_label.index(max(kp_score_label))])\n",
    "                    ulabels.append(label)\n",
    "\n",
    "                kps = [torch.tensor(kp, dtype=torch.float32) for kp in kps]\n",
    "                if not kps:\n",
    "                    default_value = torch.tensor([[320, 240, 1]], dtype=torch.float32, device=images[i].device)\n",
    "                    keypoints = default_value.repeat(6, 1)\n",
    "                else:\n",
    "                    keypoints = torch.stack(kps)\n",
    "                        \n",
    "                print(\"kp before placeholder\", keypoints)\n",
    "                keypoints = self.complete_missing_keypoints(keypoints, unique_labels)[:, 0:2]\n",
    "                print(\"kp after placeholder\", keypoints)\n",
    "                keypoints = self.graph_conv1(keypoints, adj_matrix)\n",
    "                keypoints = nn.functional.relu(keypoints)\n",
    "                keypoints = self.graph_conv2(keypoints, adj_matrix)\n",
    "#                 keypoints = nn.functional.relu(keypoints)\n",
    "                print(\"kp after graph\", keypoints)\n",
    "                print(\"denormalized kp after graph\", denormalize_keypoints(keypoints))\n",
    "            print(\"All keypoints\", keypoints)\n",
    "\n",
    "            return keypoints\n",
    "\n",
    "\n",
    "#                 for i in range(len(images)):  # Process each image in the batch\n",
    "#                     print(\"image in sample\", i)\n",
    "#                     keypoints = output[i]['keypoints'].detach().cpu().numpy()\n",
    "#                     kp_score = output[i]['keypoints_scores'].detach().cpu().numpy()\n",
    "#                     labels = output[i]['labels'].detach().cpu().numpy()\n",
    "#                     unique_labels = list(set(labels))\n",
    "#                     scores = output[i]['scores'].detach().cpu().numpy()\n",
    "#                     print(\"labels\", unique_labels)\n",
    "\n",
    "#                     kps = []\n",
    "#                     kp_scores = []\n",
    "#                     ulabels = []\n",
    "\n",
    "#                     for label in unique_labels:\n",
    "#                         indices = [j for j, x in enumerate(labels) if x == label]\n",
    "#                         scores_for_label = [scores[j] for j in indices]\n",
    "#                         max_score_index = indices[scores_for_label.index(max(scores_for_label))]\n",
    "#                         kp_score_label = kp_score[max_score_index].tolist()\n",
    "#                         kps.append(keypoints[max_score_index][kp_score_label.index(max(kp_score_label))])\n",
    "#                         ulabels.append(label)\n",
    "\n",
    "#                     kps = [torch.tensor(kp, dtype=torch.float32) for kp in kps]\n",
    "#                     if not kps:\n",
    "#                         default_value = torch.tensor([[320, 240, 1]], dtype=torch.float32, device=images[i].device)\n",
    "#                         keypoints = default_value.repeat(6, 1)\n",
    "#                     else:\n",
    "#                         keypoints = torch.stack(kps)\n",
    "                        \n",
    "#                     print(\"kp before placeholder\", keypoints)\n",
    "#                     keypoints = self.complete_missing_keypoints(keypoints, unique_labels)[:, 0:2]\n",
    "#                     print(\"kp after placeholder\", keypoints)\n",
    "#                     keypoints = self.graph_conv1(keypoints, adj_matrix)\n",
    "#                     keypoints = nn.functional.relu(keypoints)\n",
    "#                     keypoints = self.graph_conv2(keypoints, adj_matrix)\n",
    "# #                     keypoints = nn.functional.relu(keypoints)\n",
    "#                     print(\"kp after graph\", keypoints)\n",
    "#                     print(\"denormalized kp after graph\", denormalize_keypoints(keypoints))\n",
    "# #                     keypoints = self.graph_conv3(keypoints, adj_matrix)\n",
    "#                     all_keypoints.append(denormalize_keypoints(keypoints))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b41cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keypoints(keypoints):\n",
    "    normalized_keypoints = keypoints.clone()\n",
    "    normalized_keypoints[..., 0] = keypoints[..., 0] / 320.0 - 1.0  # x\n",
    "    normalized_keypoints[..., 1] = keypoints[..., 1] / 240.0 - 1.0  # y\n",
    "    return normalized_keypoints\n",
    "\n",
    "def denormalize_keypoints(normalized_keypoints):\n",
    "    keypoints = normalized_keypoints.clone()\n",
    "    keypoints[..., 0] = (normalized_keypoints[..., 0] + 1.0) * 320.0  # x\n",
    "    keypoints[..., 1] = (normalized_keypoints[..., 1] + 1.0) * 240.0  # y\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(predicted_keypoints, gt_keypoints):\n",
    "    mse_loss = nn.MSELoss()\n",
    "#     l1_loss = nn.L1Loss()\n",
    "#     batch_size = adj_matrix.size(0)\n",
    "    \n",
    "    keypoint_loss = mse_loss(predicted_keypoints, gt_keypoints)    \n",
    "    print(\"keypoint_loss\", keypoint_loss)\n",
    "    print(\"predicted_keypoints.shape\",predicted_keypoints)\n",
    "    print(\"ground_truth_keypoints.shape\", gt_keypoints)\n",
    "    \n",
    "    \n",
    "# #     # Adjusting adjacency matrix to match batch size\n",
    "# #     batch_adj_matrix = adj_matrix.unsqueeze(0).repeat(diff.shape[0], 1, 1)\n",
    "#     # Compute graph loss by only considering the edges present in the adjacency matrix\n",
    "#     graph_loss = torch.sum(diff.unsqueeze(-1) * adj_matrix)\n",
    "#     #     box_loss = mse_loss(predicted_boxes, gt_boxes)\n",
    "#     print(\"graph_loss\", graph_loss)\n",
    "#     print(\"loss keypoint\", loss_keypoint)\n",
    "    \n",
    "#     total_loss = keypoint_loss + 0.1 * box_loss + 0.01 * graph_loss + loss_keypoint\n",
    "    total_loss = keypoint_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_loss(predicted_keypoints, gt_keypoints, adj_matrix, loss_keypoint):\n",
    "# #     mse_loss = nn.MSELoss()\n",
    "#     l1_loss = nn.L1Loss()\n",
    "#     batch_size = adj_matrix.size(0)\n",
    "    \n",
    "# #     normalized_predicted_keypoints = torch.clamp(predicted_keypoints, -1.0, 1.0)\n",
    "# #     normalized_gt_keypoints = torch.clamp(gt_keypoints, -1.0, 1.0)\n",
    "#     normalized_predicted_keypoints = normalize_keypoints(predicted_keypoints)\n",
    "#     normalized_gt_keypoints = normalize_keypoints(gt_keypoints)\n",
    "#     keypoint_loss = l1_loss(normalized_predicted_keypoints, normalized_gt_keypoints)    \n",
    "#     print(\"keypoint_loss\", keypoint_loss)\n",
    "#     print(\"predicted_keypoints.shape\",normalized_predicted_keypoints)\n",
    "#     print(\"ground_truth_keypoints.shape\", normalized_gt_keypoints)\n",
    "#     # Pairwise distances for all adjacent keypoints\n",
    "#     pairwise_distances_pred_adj = torch.sqrt(torch.sum((normalized_predicted_keypoints[:, :-1] - normalized_predicted_keypoints[:, 1:])**2, dim=2))\n",
    "#     pairwise_distances_gt_adj = torch.sqrt(torch.sum((normalized_gt_keypoints[:, :-1] - normalized_gt_keypoints[:, 1:])**2, dim=2))\n",
    "    \n",
    "#     print(\"pairwise all kps pred\", pairwise_distances_pred_adj)\n",
    "#     print(\"pairwise all kps gt\", pairwise_distances_gt_adj)\n",
    "#     # Distance between the last and the first keypoints\n",
    "#     pairwise_distances_pred_last_first = torch.sqrt(torch.sum((normalized_predicted_keypoints[:, -1] - normalized_predicted_keypoints[:, 0])**2, dim=1)).unsqueeze(1)\n",
    "# #     pairwise_distances_pred_last_first = torch.sqrt(torch.sum((predicted_keypoints[:, -1] - predicted_keypoints[:, 0])**2, dim=1))\n",
    "#     pairwise_distances_gt_last_first = torch.sqrt(torch.sum((normalized_gt_keypoints[:, -1] - normalized_gt_keypoints[:, 0])**2, dim=1)).unsqueeze(1)\n",
    "# #     pairwise_distances_gt_last_first = torch.sqrt(torch.sum((gt_keypoints[:, -1] - gt_keypoints[:, 0])**2, dim=1))\n",
    "#     print(\"pairwise first and last kps pred\", pairwise_distances_pred_last_first)\n",
    "#     print(\"pairwise first and last kps gt\", pairwise_distances_gt_last_first)\n",
    "#     # Concatenate the distances\n",
    "#     pairwise_distances_pred = torch.cat((pairwise_distances_pred_adj, pairwise_distances_pred_last_first), dim=1)\n",
    "#     pairwise_distances_gt = torch.cat((pairwise_distances_gt_adj, pairwise_distances_gt_last_first), dim=1)\n",
    "\n",
    "#     diff = torch.abs(pairwise_distances_pred - pairwise_distances_gt)\n",
    "#     print(pairwise_distances_pred)\n",
    "#     print(pairwise_distances_gt)\n",
    "#     print(diff)\n",
    "    \n",
    "# #     # Adjusting adjacency matrix to match batch size\n",
    "# #     batch_adj_matrix = adj_matrix.unsqueeze(0).repeat(diff.shape[0], 1, 1)\n",
    "#     # Compute graph loss by only considering the edges present in the adjacency matrix\n",
    "#     graph_loss = torch.sum(diff.unsqueeze(-1) * adj_matrix)\n",
    "#     #     box_loss = mse_loss(predicted_boxes, gt_boxes)\n",
    "#     print(\"graph_loss\", graph_loss)\n",
    "#     print(\"loss keypoint\", loss_keypoint)\n",
    "    \n",
    "# #     total_loss = keypoint_loss + 0.1 * box_loss + 0.01 * graph_loss + loss_keypoint\n",
    "#     total_loss = keypoint_loss + graph_loss + 0.01*loss_keypoint\n",
    "    \n",
    "#     return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(num_keypoints):\n",
    "    # Initialize a zero matrix\n",
    "    adj_matrix = torch.zeros((num_keypoints, num_keypoints))\n",
    "    \n",
    "    # Fill the diagonal above the main diagonal with ones\n",
    "    for i in range(num_keypoints-1):\n",
    "        adj_matrix[i, i + 1] = 1\n",
    "        \n",
    "    # Connect the last keypoint to the first keypoint\n",
    "    adj_matrix[num_keypoints-1, 0] = 1\n",
    "        \n",
    "    print(\"adj\", adj_matrix)\n",
    "        \n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "model = CombinedModel()\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Create adjacency matrix\n",
    "num_keypoints = 6\n",
    "adj_matrix = create_adjacency_matrix(num_keypoints).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a1556",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 2\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_val = ClassDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize the GradScaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "top_5_models = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):  # for 50 epochs\n",
    "    for batch_idx, batch in enumerate(data_loader_train):\n",
    "        images, targets = batch \n",
    "        \n",
    "        imgs = [img.to(device) for img in images]  # Create list of images\n",
    "        \n",
    "        images = torch.stack(images).to(device)        \n",
    "        # Move targets to GPU\n",
    "        for target in targets:\n",
    "            for key, val in target.items():\n",
    "                target[key] = val.cuda()        \n",
    "        \n",
    "#         if len(images) != batch_size:\n",
    "#             continue\n",
    "        print(\"The training is continued\")\n",
    "        losses = []\n",
    "        with autocast():\n",
    "            output_train = model(images, adj_matrix=adj_matrix, targets=targets, train=True)\n",
    "    \n",
    "            for i in range(len(imgs)):\n",
    "                img = imgs[i].unsqueeze(0) \n",
    "                ground_truth_keypoints = targets[i]['keypoints'].to(device).squeeze()\n",
    "                print(\"ground truth keypoints shape\", ground_truth_keypoints.shape)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                # automatic precision for forward pass\n",
    "                predicted_keypoints = model(img, train=False)            \n",
    "\n",
    "                    # Compute loss\n",
    "                loss = custom_loss(predicted_keypoints, ground_truth_keypoints)\n",
    "                losses.append(loss)\n",
    "\n",
    "                print(loss.device)\n",
    "                \n",
    "        loss_keypoint = output_train['loss_keypoint']\n",
    "        print(\"loss keypoint\", loss_keypoint)\n",
    "        \n",
    "        loss = loss_keypoint + torch.stack(losses).sum()\n",
    "        \n",
    "        print(loss)\n",
    "\n",
    "        # Scale the loss and backpropagate\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scheduler.step()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        v = 5\n",
    "        # Check if the current model should be saved as a top model\n",
    "        if len(top_5_models) < 5 or total_loss.item() < max(top_5_models, key=lambda x: x[0])[0]:\n",
    "            # Save the model state and loss\n",
    "            model_state = {\n",
    "                'epoch': epoch,\n",
    "                'complete_model': model,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "            }\n",
    "            top_5_models.append((loss.item(), model_state))\n",
    "\n",
    "            # Sort the list based on loss (ascending order)\n",
    "            top_5_models.sort(key=lambda x: x[0])\n",
    "\n",
    "            # If there are more than 5 models, remove the one with the highest loss\n",
    "            if len(top_5_models) > 5:\n",
    "                top_5_models.pop()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx + 1}/{len(data_loader_train)}, Loss: {loss.item()}\")\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "    \n",
    "# After all epochs, save the top 5 models to disk\n",
    "for idx, (_, model_state) in enumerate(top_5_models):\n",
    "    torch.save(model_state, f'/home/jc-merlab/Pictures/Data/trained_models/best_gnn_model_b{batch_size}_e{num_epochs}_{idx+1}_v{v}.pth')\n",
    "    \n",
    "                \n",
    "                \n",
    "        \n",
    "#         # Move images to GPU\n",
    "#         images = torch.stack(images).cuda()\n",
    "        # Move targets to GPU\n",
    "#         for target in targets:\n",
    "#             for key, val in target.items():\n",
    "#                 target[key] = val.cuda()\n",
    "        \n",
    "#         ground_truth_keypoints = [target['keypoints'] for target in targets]\n",
    "#         ground_truth_boxes = [target['boxes'] for target in targets]\n",
    "        \n",
    "#         # Assuming you want all images to be of size [3, 640, 480]\n",
    "# #         desired_size = (640, 480)  \n",
    "\n",
    "#         # Resize all images to the desired size\n",
    "# #         resized_images = [F.resize(img, desired_size) for img in images]\n",
    "\n",
    "#         ground_truth_keypoints = torch.stack(ground_truth_keypoints).squeeze()[:, :, 0:2]\n",
    "#         print(\"ground_truth_keypoints\", ground_truth_keypoints)\n",
    "#         ground_truth_boxes = torch.stack(ground_truth_boxes)[:, :, 0:2]\n",
    "        \n",
    "        \n",
    "\n",
    "#         Create a batched adjacency matrix with the same batch size\n",
    "#         batch_adj_matrix = adj_matrix.repeat(batch_size, 1, 1)\n",
    "#         batch_adj_matrix = adj_matrix\n",
    "#         print(batch_adj_matrix.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # Forward pass for training\n",
    "#         output_train = model(images, adj_matrix=batch_adj_matrix, targets=targets, train=True)\n",
    "#         print(\"Output keypoints shape\", output_train.keys())\n",
    "        \n",
    "#         #Forward pass for loss\n",
    "#         predicted_keypoints = model(images, adj_matrix=batch_adj_matrix, train=False)\n",
    "        \n",
    "        \n",
    "#         print(\"predicted keypoints\", predicted_keypoints.shape)\n",
    "                \n",
    "#         loss_keypoint = output_train['loss_keypoint']\n",
    "        \n",
    "#         # Compute loss and backpropagate\n",
    "#         loss = custom_loss(predicted_keypoints, ground_truth_keypoints, \n",
    "#                            adj_matrix=batch_adj_matrix, loss_keypoint=loss_keypoint)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Automatic mixed precision for forward pass\n",
    "#         with autocast():\n",
    "#             output_train = model(images, adj_matrix=batch_adj_matrix, targets=targets, train=True)\n",
    "# #             print(\"Output keypoints shape\", output_train.keys())\n",
    "            \n",
    "#             predicted_keypoints = model(images, adj_matrix=batch_adj_matrix, train=False)\n",
    "         \n",
    "# #             print(\"predicted keypoints\", predicted_keypoints)\n",
    "\n",
    "#             loss_keypoint = output_train['loss_keypoint']\n",
    "\n",
    "#             # Compute loss\n",
    "#             loss = custom_loss(predicted_keypoints, ground_truth_keypoints, adj_matrix=batch_adj_matrix, loss_keypoint=loss_keypoint)\n",
    "            \n",
    "#             print(loss.device)\n",
    "        \n",
    "#         # Scale the loss and backpropagate\n",
    "#         scaler.scale(loss).backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#         scheduler.step()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "        \n",
    "#         # Check if the current model should be saved as a top model\n",
    "#         if len(top_5_models) < 5 or loss.item() < max(top_5_models, key=lambda x: x[0])[0]:\n",
    "#             # Save the model state and loss\n",
    "#             model_state = {\n",
    "#                 'epoch': epoch,\n",
    "#                 'complete_model': model,\n",
    "# #                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'loss': loss.item(),\n",
    "#             }\n",
    "#             top_5_models.append((loss.item(), model_state))\n",
    "\n",
    "#             # Sort the list based on loss (ascending order)\n",
    "#             top_5_models.sort(key=lambda x: x[0])\n",
    "\n",
    "#             # If there are more than 5 models, remove the one with the highest loss\n",
    "#             if len(top_5_models) > 5:\n",
    "#                 top_5_models.pop()\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx + 1}/{len(data_loader_train)}, Loss: {loss.item()}\")\n",
    "        \n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def predict_keypoints(model_path, image_path, transform=None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load the saved model\n",
    "    load_model_state = torch.load(model_path)\n",
    "    model = load_model_state['complete_model'].to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "        \n",
    "    # Convert PIL image to tensor and expand dimensions to represent a batch of size 1\n",
    "    image = torch.tensor(np.array(image)).permute(2, 0, 1).float().unsqueeze(0).to(device)\n",
    "\n",
    "    # Create an adjacency matrix\n",
    "    num_keypoints = 6\n",
    "    adj_matrix = create_adjacency_matrix(num_keypoints).unsqueeze(0).to(device)\n",
    "\n",
    "    # Predict keypoints\n",
    "    with torch.no_grad():\n",
    "        keypoints = model(image, adj_matrix=adj_matrix, train=False)\n",
    "    \n",
    "    print(keypoints.shape)\n",
    "    \n",
    "    # Convert tensor to numpy array for returning\n",
    "    return keypoints.cpu().numpy()\n",
    "\n",
    "# Example usage:\n",
    "model_path = '/home/jc-merlab/Pictures/Data/trained_models/best_gnn_model_b4_e30_5.pth'\n",
    "image_path = '/home/jc-merlab/Pictures/Data/2023-08-14-Occluded/002652.rgb.jpg'\n",
    "predicted_kpts = predict_keypoints(model_path, image_path, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc38a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_keypoints(image_path, keypoints):\n",
    "    \"\"\"\n",
    "    Visualize the keypoints on an image.\n",
    "    \n",
    "    Args:\n",
    "    - image_path (str): Path to the image.\n",
    "    - keypoints (np.array): Array of keypoints, assumed to be in (x, y) format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Extract the x and y coordinates\n",
    "    x_coords = keypoints[0, 0, :, 0]\n",
    "    y_coords = keypoints[0, 0, :, 1]\n",
    "    \n",
    "    # Plot the keypoints\n",
    "    ax.scatter(x_coords, y_coords, c='r', s=40, label=\"Keypoints\")\n",
    "    \n",
    "    # Show the image with keypoints\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "model_path = '/home/jc-merlab/Pictures/Data/trained_models/best_gnn_model_b4_e30_5.pth'\n",
    "image_path = '/home/jc-merlab/Pictures/Data/2023-08-14-Occluded/002654.rgb.jpg'\n",
    "predicted_kpts = predict_keypoints(model_path, image_path, transform=None)\n",
    "visualize_keypoints(image_path, predicted_kpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cd8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

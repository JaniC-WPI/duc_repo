{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b355d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fucntion tranforms an input image for diverseifying data for training\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), \n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1), \n",
    "        ], p=1),\n",
    "        A.Resize(640, 480),  # Resize every image to 640x480 after all other transformations\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'),\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9395a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, vertices_dim=3, hidden_dim=128, num_vertices=6, num_edge_features=2):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.f_enc = nn.Linear(vertices_dim, hidden_dim)\n",
    "        self.f_e1 = nn.Linear((hidden_dim * 2), hidden_dim)\n",
    "        self.f_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.f_e2 = nn.Linear((hidden_dim * 2), num_edge_features)\n",
    "        self.num_vertices = num_vertices        \n",
    "    \n",
    "#     def get_node_features(self, vertices):\n",
    "# #         print(\"Vertices in node features\", vertices)\n",
    "#         node_features = []\n",
    "#         for keypoint in vertices:\n",
    "#             x, y, confidence, visibility, label = keypoint\n",
    "#             node_features.append([x, y, confidence, visibility, label])        \n",
    "#         nodes = torch.tensor(node_features, dtype=torch.float).to(device)\n",
    "# #         print(nodes)\n",
    "#         return nodes\n",
    "\n",
    "    def get_edge_features(self, vertices):\n",
    "        edges = [(0,1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)]\n",
    "#         print(edges)\n",
    "#         edge_features = []\n",
    "#         for edge in edges:\n",
    "#             print(edge)\n",
    "#             print(edge[0])\n",
    "#             print(edge[1])\n",
    "#             k1, k2 = vertices[edge[0]][:2], vertices[edge[1]][:2]\n",
    "#             distance = torch.norm(k1 - k2)\n",
    "#             angle = torch.atan2(k2[1] - k1[1], k2[0] - k1[0])\n",
    "#             edge_features.append([distance.item(), angle.item()])\n",
    "            \n",
    "        edges = torch.tensor(edges, dtype=torch.long).to(device)\n",
    "#         edge_features = torch.tensor(edge_features, dtype=torch.float).to(device)\n",
    "#         return edges, edge_features\n",
    "        return edges\n",
    "\n",
    "    def forward(self, vertices):\n",
    "        vertices = vertices.to(device)\n",
    "        edges = self.get_edge_features(vertices)\n",
    "        h1 = self.f_enc(vertices)\n",
    "        h1_source = h1[edges[:, 0]]\n",
    "        h1_target = h1[edges[:, 1]]\n",
    "        h_e1 = self.f_e1(torch.cat((h1_source, h1_target), dim=-1))\n",
    "        h_j_2 = self.f_v(h_e1)\n",
    "        h2_source = h_j_2[edges[:, 0]]\n",
    "        h2_target = h_j_2[edges[:, 1]]\n",
    "        h_e2 = self.f_e2(torch.cat((h2_source, h2_target), dim=-1))\n",
    "        h_e2_prob = torch.sigmoid(h_e2)\n",
    "        return vertices, h_e2_prob, edges\n",
    "\n",
    "class GNNDecoder(nn.Module):\n",
    "    def __init__(self, vertices_dim=3, hidden_dim=128, num_vertices=6, num_edge_features=2):\n",
    "        super(GNNDecoder, self).__init__()\n",
    "        self.f_e = nn.Linear((vertices_dim * 2), num_edge_features)  # Concatenate two vertices features\n",
    "        self.f_h = nn.Linear(num_edge_features, vertices_dim)  # Transform h_ij to the same dimension as vertices\n",
    "        self.f_v = nn.Linear(vertices_dim, vertices_dim)  # Update vertex feature\n",
    "    \n",
    "    def forward(self, vertices, h_e2_prob, edges):\n",
    "        h_source = vertices[edges[:, 0]]\n",
    "        h_target = vertices[edges[:, 1]]\n",
    "        h = torch.zeros_like(vertices)\n",
    "\n",
    "        for idx, (i, j) in enumerate(edges):  # Iterate over edges \n",
    "            h_ij = h_e2_prob[idx] * self.f_e(torch.cat((h_source[idx].unsqueeze(0), h_target[idx].unsqueeze(0)), dim=1))  # Include edge weights in the input\n",
    "            h_ij_transformed = self.f_h(h_ij.squeeze())  # Transform h_ij to the same dimension as vertices\n",
    "            h[j] += h_ij_transformed  # Accumulate edge features to the target vertex\n",
    "\n",
    "        h_transformed = self.f_v(h.view(-1, vertices.shape[1]))  # Transform h\n",
    "        h_transformed = h_transformed.view(vertices.shape)  # Reshape back to original shape\n",
    "        vertices_g = vertices + h_transformed  # Update vertex features\n",
    "\n",
    "        return vertices_g  # Return vertices_g as the prediction and vertices_g itself as the mean for Gaussian distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.keypoint_rcnn = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=True, num_keypoints=6, num_classes=7)\n",
    "        self.gnn_encoder = GNNEncoder()\n",
    "        self.gnn_decoder = GNNDecoder()\n",
    "        \n",
    "    def complete_missing_keypoints(self, keypoints, labels, num_expected_keypoints=6):\n",
    "\n",
    "        detected_kps = keypoints.shape[0]\n",
    "        # Check if all keypoints are detected\n",
    "        if detected_kps == num_expected_keypoints:\n",
    "            return keypoints\n",
    "\n",
    "        # Create a placeholder tensor for keypoints with the correct shape\n",
    "        ordered_keypoints = torch.zeros((num_expected_keypoints, 3), device=keypoints.device)\n",
    "\n",
    "        # If some keypoints are detected, compute their average position\n",
    "        average_kp = torch.mean(keypoints, dim=0, keepdim=True)\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            ordered_keypoints[label - 1] = keypoints[i]\n",
    "            \n",
    "        # Fill in the missing keypoints with the average position\n",
    "        missing_indices = (torch.sum(ordered_keypoints, dim=1) == 0).nonzero(as_tuple=True)[0]\n",
    "        ordered_keypoints[missing_indices] = average_kp\n",
    "        \n",
    "        return ordered_keypoints\n",
    "\n",
    "    def forward(self, images, targets=None, train=False):\n",
    "        if train:\n",
    "            output = self.keypoint_rcnn(images, targets)\n",
    "            return output\n",
    "    \n",
    "        else:\n",
    "            all_keypoints = []\n",
    "            all_edges = []\n",
    "            with torch.no_grad():\n",
    "                self.keypoint_rcnn.eval()\n",
    "                output = self.keypoint_rcnn(images)\n",
    "                self.keypoint_rcnn.train()\n",
    "                \n",
    "                keypoints = output[0]['keypoints'].detach().cpu().numpy()\n",
    "                kp_score = output[0]['keypoints_scores'].detach().cpu().numpy()\n",
    "                labels = output[0]['labels'].detach().cpu().numpy()\n",
    "                unique_labels = list(set(labels))\n",
    "                scores = output[0]['scores'].detach().cpu().numpy()\n",
    "                print(\"labels\", unique_labels)\n",
    "\n",
    "                kps = []\n",
    "                kp_scores = []\n",
    "                ulabels = []\n",
    "                \n",
    "                for label in unique_labels:\n",
    "                    indices = [j for j, x in enumerate(labels) if x == label]\n",
    "                    scores_for_label = [scores[j] for j in indices]\n",
    "                    max_score_index = indices[scores_for_label.index(max(scores_for_label))]\n",
    "                    kp_score_label = kp_score[max_score_index].tolist()\n",
    "                    kps.append(keypoints[max_score_index][kp_score_label.index(max(kp_score_label))])\n",
    "                    ulabels.append(label)\n",
    "\n",
    "                kps = [torch.tensor(kp, dtype=torch.float32) for kp in kps]\n",
    "                if not kps:\n",
    "                    default_value = torch.tensor([[320, 240, 1]], dtype=torch.float32, device=images[i].device)\n",
    "                    keypoints = default_value.repeat(6, 1)\n",
    "                else:\n",
    "                    keypoints = torch.stack(kps)\n",
    "                        \n",
    "                print(\"kp before placeholder\", keypoints)\n",
    "                keypoints = self.complete_missing_keypoints(keypoints, unique_labels)\n",
    "                print(\"kp after placeholder\", keypoints)\n",
    "                vertices, self.enc_e, self.edges = self.gnn_encoder(keypoints)\n",
    "                vertices_pred = self.gnn_decoder(vertices, self.enc_e, self.edges)\n",
    "                edges_pred = self.enc_e\n",
    "                edges = self.edges\n",
    "\n",
    "\n",
    "            print(\"All keypoints\", vertices_pred)\n",
    "\n",
    "            return vertices_pred, edges_pred, edges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592175b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "model = CombinedModel()\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 4\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize the GradScaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "top_5_models = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):  # for 50 epochs\n",
    "    for batch_idx, batch in enumerate(data_loader_train):\n",
    "        images, targets = batch          \n",
    "        # Move images to GPU\n",
    "        images = torch.stack(images).cuda()\n",
    "        imgs = [img.to(device) for img in images]  # Create list of images\n",
    "        # Move targets to GPU\n",
    "        for target in targets:\n",
    "            for key, val in target.items():\n",
    "                target[key] = val.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        individual_losses = []\n",
    "        \n",
    "       # Automatic mixed precision for forward pass\n",
    "        with autocast():\n",
    "            output_train = model(images, targets=targets, train=True)\n",
    "    \n",
    "            for i in range(len(images)):\n",
    "                img = images[i].unsqueeze(0) \n",
    "                ground_truth_keypoints = targets[i]['keypoints'].to(device).squeeze()\n",
    "                print(\"ground truth keypoints shape\", ground_truth_keypoints)\n",
    "                optimizer.zero_grad()\n",
    "                # automatic precision for forward pass\n",
    "                predicted_keypoints, predicted_edges, ground_truth_edges = model(img, train=False)    \n",
    "                # Compute the loss for this image\n",
    "                # Compute loss\n",
    "                loss = occ_keypoints_loss(predicted_keypoints, ground_truth_keypoints, predicted_edges, ground_truth_edges)\n",
    "                individual_losses.append(loss.item())\n",
    "            \n",
    "            # Aggregate the individual losses to get a scalar loss\n",
    "            scalar_loss = sum(individual_losses) / len(individual_losses)      \n",
    "            loss_keypoint = output_train['loss_keypoint']\n",
    "            print(\"loss keypoint\", loss)\n",
    "            total_loss = scalar_loss + 0.01*loss_keypoint           \n",
    "        \n",
    "        # Scale the loss and backpropagate\n",
    "        scaler.scale(total_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scheduler.step()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Check if the current model should be saved as a top model\n",
    "        if len(top_5_models) < 5 or loss.item() < max(top_5_models, key=lambda x: x[0])[0]:\n",
    "            # Save the model state and loss\n",
    "            model_state = {\n",
    "                'epoch': epoch,\n",
    "                'complete_model': model,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': total_loss.item(),\n",
    "            }\n",
    "            top_5_models.append((total_loss.item(), model_state))\n",
    "\n",
    "            # Sort the list based on loss (ascending order)\n",
    "            top_5_models.sort(key=lambda x: x[0])\n",
    "\n",
    "            # If there are more than 5 models, remove the one with the highest loss\n",
    "            if len(top_5_models) > 5:\n",
    "                top_5_models.pop()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx + 1}/{len(data_loader_train)}, Loss: {total_loss.item()}\")\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss.item()}\")\n",
    "    \n",
    "# After all epochs, save the top 5 models to disk\n",
    "for idx, (_, model_state) in enumerate(top_5_models):\n",
    "    torch.save(model_state, f'/home/jc-merlab/Pictures/Data/trained_models/occ_gnn_model_b{batch_size}_e{num_epochs}_{idx+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d23048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7eb2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

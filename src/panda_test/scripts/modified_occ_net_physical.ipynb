{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_planning_b1_e100_v4.pth'\n",
    "\n",
    "n_nodes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "# parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "parent_path =  \"/home/jc-merlab/Pictures/Data/\"\n",
    "\n",
    "# root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "root_dir = parent_path + \"occ_panda_physical_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a6d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "#         A.Resize(640, 480)  # Resize all images to be 640x480\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(0.95, 0.025, 0.025), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "            bboxes_labels_original.append('joint7')\n",
    "            bboxes_labels_original.append('joint8')\n",
    "            bboxes_labels_original.append('joint9')\n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
    "#         labels = [1, 2, 3, 4, 5, 6]\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebe0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch_geometric.nn as pyg\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "_EPS = 1e-10\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        self.dropout_prob = do_prob\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def batch_norm(self, inputs):\n",
    "        x = inputs.view(inputs.size(0) * inputs.size(1), -1)\n",
    "        x = self.bn(x)\n",
    "        return x.view(inputs.size(0), inputs.size(1), -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = func.elu(self.fc1(inputs))\n",
    "        x = func.dropout(x, self.dropout_prob, training=self.training)\n",
    "        x = func.elu(self.fc2(x))\n",
    "        return self.batch_norm(x)\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_out=4, do_prob=0., factor=True):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.factor = factor\n",
    "\n",
    "        self.mlp1 = MLP(n_in, n_hid, n_hid, do_prob)\n",
    "        self.mlp2 = MLP(n_hid * 2, n_hid, n_hid, do_prob)  # +2 for edge features (distance and angle)\n",
    "        self.mlp3 = MLP(n_hid, n_hid, n_hid, do_prob)\n",
    "        if self.factor:\n",
    "            self.mlp4 = MLP(n_hid * 3, n_hid, n_hid, do_prob)\n",
    "        else:\n",
    "            self.mlp4 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "        self.fc_out = nn.Linear(n_hid, n_out)\n",
    "\n",
    "    def edge2node(self, x, rel_rec, rel_send):\n",
    "        incoming = torch.matmul(rel_rec.t(), x)\n",
    "        return incoming / incoming.size(1)\n",
    "\n",
    "    def node2edge(self, x, rel_rec, rel_send):\n",
    "        receivers = torch.matmul(rel_rec, x)\n",
    "        senders = torch.matmul(rel_send, x)\n",
    "        edges = torch.cat([receivers, senders], dim=2)\n",
    "        \n",
    "        return edges\n",
    "\n",
    "    def forward(self, inputs, rel_rec, rel_send):\n",
    "        x = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "        x = self.mlp1(x)\n",
    "        x = self.node2edge(x, rel_rec, rel_send)\n",
    "        x = self.mlp2(x)\n",
    "        x_skip = x   \n",
    "\n",
    "        if self.factor:\n",
    "            x = self.edge2node(x, rel_rec, rel_send)\n",
    "            x = self.mlp3(x)\n",
    "            x = self.node2edge(x, rel_rec, rel_send)\n",
    "            x = torch.cat((x, x_skip), dim=2)\n",
    "            x = self.mlp4(x)\n",
    "        else:\n",
    "            x = self.mlp3(x)\n",
    "            x = torch.cat((x, x_skip), dim=2)\n",
    "            x = self.mlp4(x)\n",
    "\n",
    "        return self.fc_out(x)\n",
    "\n",
    "class GraphDecoder(nn.Module):\n",
    "    def __init__(self, n_in_node, edge_types, msg_hid, msg_out, n_hid, do_prob=0., skip_first=False):\n",
    "        super(GraphDecoder, self).__init__()\n",
    "        self.msg_fc1 = nn.ModuleList([nn.Linear(2 * n_in_node, msg_hid) for _ in range(edge_types)])\n",
    "        self.msg_fc2 = nn.ModuleList([nn.Linear(msg_hid, msg_out) for _ in range(edge_types)])\n",
    "        self.msg_out_shape = msg_out\n",
    "        self.skip_first_edge_type = skip_first\n",
    "\n",
    "        self.out_fc1 = nn.Linear(n_in_node + msg_out, n_hid)\n",
    "        self.out_fc2 = nn.Linear(n_hid, n_hid)\n",
    "        self.out_fc3 = nn.Linear(n_hid, n_in_node)\n",
    "        print('Using learned graph decoder.')\n",
    "\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "    def single_step_forward(self, single_timestep_inputs, rel_rec, rel_send, single_timestep_rel_type):\n",
    "        receivers = torch.matmul(rel_rec, single_timestep_inputs)\n",
    "        senders = torch.matmul(rel_send, single_timestep_inputs)\n",
    "        pre_msg = torch.cat([receivers, senders], dim=-1)\n",
    "\n",
    "        all_msgs = Variable(torch.zeros(pre_msg.size(0), pre_msg.size(1), self.msg_out_shape))\n",
    "        if single_timestep_inputs.is_cuda:\n",
    "            all_msgs = all_msgs.cuda()\n",
    "\n",
    "        if self.skip_first_edge_type:\n",
    "            start_idx = 1\n",
    "        else:\n",
    "            start_idx = 0\n",
    "\n",
    "        for i in range(start_idx, len(self.msg_fc2)):\n",
    "            msg = func.relu(self.msg_fc1[i](pre_msg))\n",
    "            msg = func.dropout(msg, p=self.dropout_prob)\n",
    "            msg = func.relu(self.msg_fc2[i](msg))\n",
    "            msg = msg * single_timestep_rel_type[:, :, i:i + 1]\n",
    "            all_msgs += msg\n",
    "\n",
    "        agg_msgs = all_msgs.transpose(-2, -1).matmul(rel_rec).transpose(-2, -1)\n",
    "        agg_msgs = agg_msgs.contiguous()\n",
    "\n",
    "        aug_inputs = torch.cat([single_timestep_inputs, agg_msgs], dim=-1)\n",
    "\n",
    "        pred = func.dropout(func.relu(self.out_fc1(aug_inputs)), p=self.dropout_prob)\n",
    "        pred = func.dropout(func.relu(self.out_fc2(pred)), p=self.dropout_prob)\n",
    "        pred = self.out_fc3(pred)\n",
    "        return single_timestep_inputs + pred\n",
    "\n",
    "    def forward(self, inputs, rel_type, rel_rec, rel_send, pred_steps=4):\n",
    "        last_pred = inputs[:, :, :]\n",
    "        curr_rel_type = rel_type[:, :, :]\n",
    "        preds = []\n",
    "\n",
    "        for step in range(0, pred_steps):\n",
    "            last_pred = self.single_step_forward(last_pred, rel_rec, rel_send, curr_rel_type)\n",
    "            preds.append(last_pred)\n",
    "\n",
    "        sizes = [preds[0].size(0), preds[0].size(1), preds[0].size(2)]\n",
    "        output = Variable(torch.zeros(sizes))\n",
    "        if inputs.is_cuda:\n",
    "            output = output.cuda()\n",
    "\n",
    "        for i in range(len(preds)):\n",
    "            output[:, :, :] = preds[i]\n",
    "\n",
    "        pred_all = output[:, :, :]\n",
    "        return pred_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6916a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(input, axis=1):\n",
    "    trans_input = input.transpose(axis, 0).contiguous()\n",
    "    soft_max_1d = func.softmax(trans_input,dim=0)\n",
    "    return soft_max_1d.transpose(axis, 0)\n",
    "\n",
    "def calculate_distance_angle(kp1, kp2):\n",
    "    dx = kp2[0] - kp1[0]\n",
    "    dy = kp2[1] - kp1[1]\n",
    "    distance = torch.sqrt(dx ** 2 + dy ** 2)\n",
    "    angle = torch.atan2(dy, dx)\n",
    "    return distance, angle\n",
    "\n",
    "def calculate_gt_distances_angles(keypoints_gt):\n",
    "    print(f\"keypoints_gt shape: {keypoints_gt.shape}\")  # Debug print\n",
    "    batch_size, num_keypoints, num_dims = keypoints_gt.shape\n",
    "    assert num_keypoints == n_nodes and num_dims == 2, \"keypoints_gt must have shape (batch_size, 9, 2)\"\n",
    "    distances_angles = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        batch_distances_angles = torch.zeros((num_keypoints, 4), dtype=torch.float32).to(device)  # Initialize with zeros\n",
    "        \n",
    "        for i in range(num_keypoints):\n",
    "            current_kp = keypoints_gt[b, i]\n",
    "            next_i = (i + 1) % num_keypoints\n",
    "            prev_i = (i - 1 + num_keypoints) % num_keypoints\n",
    "\n",
    "            # Calculate distance and angle to the next keypoint\n",
    "            dist, angle = calculate_distance_angle(current_kp, keypoints_gt[b, next_i])\n",
    "            batch_distances_angles[i, 0] = dist\n",
    "            batch_distances_angles[i, 1] = angle\n",
    "\n",
    "            # Calculate distance and angle to the previous keypoint\n",
    "            dist, angle = calculate_distance_angle(current_kp, keypoints_gt[b, prev_i])\n",
    "            batch_distances_angles[i, 2] = dist\n",
    "            batch_distances_angles[i, 3] = angle\n",
    "\n",
    "        distances_angles.append(batch_distances_angles)\n",
    "\n",
    "    distances_angles = torch.stack(distances_angles)\n",
    "    print(\"ground truth dist and angles\", distances_angles)\n",
    "    return distances_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ffa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path):\n",
    "        super(KeypointPipeline, self).__init__()  \n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.encoder = GraphEncoder(8,512,4,0.5,False)\n",
    "        self.decoder = GraphDecoder(n_in_node=8,\n",
    "                                 edge_types=2,\n",
    "                                 msg_hid=512,\n",
    "                                 msg_out=512,\n",
    "                                 n_hid=512,\n",
    "                                 do_prob=0.5,\n",
    "                                 skip_first=False)\n",
    "\n",
    "        num_nodes = n_nodes\n",
    "        self.off_diag = np.zeros([num_nodes, num_nodes])\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            next_node = (i + 1) % num_nodes\n",
    "            self.off_diag[i, next_node] = 1\n",
    "            self.off_diag[next_node, i] = 1\n",
    "                       \n",
    "        print(\"Class off diag\", self.off_diag)\n",
    "            \n",
    "        self.rel_rec = np.array(encode_onehot(np.where(self.off_diag)[1]), dtype=np.float32)\n",
    "        self.rel_send = np.array(encode_onehot(np.where(self.off_diag)[0]), dtype=np.float32)\n",
    "        self.rel_rec = torch.FloatTensor(self.rel_rec).to(device)\n",
    "        self.rel_send = torch.FloatTensor(self.rel_send).to(device)\n",
    "        self.encoder = self.encoder.cuda()\n",
    "        self.decoder = self.decoder.cuda()\n",
    "        self.rel_rec = self.rel_rec.cuda()\n",
    "        self.rel_send = self.rel_send.cuda()\n",
    "\n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "        confidence = output[0]['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0, 0:2])) + [confidence[idx]] + [labels[idx]])\n",
    "        keypoints.sort(key=lambda x: x[-1])\n",
    "        return keypoints  \n",
    "\n",
    "\n",
    "    def keypoints_to_graph(self, keypoints, image_width, image_height):        \n",
    "        keypoints = [torch.tensor(kp, dtype=torch.float32).to(device) if not isinstance(kp, torch.Tensor) else kp for kp in keypoints]\n",
    "        keypoints = torch.stack(keypoints).to(device)\n",
    "        \n",
    "        print(\"kprcnn output\", keypoints)\n",
    "\n",
    "        unique_labels, best_keypoint_indices = torch.unique(keypoints[:, 3], return_inverse=True)\n",
    "        best_scores, best_indices = torch.max(keypoints[:, 2].unsqueeze(0) * (best_keypoint_indices == torch.arange(len(unique_labels)).unsqueeze(1).cuda()), dim=1)\n",
    "        keypoints = keypoints[best_indices]\n",
    "\n",
    "        keypoints[:, 0] = (keypoints[:, 0] - image_width / 2) / (image_width / 2)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] - image_height / 2) / (image_height / 2)\n",
    "\n",
    "        # Initialize graph_features tensor\n",
    "        graph_features = torch.zeros((n_nodes, 8), dtype=torch.float32).to(device)\n",
    "\n",
    "        # Fill in the known keypoints in their corresponding positions\n",
    "        for kp in keypoints:\n",
    "            label = int(kp[3].item()) - 1  # Convert one-based label to zero-based index\n",
    "            graph_features[label, :4] = kp[:4]\n",
    "\n",
    "        # Calculate distances and angles for valid connections\n",
    "        for i in range(n_nodes):\n",
    "            if graph_features[i, 3] != 0:  # Check if current keypoint exists\n",
    "                current_kp = graph_features[i, :2]\n",
    "                next_label = (i + 1) % n_nodes\n",
    "                prev_label = (i - 1 + n_nodes) % n_nodes\n",
    "\n",
    "                # Check if next keypoint exists\n",
    "                if graph_features[next_label, 3] != 0:\n",
    "                    next_kp = graph_features[next_label, :2]\n",
    "                    dist, angle = calculate_distance_angle(current_kp, next_kp)\n",
    "                    graph_features[i, 4] = dist\n",
    "                    graph_features[i, 5] = angle\n",
    "                else:\n",
    "                    graph_features[i, 4] = 0  # Set distance to 0\n",
    "                    graph_features[i, 5] = 0  # Set angle to 0\n",
    "\n",
    "                # Check if previous keypoint exists\n",
    "                if graph_features[prev_label, 3] != 0:\n",
    "                    prev_kp = graph_features[prev_label, :2]\n",
    "                    dist, angle = calculate_distance_angle(current_kp, prev_kp)\n",
    "                    graph_features[i, 6] = dist\n",
    "                    graph_features[i, 7] = angle\n",
    "                else:\n",
    "                    graph_features[i, 6] = 0  # Set distance to 0\n",
    "                    graph_features[i, 7] = 0  # Set angle to 0\n",
    "\n",
    "        return graph_features\n",
    "\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        keypoint_model_training = self.keypoint_model.training\n",
    "        self.keypoint_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = [self.keypoint_model(img.unsqueeze(0).to(device)) for img in imgs]\n",
    "\n",
    "        self.keypoint_model.train(mode=keypoint_model_training)\n",
    "\n",
    "        batch_labeled_keypoints = [self.process_model_output(output) for output in batch_outputs]\n",
    "        batch_x = []\n",
    "        graphs = [self.keypoints_to_graph(kp, images.shape[3], images.shape[2]) for kp in all_keypoints]\n",
    "        for labeled_keypoints in batch_labeled_keypoints:\n",
    "            keypoints = self.keypoints_to_graph(labeled_keypoints, 640, 480)\n",
    "            print(\"keypoints with distance and angles\", keypoints)\n",
    "            # Initialize x with zeros for n_nodes nodes with 8 features each\n",
    "            x = torch.zeros(1, n_nodes, 8, device=device)\n",
    "\n",
    "            # Create a dictionary for quick lookup by label\n",
    "            keypoint_dict = {int(kp[3].item()): kp for kp in keypoints}\n",
    "\n",
    "            # Fill x with keypoints arranged according to labels\n",
    "            for i in range(n_nodes):\n",
    "                label = i + 1\n",
    "                if label in keypoint_dict:\n",
    "                    x[0, i, :] = keypoint_dict[label]\n",
    "                else:\n",
    "                    x[0, i, 3] = label  # Set the label in the fourth position\n",
    "\n",
    "            batch_x.append(x)\n",
    "            print(\"keypoints after label rearranged\", batch_x)\n",
    "        \n",
    "        batch_x = torch.cat(batch_x, dim=0)        \n",
    "        logits = self.encoder(batch_x, self.rel_rec, self.rel_send)\n",
    "        edges = my_softmax(logits, -1)\n",
    "        KGNN2D = self.decoder(batch_x, edges, self.rel_rec, self.rel_send)\n",
    "\n",
    "        return logits, KGNN2D, batch_labeled_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_edges(valid_points, edges):\n",
    "    batch_size, num_nodes = valid_points.shape\n",
    "\n",
    "    # Initialize off_diag matrix\n",
    "    off_diag = np.zeros([num_nodes, num_nodes])\n",
    "    for i in range(num_nodes):\n",
    "        next_node = (i + 1) % num_nodes\n",
    "        off_diag[i, next_node] = 1\n",
    "        off_diag[next_node, i] = 1\n",
    "    \n",
    "    # Convert off_diag to tensor\n",
    "    off_diag = torch.tensor(off_diag, dtype=torch.bool, device='cuda')\n",
    "    \n",
    "    # Create idx tensor to index valid relationships\n",
    "    idx = torch.where(off_diag)\n",
    "    \n",
    "    # Initialize relations tensor\n",
    "    relations = torch.zeros((batch_size, num_nodes * 2), device='cuda')\n",
    "    \n",
    "    for count, vis in enumerate(valid_points):\n",
    "        vis = vis.view(-1, 1).float()\n",
    "        vis_tran_mat = vis * vis.t()\n",
    "        \n",
    "        # Debug print to check the shape\n",
    "        print(f'vis_tran_mat shape: {vis_tran_mat.shape}')\n",
    "#         print(f'idx shape: {idx.shape}')\n",
    "        \n",
    "        # Gather valid relationships\n",
    "        vis_selected = vis_tran_mat[idx].view(-1)\n",
    "        \n",
    "        # Ensure correct dimensions\n",
    "        relations[count, :] = vis_selected[:num_nodes * 2]\n",
    "\n",
    "    # Ensure correct dtype for loss calculation\n",
    "    relations = relations.to(torch.long)\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    loss_edges = func.cross_entropy(edges.view(-1, 4), relations.view(-1))\n",
    "    return loss_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nll_gaussian(preds, target, variance, add_const=False):\n",
    "#     neg_log_p = ((preds - target) ** 2 / (2 * variance))\n",
    "#     if add_const:\n",
    "#         const = 0.5 * np.log(2 * np.pi * variance)\n",
    "#         neg_log_p += const\n",
    "#     return neg_log_p.sum() / (target.size(0) * target.size(1))\n",
    "\n",
    "# def kgnn2d_loss(keypoints_gt, valid_points, keypoints_logits):\n",
    "#     # Ensure data types are consistent and move tensors to the appropriate device\n",
    "#     keypoints_gt = keypoints_gt.type(torch.FloatTensor).cuda()\n",
    "#     keypoints_logits = keypoints_logits.type(torch.FloatTensor).cuda()\n",
    "#     valid_points = valid_points.type(torch.FloatTensor).cuda()\n",
    "\n",
    "#     # Print shapes for debugging\n",
    "# #     print(f\"keypoints_gt.shape: {keypoints_gt.shape}\")\n",
    "# #     print(f\"keypoints_logits.shape: {keypoints_logits.shape}\")\n",
    "# #     print(f\"valid_points.shape: {valid_points.shape}\")\n",
    "#     keypoints_gt = keypoints_gt.type(torch.FloatTensor)*valid_points.unsqueeze(2).type(torch.FloatTensor)\n",
    "#     keypoints_logits = keypoints_logits.type(torch.FloatTensor)*valid_points.unsqueeze(2).type(torch.FloatTensor)\n",
    "#     keypoints_gt = keypoints_gt.cuda()\n",
    "#     keypoints_logits = keypoints_logits.cuda()\n",
    "#     loss_occ = nll_gaussian(keypoints_gt[:,:,0:2], keypoints_logits[:,:,0:2] , 0.1)\n",
    "#     return loss_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgnn2d_loss(gt_keypoints, pred_keypoints, gt_distances_next, gt_angles_next, gt_distances_prev, gt_angles_prev, pred_distances_next, pred_angles_next, pred_distances_prev, pred_angles_prev):\n",
    "    keypoints_loss = func.mse_loss(pred_keypoints, gt_keypoints)\n",
    "    prev_distances_loss = func.mse_loss(pred_distances_prev, gt_distances_prev)\n",
    "    prev_angles_loss = func.mse_loss(pred_angles_prev, gt_angles_prev)\n",
    "    next_distances_loss = func.mse_loss(pred_distances_next, gt_distances_next)\n",
    "    next_angles_loss = func.mse_loss(pred_angles_next, gt_angles_next)\n",
    "    return keypoints_loss + prev_distances_loss + prev_angles_loss + next_distances_loss + next_angles_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "def process_batch_keypoints(target_dicts):\n",
    "    batch_size = len(target_dicts)\n",
    "    keypoints_list = []\n",
    "    visibilities_list = []\n",
    "    for dict_ in target_dicts:\n",
    "        keypoints = dict_['keypoints'].squeeze(1).to(device)\n",
    "        xy_coords = keypoints[:, :2]\n",
    "        visibilities = keypoints[:, 2]\n",
    "        keypoints_list.append(xy_coords)\n",
    "        visibilities_list.append(visibilities)\n",
    "    keypoints_gt = torch.stack(keypoints_list).float().cuda()\n",
    "    visibilities = torch.stack(visibilities_list).cuda()\n",
    "    valid_vis_all = (visibilities == 1).long().cuda()\n",
    "    valid_invis_all = (visibilities == 0).long().cuda()\n",
    "    return keypoints_gt, valid_vis_all, valid_invis_all\n",
    "\n",
    "def reorder_batch_keypoints(batch_keypoints):\n",
    "    batch_size, num_keypoints, num_features = batch_keypoints.shape\n",
    "    reordered_keypoints_batch = []\n",
    "    for i in range(batch_size):\n",
    "        normalized_keypoints = batch_keypoints[i]\n",
    "        reordered_normalized_keypoints = torch.zeros(num_keypoints, 2, device=batch_keypoints.device)\n",
    "        rounded_labels = torch.round(normalized_keypoints[:, -1]).int()\n",
    "        used_indices = []\n",
    "        for label in range(1, 10):\n",
    "            valid_idx = (rounded_labels == label).nonzero(as_tuple=True)[0]\n",
    "            if valid_idx.numel() > 0:\n",
    "                reordered_normalized_keypoints[label - 1] = normalized_keypoints[valid_idx[0], :2]\n",
    "            else:\n",
    "                invalid_idx = ((rounded_labels < 1) | (rounded_labels > 6)).nonzero(as_tuple=True)[0]\n",
    "                invalid_idx = [idx for idx in invalid_idx if idx not in used_indices]\n",
    "                if invalid_idx:\n",
    "                    reordered_normalized_keypoints[label - 1] = normalized_keypoints[invalid_idx[0], :2]\n",
    "                    used_indices.append(invalid_idx[0])\n",
    "        reordered_keypoints_batch.append(reordered_normalized_keypoints)\n",
    "    return torch.stack(reordered_keypoints_batch)\n",
    "\n",
    "def denormalize_keypoints(batch_keypoints, width=640, height=480):\n",
    "    denormalized_keypoints = []\n",
    "    for kp in batch_keypoints:\n",
    "        denormalized_x = (kp[:, 0] * (width / 2)) + (width / 2)\n",
    "        denormalized_y = (kp[:, 1] * (height / 2)) + (height / 2)\n",
    "        denormalized_kp = torch.stack((denormalized_x, denormalized_y), dim=1)\n",
    "        denormalized_keypoints.append(denormalized_kp)\n",
    "    denormalized_keypoints = torch.stack(denormalized_keypoints)\n",
    "    return denormalized_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292608ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = KeypointPipeline(weights_path)\n",
    "# model = model.to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# num_epochs = 1\n",
    "# batch_size = 2\n",
    "\n",
    "# split_folder_path = train_test_split(root_dir)\n",
    "# KEYPOINTS_FOLDER_TRAIN = split_folder_path +\"/train\"\n",
    "# KEYPOINTS_FOLDER_VAL = split_folder_path +\"/val\"\n",
    "# KEYPOINTS_FOLDER_TEST = split_folder_path +\"/test\"\n",
    "\n",
    "# dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "# dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "# dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "# data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "# data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "# data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "# start_time = time.time()\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for imgs, target_dicts, _ in data_loader_train:\n",
    "#         imgs = [img.to(device) for img in imgs]\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         logits, KGNN2D, batch_labeled_keypoints = model(imgs)\n",
    "#         keypoints_gt, valid_vis_all, valid_invis_all = process_batch_keypoints(target_dicts)\n",
    "#         print(\"keypoints GT\", keypoints_gt)\n",
    "#         # Debug prints to check shapes\n",
    "#         print(f'gt_keypoints shape: {keypoints_gt.shape}')    \n",
    "#         print(KGNN2D.shape)\n",
    "# #         print(f'gt_distances_angles shape: {gt_distances_angles.shape}')\n",
    "# #         print(f'pred_distances_angles shape: {pred_distances_angles.shape}')\n",
    "#         reordered_normalized_keypoints = reorder_batch_keypoints(KGNN2D)\n",
    "#         denormalized_keypoints = denormalize_keypoints(reordered_normalized_keypoints)\n",
    "#         print(f'denormalized_keypoints shape: {denormalized_keypoints}')\n",
    "#         gt_distances_angles = calculate_gt_distances_angles(keypoints_gt)\n",
    "#         pred_distances_angles = calculate_gt_distances_angles(denormalized_keypoints)\n",
    "#         loss_kgnn2d = kgnn2d_loss(keypoints_gt, denormalized_keypoints, gt_distances_angles[:, :, 0], gt_distances_angles[:, :, 1], pred_distances_angles[:, :, 0], pred_distances_angles[:, :, 1])\n",
    "#         edge_loss = loss_edges(valid_vis_all, logits)\n",
    "#         total_batch_loss = edge_loss + loss_kgnn2d\n",
    "#         total_batch_loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += total_batch_loss.item()\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader_train)}')\n",
    "# end_time = time.time()\n",
    "\n",
    "# total_time = end_time - start_time\n",
    "# print(\"total time\", total_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e60a05b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = KeypointPipeline(weights_path)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scaler = GradScaler()\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 2\n",
    "\n",
    "split_folder_path = train_test_split(root_dir)\n",
    "KEYPOINTS_FOLDER_TRAIN = split_folder_path +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = split_folder_path +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = split_folder_path +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, target_dicts, _ in data_loader_train:\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            logits, KGNN2D, batch_labeled_keypoints = model(imgs)\n",
    "            keypoints_gt, valid_vis_all, valid_invis_all = process_batch_keypoints(target_dicts)\n",
    "            print(\"Ground truth keypoints\", keypoints_gt)\n",
    "#             print(\"predicted keypoints before reorder\", KGNN2D)\n",
    "            reordered_normalized_keypoints = reorder_batch_keypoints(KGNN2D)\n",
    "#             print(\"predicted keypoints after reorder\", reordered_normalized_keypoints)\n",
    "            denormalized_keypoints = denormalize_keypoints(reordered_normalized_keypoints)\n",
    "            print(\"final predicted keypoints\", denormalized_keypoints)\n",
    "            gt_distances_angles = calculate_gt_distances_angles(keypoints_gt)\n",
    "            pred_distances_angles = calculate_gt_distances_angles(denormalized_keypoints)\n",
    "            loss_kgnn2d = kgnn2d_loss(keypoints_gt, denormalized_keypoints, gt_distances_angles[:, :, 0], \n",
    "                                      gt_distances_angles[:, :, 1], gt_distances_angles[:, :, 2], \n",
    "                                      gt_distances_angles[:, :, 3], pred_distances_angles[:, :, 0], \n",
    "                                      pred_distances_angles[:, :, 1], pred_distances_angles[:, :, 2], \n",
    "                                      pred_distances_angles[:, :, 3])\n",
    "            edge_loss = loss_edges(valid_vis_all, logits)\n",
    "            total_batch_loss = edge_loss + loss_kgnn2d\n",
    "        \n",
    "        scaler.scale(total_batch_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "#         total_batch_loss.backward()\n",
    "#         optimizer.step()\n",
    "        total_loss += total_batch_loss.item()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader_train)}')\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "print(\"total time\", total_time)\n",
    "    \n",
    "#     # Save checkpoint every 10 epochs\n",
    "#     if (epoch + 1) % 2 == 0:\n",
    "#         checkpoint_path = f'/home/schatterjee/lama/kprcnn_panda/trained_models/occ_ckpt/occ_net_ckpt_e{epoch+1}.pth'\n",
    "#         torch.save(model, checkpoint_path)\n",
    "#         print(f'Checkpoint saved to {checkpoint_path}')\n",
    "\n",
    "# model_save_path = f\"/home/schatterjee/lama/kprcnn_panda/trained_models/kprcnn_occ_b{batch_size}_e{num_epochs}_v{v}.pth\"\n",
    "# torch.save(model, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b99cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeypointPipeline(weights_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assuming KeypointPipeline and all necessary classes/functions are defined above or imported\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Path to the trained model weights\n",
    "model_path = '/home/jc-merlab/Pictures/Data/trained_models/occ_ckpt/ckpt_e120.pth'\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(model_path)\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image and prepare it for the model prediction.\n",
    "    This function should replicate the preprocessing applied during training.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Your preprocessing steps here, for example, resizing, normalization, etc.\n",
    "    # This is a placeholder transformation, adjust it according to your training transformations\n",
    "    img_tensor = F.to_tensor(img).to(device)  # Add batch dim and convert to tensor\n",
    "    return img_tensor\n",
    "\n",
    "def predict(model, img_tensor):\n",
    "    # Assuming the model takes a list of tensors as input\n",
    "    with torch.no_grad():\n",
    "        logits, KGNN2D, batch_labeled_keypoints = model([img_tensor])\n",
    "    # Process the output as needed\n",
    "    return KGNN2D\n",
    "\n",
    "def postprocess_keypoints(keypoints, width=640, height=480):\n",
    "    # Adjust this function based on your model's output format\n",
    "    denormalized_keypoints = denormalize_keypoints(keypoints, width, height)\n",
    "    return denormalized_keypoints\n",
    "\n",
    "image_path = '/home/jc-merlab/Pictures/Data/source_folder_occlusion/001471.rgb.jpg'\n",
    "\n",
    "\n",
    "# Example usage\n",
    "img_tensor = prepare_image(image_path).to(device)\n",
    "KGNN2D = predict(model, img_tensor)\n",
    "ordered_keypoints = reorder_batch_keypoints(KGNN2D)\n",
    "denormalized_keypoints = postprocess_keypoints(ordered_keypoints)\n",
    "print(denormalized_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0470fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"keypoints\": [[[257.95220042652915, 366.9198630617724, 1]], [[257.95973939799904, 283.013113744617, 1]], \n",
    "              [[179.70016595863137, 298.244571585954, 1]], [[175.69899307811323, 277.8348649543144, 0]], \n",
    "              [[79.90486225732596, 303.90392338594796, 0]], \n",
    "              [[66.15504343164937, 289.4557892368882, 1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "import copy\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the model\n",
    "model_path = '/home/jc-merlab/Pictures/Data/trained_models/occ_ckpt/kprcnn_occ_net_ckpt_b128e10.pth'\n",
    "# model_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'\n",
    "model = torch.load(model_path)\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image and prepare it for the model prediction.\n",
    "    This function should replicate the preprocessing applied during training.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Your preprocessing steps here, for example, resizing, normalization, etc.\n",
    "    # This is a placeholder transformation, adjust it according to your training transformations\n",
    "    img_tensor = F.to_tensor(img).to(device)  # Add batch dim and convert to tensor\n",
    "    return img_tensor\n",
    "\n",
    "def load_ground_truth(json_path):\n",
    "    \"\"\"\n",
    "    Load ground truth keypoints from the corresponding JSON file.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    ground_truth_keypoints = [[int(kp[0][0]), int(kp[0][1])] for kp in data['keypoints']]  # Convert coordinates to integers\n",
    "    return ground_truth_keypoints\n",
    "\n",
    "# def draw_lines_between_keypoints(image, keypoints, color=(255, 255, 255)):\n",
    "#     \"\"\"\n",
    "#     Draw lines connecting adjacent keypoints.\n",
    "#     \"\"\"\n",
    "#     for i in range(len(keypoints) - 1):\n",
    "#         start_point = keypoints[i]\n",
    "#         end_point = keypoints[i + 1]\n",
    "#         cv2.line(image, start_point, end_point, color, thickness=2)\n",
    "        \n",
    "def draw_lines_between_keypoints(image, keypoints, color=(255, 255, 255)):\n",
    "    \"\"\"\n",
    "    Draw lines connecting adjacent keypoints, ensuring coordinates are integers.\n",
    "    \"\"\"\n",
    "    for i in range(len(keypoints) - 1):\n",
    "        start_point = (int(keypoints[i][0]), int(keypoints[i][1]))\n",
    "        end_point = (int(keypoints[i + 1][0]), int(keypoints[i + 1][1]))\n",
    "        cv2.line(image, start_point, end_point, color, thickness=2)\n",
    "        \n",
    "    for x, y in keypoints:\n",
    "        cv2.circle(image, (x, y), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "def predict(model, img_tensor):\n",
    "    # Assuming the model takes a list of tensors as input\n",
    "    with torch.no_grad():\n",
    "        logits, KGNN2D, batch_labeled_keypoints = model([img_tensor])\n",
    "    # Process the output as needed\n",
    "    return KGNN2D\n",
    "\n",
    "def postprocess_keypoints(keypoints, width=640, height=480):\n",
    "    # Adjust this function based on your model's output format\n",
    "    denormalized_keypoints = denormalize_keypoints(keypoints, width, height)\n",
    "    return denormalized_keypoints\n",
    "\n",
    "def visualize_keypoints_with_ground_truth(image_path, predicted_keypoints, ground_truth_keypoints, out_dir):\n",
    "    \"\"\"\n",
    "    Visualize predicted and ground truth keypoints on the image using cv2.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if torch.is_tensor(predicted_keypoints):\n",
    "        predicted_keypoints = predicted_keypoints.cpu().numpy()\n",
    "    # Predicted keypoints in green\n",
    "    for kp in predicted_keypoints[0]:  # Assuming the keypoints shape is [1, N, 2]\n",
    "        x, y = int(kp[0]), int(kp[1])\n",
    "        cv2.circle(img, (x, y), radius=8, color=(255, 0, 0), thickness=-1)\n",
    "    \n",
    "    # Ground truth keypoints in red\n",
    "    for x, y in ground_truth_keypoints:\n",
    "        cv2.circle(img, (x, y), radius=6, color=(0, 0, 255), thickness=-1)\n",
    "    \n",
    "    filename = os.path.basename(image_path)\n",
    "    output_path = os.path.join(out_dir, filename)\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "def visualize_keypoints(image_path, keypoints, out_dir):\n",
    "    \"\"\"\n",
    "    Visualize keypoints on the image using cv2.\n",
    "    \"\"\"\n",
    "    # Load the original image\n",
    "    img = cv2.imread(image_path)\n",
    "    # Convert keypoints to a NumPy array if it's a tensor\n",
    "    if torch.is_tensor(keypoints):\n",
    "        keypoints = keypoints.cpu().numpy()\n",
    "    \n",
    "    # Draw keypoints on the image\n",
    "    for kp in keypoints[0]:  # Assuming the keypoints shape is [1, N, 2]\n",
    "        x, y = int(kp[0]), int(kp[1])\n",
    "        cv2.circle(img, (x, y), radius=8, color=(0, 0, 0), thickness=-1)\n",
    "    \n",
    "   # Construct the output path and save the image\n",
    "    filename = os.path.basename(image_path)\n",
    "    output_path = os.path.join(out_dir, filename)\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "def process_folder(folder_path, output_path, output_path_line):\n",
    "    \"\"\"\n",
    "    Process all images in the specified folder, predict and visualize keypoints.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            img_tensor = prepare_image(image_path).to(device)\n",
    "            KGNN2D = predict(model, img_tensor)\n",
    "            ordered_keypoints = reorder_batch_keypoints(KGNN2D)  # Ensure this function is defined\n",
    "            denormalized_keypoints = postprocess_keypoints(ordered_keypoints)\n",
    "#             visualize_keypoints(image_path, denormalized_keypoints, output_path)\n",
    "            \n",
    "            # Load ground truth keypoints\n",
    "            json_filename = filename.split('.')[0] + '.json'  # Construct JSON filename\n",
    "            json_path = os.path.join(folder_path, json_filename)\n",
    "            ground_truth_keypoints = load_ground_truth(json_path) \n",
    "           \n",
    "\n",
    "            img = cv2.imread(image_path)\n",
    "            img_copy = copy.deepcopy(img)                      \n",
    "\n",
    "            draw_lines_between_keypoints(img_copy, ground_truth_keypoints, (0, 0, 255))\n",
    "            \n",
    "            cv2.imwrite(os.path.join(output_path_line, filename), img_copy)\n",
    "            # Visualize keypoints and ground truth\n",
    "            visualize_keypoints_with_ground_truth(image_path, denormalized_keypoints, ground_truth_keypoints, output_path)\n",
    "            img_with_keypoints = cv2.imread(os.path.join(output_path, filename))  # Reload the image with keypoints\n",
    "\n",
    "            # Optionally draw lines between keypoints\n",
    "#             draw_lines_between_keypoints(img_with_keypoints, denormalized_keypoints[0], (0, 255, 0))\n",
    "#             draw_lines_between_keypoints(img_with_keypoints, ground_truth_keypoints, (0, 0, 255))\n",
    "\n",
    "            # Save the image with lines\n",
    "            cv2.imwrite(os.path.join(output_path, filename), img_with_keypoints)\n",
    "            \n",
    "            # Optionally draw lines between keypoints\n",
    "#             draw_lines_between_keypoints(img_with_keypoints, denormalized_keypoints[0], (0, 255, 0))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "# Example usage\n",
    "folder_path = '/home/jc-merlab/Pictures/Data/occ_test_data/'\n",
    "output_path = '/home/jc-merlab/Pictures/Data/occ_test_data/output/'\n",
    "output_path_line = '/home/jc-merlab/Pictures/Data/occ_test_op_line/'\n",
    "process_folder(folder_path, output_path, output_path_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea838c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad751b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6083464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bf7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KpVelDataset(Dataset):\n",
    "#     def __init__(self, json_folder):\n",
    "#         super(KpVelDataset, self).__init__()\n",
    "#         self.data = []\n",
    "#         for json_file in sorted(os.listdir(json_folder)):\n",
    "#             if json_file.endswith('_combined.json'):\n",
    "#                 with open(os.path.join(json_folder, json_file), 'r') as file:\n",
    "#                     data = json.load(file)\n",
    "#                     start_kp = data['start_kp']\n",
    "#                     next_kp = data['next_kp']\n",
    "#                     position = data['position']\n",
    "#                     self.data.append((start_kp, next_kp, position))\n",
    "\n",
    "    def __init__(self, json_folder):\n",
    "        super(KpVelDataset, self).__init__()\n",
    "        self.data = []\n",
    "        for json_file in sorted(os.listdir(json_folder)):\n",
    "            if json_file.endswith('_combined.json'):\n",
    "                with open(os.path.join(json_folder, json_file), 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    # Ensure data contains 'start_kp', 'next_kp', and 'position'\n",
    "                    if 'start_kp' in data and 'next_kp' in data and 'position' in data:\n",
    "                        start_kp = data['start_kp']\n",
    "                        next_kp = data['next_kp']\n",
    "                        position = data['position']\n",
    "                        # Only append if start_kp and next_kp are not empty\n",
    "                        if start_kp and next_kp:\n",
    "                            self.data.append((start_kp, next_kp, position))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_kp, next_kp, position = self.data[idx]\n",
    "        # Ensure start_kp and next_kp have consistent dimensions\n",
    "        if not start_kp or not next_kp:\n",
    "            raise ValueError(f\"Empty keypoints found at index {idx}\")\n",
    "        start_kp_flat = torch.tensor([kp for sublist in start_kp for kp in sublist[0][:2]], dtype=torch.float)\n",
    "        next_kp_flat = torch.tensor([kp for sublist in next_kp for kp in sublist[0][:2]], dtype=torch.float)\n",
    "        position = torch.tensor(position, dtype=torch.float)\n",
    "        return start_kp_flat, next_kp_flat, position\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "# #     def __getitem__(self, idx):\n",
    "# #         start_kp, next_kp, velocity = self.data[idx]\n",
    "# #         start_kp_flat = torch.tensor([kp for sublist in start_kp for kp in sublist[0]])\n",
    "# #         next_kp_flat = torch.tensor([kp for sublist in next_kp for kp in sublist[0]])\n",
    "# #         velocity = torch.tensor(velocity)\n",
    "# #         return start_kp_flat, next_kp_flat, velocity\n",
    "  \n",
    "#     def __getitem__(self, idx):\n",
    "#         start_kp, next_kp, position = self.data[idx]\n",
    "#         # Extract and flatten the first two elements of each keypoint in start_kp\n",
    "#         start_kp_flat = torch.tensor([kp for sublist in start_kp for kp in sublist[0][:2]], dtype=torch.float)\n",
    "#         # Extract and flatten the first two elements of each keypoint in next_kp\n",
    "#         next_kp_flat = torch.tensor([kp for sublist in next_kp for kp in sublist[0][:2]], dtype=torch.float)\n",
    "#         position = torch.tensor(position)\n",
    "#         return start_kp_flat, next_kp_flat, position\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15778039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "#     dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "#     for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "#         shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*_combined.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = root_dir + \"split_folder_reg\"\n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(0.8, 0.1, 0.1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "#     shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3049426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosRegModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(PosRegModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size * 2, 1024)  # Assuming start_kp and next_kp are concatenated\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512,256)\n",
    "        self.fc4 = nn.Linear(256,128)\n",
    "        self.fc5 = nn.Linear(128,64)\n",
    "        self.fc6 = nn.Linear(64,64)\n",
    "        self.fc7 = nn.Linear(64,3)  # Output size is 3 for velocity\n",
    "\n",
    "    def forward(self, start_kp, next_kp):\n",
    "        x = torch.cat((start_kp.to(device), next_kp.to(device)), dim=1)\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = func.relu(self.fc3(x))\n",
    "        x = func.relu(self.fc4(x))\n",
    "        x = func.relu(self.fc5(x))\n",
    "        x = func.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843e5d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jc-merlab/Pictures/panda_data/panda_sim_vel/regression_combined_test_new/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 12428 files [00:00, 16829.36 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataset and data loader\n",
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "v = 1\n",
    "root_dir = '/home/jc-merlab/Pictures/panda_data/panda_sim_vel/regression_combined_test_new/'\n",
    "print(root_dir)\n",
    "split_folder_path = train_test_split(root_dir)\n",
    "dataset = KpVelDataset(root_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "model = PosRegModel(12)  # Adjust input_size as necessary\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for start_kp, next_kp, position in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        position = position.squeeze(1)\n",
    "        print(position)\n",
    "        print(start_kp.shape)\n",
    "        print(position.shape)\n",
    "        output = model(start_kp, next_kp)\n",
    "        loss = criterion(output, position.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"output\", output)\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    \n",
    "# # Save the trained model\n",
    "model_save_path = f'/home/jc-merlab/Pictures/Data/trained_models/reg_pos_b{batch_size}_e{num_epochs}_v{v}.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# model_save_path = f'/home/jc-merlab/Pictures/Data/trained_models/reg_nkp_b{batch_size}_e{num_epochs}_v{v}.pth'\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'model_structure': KeypointRegressionNet()\n",
    "# }, model_save_path)\n",
    "# print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_path, test_data_dir):\n",
    "    # Load the test dataset\n",
    "    test_dataset = KpVelDataset(test_data_dir)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the model and load the saved state\n",
    "    model = PosRegModel(12)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Criterion for evaluation\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "\n",
    "    # No gradient needed for evaluation\n",
    "    with torch.no_grad():\n",
    "        for start_kp, next_kp, velocity in test_loader:\n",
    "            output = model(start_kp, velocity)\n",
    "            for i in range(start_kp.size(0)):\n",
    "                individual_start_kp = start_kp[i]\n",
    "                individual_next_kp = next_kp[i]\n",
    "                individual_position = position[i]\n",
    "                predicted_position = output[i]\n",
    "\n",
    "                print(\"Start KP:\", individual_start_kp)\n",
    "                print(\"Next KP:\", individual_next_kp)\n",
    "                print(\"Actual Position:\", individual_position)\n",
    "                print(\"Predicted Position:\", predicted_position)\n",
    "                print(\"-----------------------------------------\")\n",
    "            loss = criterion(output, next_kp)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    # Calculate the average loss\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f'Average Test Loss: {avg_loss}')\n",
    "\n",
    "# Usage\n",
    "model_path = '/home/jc-merlab/Pictures/Data/trained_models/reg_pos_b64_e300_v1.pth'  # Update with your model path\n",
    "test_data_dir = '/home/jc-merlab/Pictures/panda_data/panda_sim_vel/regression_combined_test_new/split_folder_reg/test/annotations/'  # Update with your test data path\n",
    "test_model(model_path, test_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5541447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

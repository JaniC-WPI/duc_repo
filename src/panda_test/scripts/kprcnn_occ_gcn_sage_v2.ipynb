{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532722c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils import collate_fn\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "from kprcnn_occ_gcn_pipeline import load_keypoint_pipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch_geometric.nn as pyg\n",
    "from torch_geometric.nn import SAGEConv, GCNConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "path = Def_Path()\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "# weights_path = '/home/schatterjee/lama/kprcnn_panda/trained_models/keypointsrcnn_planning_b1_e50_v8.pth'\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_planning_b1_e50_v8.pth'\n",
    "n_nodes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "# parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "parent_path = \"/home/jc-merlab/Pictures/Data/\"\n",
    "# parent_path =  \"/home/schatterjee/lama/kprcnn_panda/\"\n",
    "\n",
    "# root_dir = parent_path + \"occ_new_panda_physical_dataset/\"\n",
    "root_dir = parent_path + \"occ_new_panda_physical_dataset_trunc_v2/\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)\n",
    "\n",
    "# model = load_keypoint_pipeline(weights_path)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = '/home/jc-merlab/Pictures/Data/trained_models/gcn_ckpt_v2/kprcnn_occ_gcn_ckpt_b128e23.pth'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Extract the state dictionary\n",
    "model_state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# # Load the state dictionary into the model\n",
    "# model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c749391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output_gcn_sage_v2\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(0.95, 0.025, 0.025), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GTDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes = data['bboxes']\n",
    "            keypoints = data['keypoints']\n",
    "            \n",
    "            bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)            \n",
    "        \n",
    "        return img, target, img_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50811257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "            bboxes_labels_original.append('joint7')\n",
    "            bboxes_labels_original.append('joint8')\n",
    "            bboxes_labels_original.append('joint9')\n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
    "#         labels = [1, 2, 3, 4, 5, 6]\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_angle(kp1, kp2):\n",
    "    dx = kp2[0] - kp1[0]\n",
    "    dy = kp2[1] - kp1[1]\n",
    "    dx, dy = torch.tensor(dx).to(device), torch.tensor(dy).to(device)\n",
    "    distance = torch.sqrt(dx ** 2 + dy ** 2)\n",
    "    angle = torch.atan2(dy, dx)\n",
    "    return distance, angle\n",
    "\n",
    "def calculate_gt_distances_angles(keypoints_gt):\n",
    "#     print(f\"keypoints_gt shape: {keypoints_gt.shape}\")  # Debug print\n",
    "#     print(keypoints_gt.shape)\n",
    "    batch_size, num_keypoints, num_dims = keypoints_gt.shape    \n",
    "    assert num_keypoints == n_nodes and num_dims == 2, \"keypoints_gt must have shape (batch_size, 9, 2)\"\n",
    "    distances_angles = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        batch_distances_angles = torch.zeros((num_keypoints, 4), dtype=torch.float32).to(device)  # Initialize with zeros\n",
    "\n",
    "        for i in range(num_keypoints):\n",
    "            current_kp = keypoints_gt[b, i]\n",
    "            next_i = (i + 1) % num_keypoints\n",
    "            prev_i = (i - 1 + num_keypoints) % num_keypoints\n",
    "\n",
    "            # Calculate distance and angle to the next keypoint\n",
    "            dist, angle = calculate_distance_angle(current_kp, keypoints_gt[b, next_i])\n",
    "            batch_distances_angles[i, 0] = dist\n",
    "            batch_distances_angles[i, 1] = angle\n",
    "\n",
    "            # Calculate distance and angle to the previous keypoint\n",
    "            dist, angle = calculate_distance_angle(current_kp, keypoints_gt[b, prev_i])\n",
    "            batch_distances_angles[i, 2] = dist\n",
    "            batch_distances_angles[i, 3] = angle\n",
    "\n",
    "        distances_angles.append(batch_distances_angles)\n",
    "\n",
    "    distances_angles = torch.stack(distances_angles)\n",
    "    return distances_angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee41d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_keypoints(image, pred_keypoints, gt_keypoints=None, title=\"Keypoints Visualization\"):\n",
    "    # Convert the image tensor to a numpy array and move channels (C, H, W) -> (H, W, C)\n",
    "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image_np)\n",
    "    \n",
    "    # Plot predicted keypoints (blue)\n",
    "    if pred_keypoints is not None:\n",
    "        pred_keypoints_np = pred_keypoints.cpu().numpy()\n",
    "        for kp in pred_keypoints_np:\n",
    "            plt.scatter(kp[0], kp[1], c='blue', marker='x', s=100, label='Predicted' if kp is pred_keypoints_np[0] else \"\")\n",
    "    \n",
    "    # Plot ground truth keypoints (green)\n",
    "    if gt_keypoints is not None:\n",
    "        gt_keypoints_np = gt_keypoints.cpu().numpy()\n",
    "        for kp in gt_keypoints_np:\n",
    "            plt.scatter(kp[0], kp[1], c='green', marker='o', s=100, label='Ground Truth' if kp is gt_keypoints_np[0] else \"\")\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "def denormalize_keypoints(batch_keypoints, width=640, height=480):\n",
    "    denormalized_keypoints = []\n",
    "    for kp in batch_keypoints:\n",
    "        denormalized_x = (kp[:, 0] * (width / 2)) + (width / 2)\n",
    "        denormalized_y = (kp[:, 1] * (height / 2)) + (height / 2)\n",
    "        denormalized_kp = torch.stack((denormalized_x, denormalized_y), dim=1)\n",
    "        denormalized_keypoints.append(denormalized_kp)\n",
    "    denormalized_keypoints = torch.stack(denormalized_keypoints)\n",
    "    return denormalized_keypoints\n",
    "\n",
    "def process_batch_keypoints(target_dicts):\n",
    "    keypoints_batch = target_dicts['keypoints']  # Shape: (batch_size, num_keypoints, 1, 3)\n",
    "    keypoints_gt = keypoints_batch.squeeze(2)[:, :, :2]  # Shape: (batch_size, num_keypoints, 2)\n",
    "    \n",
    "    return keypoints_gt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels_1, hidden_channels_2,  out_channels):\n",
    "        super(GraphGCN, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels_1)\n",
    "        self.conv2 = GCNConv(hidden_channels_1, hidden_channels_1)\n",
    "        self.conv3 = SAGEConv(hidden_channels_1, hidden_channels_2)\n",
    "        self.conv4 = GCNConv(hidden_channels_2, hidden_channels_2)\n",
    "        self.fc = torch.nn.Linear(hidden_channels_2, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = x.cuda()\n",
    "        edge_index = edge_index.cuda()\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointV2Pipeline(nn.Module):\n",
    "    def __init__(self, weights_path, model_state_dict):\n",
    "        super(KeypointV2Pipeline, self).__init__()\n",
    "        self.keypoint_model = model = load_keypoint_pipeline(weights_path)\n",
    "        self.keypoint_model.load_state_dict(model_state_dict)\n",
    "        self.graph_gcn = GraphGCN(6,512,256,2)\n",
    "\n",
    "       \n",
    "    def process_model_output(self, output):\n",
    "#         print(\"Original output:\", output)\n",
    "\n",
    "        # Denormalize only the x and y coordinates\n",
    "        denormalized_keypoints = denormalize_keypoints(output)  # (batch_size, keypoints, 2)\n",
    "#         print(\"Denormalized Keypoints (x, y) Shape:\", denormalized_keypoints)\n",
    "        return denormalized_keypoints\n",
    "    \n",
    "    def apply_transform(self, img, pred_keypoints, gt_keypoints=None):\n",
    "#         print(\"Predicted keypoints for clamping\", pred_keypoints)\n",
    "        img_np = img.permute(1, 2, 0).cpu().numpy()  # Convert (C, H, W) -> (H, W, C)\n",
    "        # Prepare keypoints for transformation with all attributes (x, y, confidence, label)\n",
    "        \n",
    "        pred_keypoints_np = pred_keypoints[0].cpu().numpy().tolist()\n",
    "        \n",
    "        # Debugging: Check the format of pred_keypoints_np\n",
    "#         print(\"Predicted Keypoints (Before Clamping):\", pred_keypoints_np)\n",
    "\n",
    "\n",
    "        gt_keypoints_np = [kp[:2].tolist() for kp in gt_keypoints] if gt_keypoints is not None else []\n",
    "\n",
    "        # Get the original image height and width\n",
    "        orig_height, orig_width = img_np.shape[:2]\n",
    "\n",
    "        # Clamp keypoints to stay within the original image bounds BEFORE transformation\n",
    "        def clamp_keypoints(keypoints, max_width, max_height):            \n",
    "            clamped_kps = []\n",
    "            for kp in keypoints:\n",
    "                x, y = kp\n",
    "                # Clamp x and y within the image boundaries (max_width-1, max_height-1)\n",
    "                x_clamped = min(max(x, 0), max_width - 1)  # x should be within [0, max_width-1]\n",
    "                y_clamped = min(max(y, 0), max_height - 1)  # y should be within [0, max_height-1]\n",
    "                clamped_kps.append([x_clamped, y_clamped])\n",
    "            return clamped_kps\n",
    "\n",
    "        # Clamp the predicted keypoints to the original image bounds\n",
    "        pred_keypoints_np = clamp_keypoints(pred_keypoints_np, orig_width, orig_height)\n",
    "        \n",
    "#         print(\"Predicted Keypoints (After Clamping):\", pred_keypoints_np)\n",
    "\n",
    "        # Clamp the ground truth keypoints to the original image bounds if they exist\n",
    "        if gt_keypoints_np:\n",
    "            gt_keypoints_np = clamp_keypoints(gt_keypoints_np, orig_width, orig_height)\n",
    "\n",
    "        post_prediction_transform = A.Compose(\n",
    "            [\n",
    "                A.RandomRotate90(p=0.5),\n",
    "                A.HorizontalFlip(p=0.2),\n",
    "                A.VerticalFlip(p=0.2),\n",
    "                A.OneOf([\n",
    "                    A.Resize(height=random.randint(240, 720), width=random.randint(320, 1280)),\n",
    "                    A.NoOp()  # No operation (keeps original size)\n",
    "                ], p=0.5)  # 50% chance to resize\n",
    "            ],\n",
    "            keypoint_params=A.KeypointParams(format='xy', remove_invisible=True)\n",
    "        )\n",
    "\n",
    "        # Apply the transformation after clamping\n",
    "        transformed = post_prediction_transform(\n",
    "            image=img_np, keypoints=pred_keypoints_np + gt_keypoints_np\n",
    "        )\n",
    "\n",
    "        transformed_img = F.to_tensor(transformed['image']).to(device)\n",
    "\n",
    "        # Get the new image size after transformation\n",
    "        new_height, new_width = transformed_img.shape[1], transformed_img.shape[2]\n",
    "\n",
    "        # Separate transformed predicted and ground truth keypoints\n",
    "        pred_kps_transformed = transformed['keypoints'][:len(pred_keypoints_np)]\n",
    "        gt_kps_transformed = transformed['keypoints'][len(pred_keypoints_np):] if gt_keypoints_np else None\n",
    "\n",
    "#         # Add back the confidence and labels to the transformed predicted keypoints\n",
    "#         pred_kps_transformed = [\n",
    "#             list(pred_kp) + pred_attr for pred_kp, pred_attr in zip(pred_kps_transformed, pred_attributes)\n",
    "#         ]\n",
    "\n",
    "        # Fix the warning by using .clone().detach()\n",
    "        pred_kps_transformed = torch.tensor(pred_kps_transformed, dtype=torch.float32).clone().detach().to(device)\n",
    "\n",
    "        # Convert ground truth keypoints to a tensor\n",
    "        gt_kps_transformed_tensor = (\n",
    "            torch.tensor(gt_kps_transformed, dtype=torch.float32).clone().detach().to(device) if gt_kps_transformed else None\n",
    "        )\n",
    "\n",
    "        return transformed_img, pred_kps_transformed, gt_kps_transformed_tensor\n",
    "    \n",
    "    def normalize_keypoints(self, keypoints, image_width, image_height):\n",
    "        keypoints[:, 0] = (keypoints[:, 0] - image_width / 2) / (image_width / 2)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] - image_height / 2) / (image_height / 2)\n",
    "        return keypoints    \n",
    "\n",
    "    def keypoints_to_graph(self, keypoints, image_width, image_height):\n",
    "        node_features = []\n",
    "        for i, kp in enumerate(keypoints):\n",
    "            x, y = kp\n",
    "            prev_kp = keypoints[i - 1]\n",
    "            next_kp = keypoints[(i + 1) % len(keypoints)]\n",
    "            dist_next, angle_next = calculate_distance_angle([x, y], next_kp[:2])\n",
    "            dist_prev, angle_prev = calculate_distance_angle([x, y], prev_kp[:2])\n",
    "            node_features.append([x, y, dist_next, angle_next, dist_prev, angle_prev])\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float32).to(device)\n",
    "        edge_index = torch.tensor([[i, (i + 1) % len(keypoints)] for i in range(len(keypoints))] + \n",
    "                                  [[(i + 1) % len(keypoints), i] for i in range(len(keypoints))], dtype=torch.long).t().contiguous().to(device)\n",
    "        \n",
    "#         print(Data(x=node_features, edge_index=edge_index))\n",
    "        return Data(x=node_features, edge_index=edge_index)\n",
    "    \n",
    "    def forward(self, imgs, gt_keypoints_batch=None):\n",
    "        keypoint_model_training = self.keypoint_model.training\n",
    "        self.keypoint_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = [self.keypoint_model(img.unsqueeze(0).to(device)) for img in imgs]\n",
    "\n",
    "        self.keypoint_model.train(mode=keypoint_model_training)\n",
    "\n",
    "        batch_pred_keypoints = [self.process_model_output(output) for output in batch_outputs]\n",
    "        \n",
    "        if gt_keypoints_batch is not None:  # If ground truth is provided (Training Mode)\n",
    "            batch_transformed_data = []\n",
    "            for img, pred_kp, gt_kp in zip(imgs, batch_pred_keypoints, gt_keypoints_batch):\n",
    "                transformed_img, pred_kp_transformed, gt_kp_transformed = self.apply_transform(img, pred_kp, gt_kp)\n",
    "#                 print(\"Pred KP transformed: \", pred_kp_transformed)\n",
    "                # Call the visualization function\n",
    "                visualize_keypoints(transformed_img, pred_kp_transformed, gt_kp_transformed, title=\"Transformed Keypoints\")\n",
    "               \n",
    "                pred_kp_normalized = self.normalize_keypoints(pred_kp_transformed, transformed_img.shape[1], transformed_img.shape[0])      \n",
    "\n",
    "                batch_transformed_data.append((pred_kp_transformed, pred_kp_normalized, gt_kp_transformed))\n",
    "                \n",
    "\n",
    "            all_graphs = [self.keypoints_to_graph(kp, transformed_img.shape[1], transformed_img.shape[0]) for _, kp, _ in batch_transformed_data]\n",
    "            all_predictions = [self.graph_gcn(graph.x, graph.edge_index) for graph in all_graphs]\n",
    "\n",
    "            return torch.stack(all_predictions), [gt_kp for _, _, gt_kp in batch_transformed_data], [init_kp for init_kp, _, _ in batch_transformed_data], transformed_img.shape[1], transformed_img.shape[0]\n",
    "        \n",
    "        else:  # Inference Mode\n",
    "            # Step 5: Convert keypoints to graph representations\n",
    "            all_graphs = [\n",
    "                self.keypoints_to_graph(kp, 640, 480)  # Assume fixed image size; adapt as needed\n",
    "                for kp in batch_pred_keypoints\n",
    "            ]\n",
    "\n",
    "            # Step 6: Pass graphs through Graph GCN\n",
    "            all_predictions = [\n",
    "                self.graph_gcn(graph.x, graph.edge_index) for graph in all_graphs\n",
    "            ]\n",
    "\n",
    "            # Step 7: Return Graph GCN predictions (refined keypoints)\n",
    "            return all_predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgnn2d_loss(gt_keypoints, pred_keypoints, gt_distances_next, gt_angles_next, gt_distances_prev, gt_angles_prev, pred_distances_next, pred_angles_next, pred_distances_prev, pred_angles_prev):\n",
    "    keypoints_loss = func.mse_loss(pred_keypoints, gt_keypoints)\n",
    "    prev_distances_loss = func.mse_loss(pred_distances_prev, gt_distances_prev)\n",
    "    prev_angles_loss = func.mse_loss(pred_angles_prev, gt_angles_prev)\n",
    "    next_distances_loss = func.mse_loss(pred_distances_next, gt_distances_next)\n",
    "    next_angles_loss = func.mse_loss(pred_angles_next, gt_angles_next)\n",
    "    return keypoints_loss + prev_distances_loss + prev_angles_loss + next_distances_loss + next_angles_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9dc372",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_model = KeypointV2Pipeline(weights_path, model_state_dict)\n",
    "train_model = train_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(train_model.parameters(), lr=0.0001)\n",
    "scaler = GradScaler()\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 2\n",
    "\n",
    "split_folder_path = train_test_split(root_dir)\n",
    "KEYPOINTS_FOLDER_TRAIN = split_folder_path +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = split_folder_path +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = split_folder_path +\"/test\"\n",
    "\n",
    "dataset_train = GTDataset(KEYPOINTS_FOLDER_TRAIN)\n",
    "dataset_val = GTDataset(KEYPOINTS_FOLDER_VAL)\n",
    "dataset_test = GTDataset(KEYPOINTS_FOLDER_TEST)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "\n",
    "checkpoint_dir = '/home/jc-merlab/Pictures/Data/trained_models/gcn_sage_ckpt_v2/'\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_epoch = 0\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    train_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    train_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, target_dicts, _ in data_loader_train:\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        optimizer.zero_grad()\n",
    "        gt_keypoints_batch = process_batch_keypoints(target_dicts)\n",
    "\n",
    "        with autocast():\n",
    "            KGNN2D, transformed_gt_keypoints, init_kp_denorm, width, height = train_model(imgs, gt_keypoints_batch)\n",
    "#             print(init_kp_denorm)\n",
    "#             print(\"Ground truth keypoints\", transformed_gt_keypoints)\n",
    "            denormalized_keypoints = denormalize_keypoints(KGNN2D, width, height)\n",
    "            transformed_gt_keypoints = torch.stack(transformed_gt_keypoints)\n",
    "            init_kp_denorm = torch.stack(init_kp_denorm)\n",
    "#             print(\"Final predicted keypoints\", denormalized_keypoints)\n",
    "            gt_distances_angles = calculate_gt_distances_angles(transformed_gt_keypoints)\n",
    "            init_distances_angles = calculate_gt_distances_angles(init_kp_denorm)\n",
    "            pred_distances_angles = calculate_gt_distances_angles(denormalized_keypoints)\n",
    "            loss_kprcnn = kgnn2d_loss(transformed_gt_keypoints, init_kp_denorm, gt_distances_angles[:, :, 0],\n",
    "                                      gt_distances_angles[:, :, 1], gt_distances_angles[:, :, 2],\n",
    "                                      gt_distances_angles[:, :, 3], init_distances_angles[:, :, 0],\n",
    "                                      init_distances_angles[:, :, 1], init_distances_angles[:, :, 2],\n",
    "                                      init_distances_angles[:, :, 3])\n",
    "            loss_kgnn2d = kgnn2d_loss(transformed_gt_keypoints, denormalized_keypoints, gt_distances_angles[:, :, 0],\n",
    "                                      gt_distances_angles[:, :, 1], gt_distances_angles[:, :, 2],\n",
    "                                      gt_distances_angles[:, :, 3], pred_distances_angles[:, :, 0],\n",
    "                                      pred_distances_angles[:, :, 1], pred_distances_angles[:, :, 2],\n",
    "                                      pred_distances_angles[:, :, 3])\n",
    "            \n",
    "            final_loss = loss_kprcnn + loss_kgnn2d\n",
    "\n",
    "        # Check gradients after backward\n",
    "        scaler.scale(final_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += final_loss.item()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader_train)}')\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afabdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f576080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce0ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

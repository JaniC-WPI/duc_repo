{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b355d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2189cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "#         A.Resize(640, 480)  # Resize all images to be 640x480\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9395a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f915be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 7, # Background is the first class, object is the second class\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, vertices_dim=5, hidden_dim=128, num_vertices=6):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.f_enc = nn.Linear(vertices_dim, hidden_dim)\n",
    "        self.f_e1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.f_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.f_e2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.num_vertices = num_vertices\n",
    "        \n",
    "def generate_edges(self, vertices):\n",
    "        vertices_list = vertices.tolist()\n",
    "        G = nx.Graph()\n",
    "        for idx, vertex in enumerate(vertices_list):\n",
    "            G.add_node(idx, x=vertex[0], y=vertex[1], t=vertex[2])  \n",
    "            if idx < len(vertices_list) - 1:\n",
    "                # Add the edge with weight as the Euclidean distance between vertices\n",
    "                edge_weight = torch.dist(vertices[idx], vertices[idx + 1]).item()\n",
    "                G.add_edge(idx, idx + 1, weight=edge_weight)\n",
    "        # Add the edge between the last and the first vertex to form a cycle\n",
    "#         edge_weight = torch.dist(vertices[-1], vertices[0]).item()\n",
    "#         G.add_edge(len(vertices_list) - 1, 0, weight=edge_weight)\n",
    "        edges = list(G.edges(data=True))\n",
    "        edges_tensor = torch.tensor([(edge[0], edge[1]) for edge in edges], dtype=torch.long).to(vertices.device)\n",
    "        edges_weights = torch.tensor([edge[2]['weight'] for edge in edges], dtype=torch.float32).to(vertices.device)\n",
    "        return edges_tensor, edges_weights\n",
    "\n",
    "    def forward(self, vertices):\n",
    "        h1 = self.f_enc(vertices)\n",
    "        edges = self.generate_edges(vertices)\n",
    "        h1_source = h1[edges[:, 0]]\n",
    "        h1_target = h1[edges[:, 1]]\n",
    "        h_e1 = self.f_e1(torch.cat((h1_source, h1_target, edges_weights.unsqueeze(1)), dim=1))  # Include edge weights in the input\n",
    "        h_j_2 = self.f_v(h_e1)\n",
    "        h2_source = h_j_2[edges[:, 0]]\n",
    "        h2_target = h_j_2[edges[:, 1]]\n",
    "        h_e2 = self.f_e2(torch.cat((h2_source, h2_target), dim=1))\n",
    "        h_e2_prob = torch.sigmoid(h_e2)\n",
    "        return vertices, h_e2_prob, edges\n",
    "\n",
    "class GNNDecoder(nn.Module):\n",
    "    def __init__(self, vertices_dim=5, hidden_dim=128, num_vertices=6):\n",
    "        super(GNNDecoder, self).__init__()\n",
    "        self.f_e = nn.Linear(vertices_dim * 2, hidden_dim)  # Concatenate two vertices features\n",
    "        self.f_h = nn.Linear(hidden_dim, vertices_dim)  # Transform h_ij to the same dimension as vertices\n",
    "        self.f_v = nn.Linear(vertices_dim, vertices_dim)  # Update vertex feature\n",
    "\n",
    "    def forward(self, vertices, h_e2_prob, edges):\n",
    "        h_source = vertices[edges[:, 0]]\n",
    "        h_target = vertices[edges[:, 1]]\n",
    "        h = torch.zeros_like(vertices)\n",
    "\n",
    "        for idx, (i, j) in enumerate(edges):  # Iterate over edges\n",
    "            edge_weight = edges_weights[idx].unsqueeze(0)\n",
    "            h_ij = h_e2_prob[idx] * self.f_e(torch.cat((h_source[idx], h_target[idx], edge_weight), dim=0))  # Include edge weights in the input\n",
    "            h_ij_transformed = self.f_h(h_ij)  # Transform h_ij to the same dimension as vertices\n",
    "            h[j] += h_ij_transformed  # Accumulate edge features to the target vertex\n",
    "\n",
    "        h_transformed = self.f_v(h.view(-1, vertices.shape[1]))  # Transform h\n",
    "        h_transformed = h_transformed.view(vertices.shape)  # Reshape back to original shape\n",
    "        vertices_g = vertices + h_transformed  # Update vertex features\n",
    "\n",
    "        return vertices_g  # Return vertices_g as the prediction and vertices_g itself as the mean for Gaussian distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrifocalLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, vertices_pred, vertices_gt):\n",
    "#         # Only consider the first two dimensions\n",
    "#         vertices_pred = vertices_pred[:, :3]\n",
    "#         vertices_gt = vertices_gt.squeeze()[:, :3]  # Use squeeze() to remove the singular dimension\n",
    "\n",
    "#         loss = (vertices_gt - torch.tensor(vertices_pred)).pow(2).mean()  # Changed from sum() to mean()\n",
    "#         return loss\n",
    "# class HuberLoss(nn.Module):\n",
    "#     def __init__(self, delta=1.0):\n",
    "#         super().__init__()\n",
    "#         self.delta = delta\n",
    "\n",
    "#     def forward(self, vertices_pred, vertices_gt):\n",
    "#         vertices_pred = vertices_pred[:, :3]\n",
    "#         vertices_gt = vertices_gt.squeeze()[:, :3]\n",
    "#         diff = (vertices_gt - vertices_pred).abs()\n",
    "#         loss = torch.where(diff < self.delta, 0.5 * diff.pow(2), self.delta * (diff - 0.5 * self.delta))\n",
    "#         return loss.mean()\n",
    "# def cross_entropy_loss_func(edges_prob, edges_gt):\n",
    "#     edges_gt_expanded = edges_gt.unsqueeze(-1).float()\n",
    "#     loss_func = nn.BCEWithLogitsLoss()\n",
    "#     loss = loss_func(edges_prob, edges_gt_expanded)\n",
    "#     return loss\n",
    "    \n",
    "# class VisibleHuberLoss(nn.Module):\n",
    "#     def __init__(self, delta=1.0):\n",
    "#         super().__init__()\n",
    "#         self.delta = delta\n",
    "\n",
    "#     def forward(self, vertices_pred, vertices_gt):\n",
    "#         print(\"Vertice_gt inside huber\", vertices_gt)\n",
    "#         visibility = vertices_gt[:, 3]  # extracting the visibility\n",
    "#         vertices_pred = vertices_pred[:, :3]  # considering only x, y coordinates, confidence_score\n",
    "#         vertices_gt = vertices_gt.squeeze()[:, :3]  # considering only x, y coordinates, confidence_score\n",
    "#         print(f'vertices_pred shape: {vertices_pred.shape}')  # Debugging print\n",
    "#         print(f'vertices_gt shape: {vertices_gt.shape}')  # Debugging print\n",
    "#         diff = (vertices_gt - vertices_pred).abs()\n",
    "#         loss = torch.where(diff < self.delta, 0.5 * diff.pow(2), self.delta * (diff - 0.5 * self.delta))\n",
    "#         # Multiply by visibility\n",
    "#         weighted_loss = visibility[:, None] * loss  # using None to keep dimensions consistent\n",
    "#         return weighted_loss.mean()\n",
    "\n",
    "class OccludedKeyPointLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, vertices_pred, vertices_gt):\n",
    "#         vertices_gt = vertices_gt.squeeze()\n",
    "        visibility = vertices_gt[:, 3].unsqueeze(1)  # Extracting the visibility\n",
    "        vertices_pred = vertices_pred[:, :3]  # Considering only x, y coordinates, confidence_score\n",
    "        vertices_gt = vertices_gt[:, :3]  # Considering only x, y coordinates, confidence_score\n",
    "\n",
    "        # Compute differences\n",
    "        diff = (vertices_gt - vertices_pred).abs()\n",
    "        # Compute Huber loss\n",
    "        huber_loss = torch.where(diff < self.delta, 0.5 * diff**2, self.delta * (diff - 0.5 * self.delta))\n",
    "\n",
    "        # Weighted loss\n",
    "#         weighted_loss = huber_loss * visibility\n",
    "\n",
    "        return huber_loss.mean()\n",
    "    \n",
    "def visibility_loss (vertices_pred, vertices_gt):\n",
    "    \n",
    "    return func.cross_entropy(vertices_pred[:, 3], vertices_gt[:, 3])  # Loss based on visibility of keypoints\n",
    "\n",
    "def edge_loss(edges_prob, edges_gt):\n",
    "    # Expand edges_gt to match the shape of edges_prob\n",
    "#     print(edges_prob.shape)\n",
    "    edges_gt_expanded = torch.zeros(edges_prob.shape, dtype=torch.float32)\n",
    "    \n",
    "    for i in range(edges_gt.shape[0]):\n",
    "        u, v = edges_gt[i]\n",
    "#         print(\"u:\", u)\n",
    "#         print(\"v:\", v)\n",
    "#         print(\"edges_gt_expanded.shape:\", edges_gt_expanded.shape)\n",
    "        if i < edges_gt_expanded.shape[0]:\n",
    "            if u < edges_gt_expanded.shape[1]:\n",
    "                edges_gt_expanded[i, u] = 1\n",
    "            if v < edges_gt_expanded.shape[1]:\n",
    "                edges_gt_expanded[i, v] = 1\n",
    "#         print('new u', u)\n",
    "#         print('new v', v)\n",
    "    # Compute the cross-entropy loss\n",
    "    loss = -torch.sum(edges_gt_expanded.to(device) * torch.log(torch.clamp(edges_prob, min=1e-7)))\n",
    "                      \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63300d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KeypointPipeline(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.keypoint_model = torch.load(weights_path).to(device)\n",
    "#         self.keypoint_model.eval()  # Set the model to evaluation mode\n",
    "#         self.keypoint_model = self.keypoint_model.to(device)\n",
    "#         self.gnn_encoder = GNNEncoder()\n",
    "#         self.gnn_decoder = GNNDecoder()\n",
    "        \n",
    "#     def forward(self, imgs):\n",
    "#         outputs = []\n",
    "#         for i in range(imgs.shape[0]):\n",
    "#             img = imgs[i].unsqueeze(0).to(device)  # Unsqueeze the 0th dimension to make a batch of size 1\n",
    "#             # Temporarily set the keypoint model to evaluation mode\n",
    "#             keypoint_model_training = self.keypoint_model.training  # Save the current mode\n",
    "#             self.keypoint_model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 output = self.keypoint_model(img)  # Keypoint model expects a list of images\n",
    "                \n",
    "#             # Set the keypoint model back to its previous mode\n",
    "#             self.keypoint_model.train(keypoint_model_training)\n",
    "                \n",
    "#             img = (img[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "#             scores = output[0]['scores'].detach().cpu().numpy()\n",
    "#             high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "#             post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \\\n",
    "#                 output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "#             keypoints = []\n",
    "#             key_points = []\n",
    "#             for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "#                 keypoints.append(list(map(int, kps[0,0:2])))\n",
    "#                 key_points.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "# #             print(\"keypoints\", keypoints)\n",
    "\n",
    "#             labels = []\n",
    "#             for label in output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "#                 labels.append(label)\n",
    "#     #             labels.append('j' + str(int(label)))\n",
    "    \n",
    "#             print(\"keypoints\", keypoints)\n",
    "\n",
    "# #             print(\"labels\", labels)\n",
    "\n",
    "# #             keypoints_ = [x for _,x in sorted(zip(labels,keypoints))]\n",
    "#     #         kp_label = [list(x) + [y] for (x, y) in sorted(zip(keypoints, labels))]\n",
    "\n",
    "#             # Create a dictionary where the key is the label and the value is the keypoint\n",
    "#             label_keypoint_dict = {lbl: kp for kp, lbl in zip(keypoints, labels)}\n",
    "\n",
    "#             # Convert the dictionary back to a list and sort it by the label keys\n",
    "#             labeled_keypoints = [value + [key] for key, value in sorted(label_keypoint_dict.items())] #,key=lambda item: int(item[0][1:]))]\n",
    "\n",
    "# #             print(\"keypoints_\", keypoints_)\n",
    "#             print(\"kp_label\", labeled_keypoints)\n",
    "# # \n",
    "#             keypoints = torch.stack([torch.tensor(kp) for kp in labeled_keypoints]).float().to(device)\n",
    "#             vertices, enc_e, edges = self.gnn_encoder(keypoints)\n",
    "#             updated_vertices = self.gnn_decoder(vertices, enc_e, edges)\n",
    "#             outputs.append((updated_vertices, enc_e, edges))\n",
    "\n",
    "#         return outputs  # A list of tuples, each containing updated_vertices, enc_e, edges for an image in the batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path, num_vertices):\n",
    "        super().__init__()\n",
    "\n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.num_vertices = num_vertices\n",
    "        self.gnn_encoder = GNNEncoder()\n",
    "        self.gnn_decoder = GNNDecoder()\n",
    "\n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \n",
    "                                            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "\n",
    "        confidence = output[0]['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            # Setting t_i = 1 because label is found\n",
    "            keypoints.append(list(map(int, kps[0,0:2])) + [confidence[idx]] + [1] + [labels[idx]])\n",
    "\n",
    "        # Create a dictionary where the key is the label and the value is the keypoint\n",
    "        label_to_keypoint = {}\n",
    "        for keypoint in keypoints:\n",
    "            label = keypoint[-1]\n",
    "            if label not in label_to_keypoint or label_to_keypoint[label][-2] < keypoint[-2]:\n",
    "                label_to_keypoint[label] = keypoint\n",
    "\n",
    "        # Use a dictionary to keep track of all possible keypoints and their locations.\n",
    "        # Initialize with placeholders for missing keypoints.\n",
    "        all_keypoints = {i: [0, 0, 0, 0, i] for i in range(1, self.num_vertices+1)}  # added another 0 for t_i\n",
    "\n",
    "        for label, keypoint in label_to_keypoint.items():\n",
    "            all_keypoints[label] = keypoint\n",
    "\n",
    "        # Convert the dictionary values back into a list\n",
    "        keypoints = list(all_keypoints.values())\n",
    "        keypoints = torch.stack([torch.tensor(kp) for kp in keypoints]).float().to(device)\n",
    "        visibility = keypoints[:, 3].unsqueeze(1)  # Extracting the visibility\n",
    "        keypoints_visible = keypoints * visibility  # Predicted visible vertices\n",
    "        keypoints_occluded = keypoints * (1 - visibility)  # Predicted occluded vertices\n",
    "\n",
    "        vertices, self.enc_e, self.edges = self.gnn_encoder(keypoints_visible)\n",
    "        vertices_pred = self.gnn_decoder(vertices, self.enc_e, self.edges)\n",
    "        vertices_pred_occluded = torch.cat((vertices_pred, keypoints_visible[:, 3].unsqueeze(1)), dim=1)\n",
    "        nonzero_indices = keypoints_occluded.nonzero(as_tuple=True)\n",
    "        if nonzero_indices[0].size()[0] > 0:  # Check if there are any non-zero elements\n",
    "            keypoints_occluded[nonzero_indices] = vertices_pred_occluded[nonzero_indices]\n",
    "\n",
    "            \n",
    "        print(keypoints_visible + keypoints_occluded)\n",
    "        return keypoints_visible + keypoints_occluded\n",
    "\n",
    "    def process_image(self, img):\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        # Temporarily set the keypoint model to evaluation mode\n",
    "        keypoint_model_training = self.keypoint_model.training  # Save the current mode\n",
    "        self.keypoint_model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.keypoint_model(img)\n",
    "        # Set the keypoint model back to its previous mode\n",
    "        self.keypoint_model.train(keypoint_model_training)\n",
    "        img = (img[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        labeled_keypoints = self.process_model_output(output)\n",
    "\n",
    "        return labeled_keypoints\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(imgs.shape[0]):\n",
    "            labeled_keypoints = self.process_image(imgs[i])\n",
    "            outputs.append(labeled_keypoints)\n",
    "            \n",
    "        print(outputs)\n",
    "\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fef485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KeypointPipeline(nn.Module):\n",
    "#     def __init__(self, num_keypoints, weights_path=None):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Instantiate your Keypoint R-CNN model\n",
    "#         self.keypoint_model = get_model(num_keypoints, weights_path)\n",
    "#         self.keypoint_model.to(device)\n",
    "\n",
    "#         self.gnn_encoder = GNNEncoder()\n",
    "#         self.gnn_decoder = GNNDecoder()\n",
    "\n",
    "#     def process_model_output(self, output):\n",
    "#         scores = output[0]['scores'].detach().cpu().numpy()\n",
    "#         high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "\n",
    "#         post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \n",
    "#                                             output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "\n",
    "#         keypoints = []\n",
    "#         for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "#             keypoints.append(list(map(int, kps[0,0:2])))\n",
    "\n",
    "#         labels = []\n",
    "#         for label in output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "#             labels.append(label)\n",
    "\n",
    "#         label_keypoint_dict = {lbl: kp for kp, lbl in zip(keypoints, labels)}\n",
    "#         labeled_keypoints = [value + [key] for key, value in sorted(label_keypoint_dict.items())]\n",
    "\n",
    "#         return labeled_keypoints\n",
    "\n",
    "#     def process_image(self, img):\n",
    "#         img = img.unsqueeze(0).to(device)\n",
    "#         # Temporarily set the keypoint model to evaluation mode\n",
    "#         keypoint_model_training = self.keypoint_model.training  # Save the current mode\n",
    "#         self.keypoint_model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             output = self.keypoint_model(img)\n",
    "#         print(\"Output\", output)\n",
    "#         # Set the keypoint model back to its previous mode\n",
    "#         self.keypoint_model.train(keypoint_model_training)\n",
    "#         img = (img[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "#         labeled_keypoints = self.process_model_output(output)\n",
    "\n",
    "#         return labeled_keypoints\n",
    "\n",
    "#     def forward(self, imgs):\n",
    "#         outputs = []\n",
    "\n",
    "#         for i in range(imgs.shape[0]):\n",
    "#             labeled_keypoints = self.process_image(imgs[i])\n",
    "#             keypoints = torch.stack([torch.tensor(kp) for kp in labeled_keypoints]).float().to(device)\n",
    "\n",
    "#             vertices, enc_e, edges = self.gnn_encoder(keypoints)\n",
    "#             updated_vertices = self.gnn_decoder(vertices, enc_e, edges)\n",
    "#             outputs.append((updated_vertices, enc_e, edges))\n",
    "\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ce7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = KeypointPipeline(weights_path, num_vertices=6)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss\n",
    "criterion = OccludedKeyPointLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 25  # Define your number of epochs\n",
    "batch_size = 8\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "v = 4\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for i, batch in enumerate(data_loader_train):\n",
    "        img_tuple, target_dict_tuple, img_files = batch\n",
    "        print(f\"Processing batch {i+1} with images:\", img_files)\n",
    "        \n",
    "        imgs = [img.to(device) for img in img_tuple]  # Create list of images\n",
    "\n",
    "        # Process each image individually\n",
    "        losses = []\n",
    "        for i in range(len(imgs)):\n",
    "            img = imgs[i].unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "\n",
    "            # Prepare ground truth vertices for the image\n",
    "            keypoints = target_dict_tuple[i]['keypoints'].to(device)\n",
    "            visibility = torch.ones((keypoints.shape[0], keypoints.shape[1], 1)).to(device)\n",
    "            vertices_gt = torch.cat((keypoints, visibility), dim=2).unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "            vertices_gt = vertices_gt.squeeze()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(img)\n",
    "            vertices_pred = output[0]\n",
    "            edges_prob = model.enc_e\n",
    "            edges_gt = model.edges\n",
    "\n",
    "            # Compute loss for the image\n",
    "            huber_loss = criterion(vertices_pred, vertices_gt)\n",
    "            ce_loss = edge_loss(edges_prob, edges_gt)\n",
    "            vis_loss = visibility_loss(vertices_pred, vertices_gt)\n",
    "\n",
    "            loss = huber_loss + ce_loss + vis_loss\n",
    "            losses.append(loss)  # Store loss for the image\n",
    "\n",
    "        # Average loss over all images in the batch\n",
    "        loss = torch.mean(torch.stack(losses))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    eta = epoch_time * (num_epochs - epoch - 1)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, ETA: {eta} seconds')\n",
    "\n",
    "model_save_path = f\"/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_occ_b{batch_size}_e{num_epochs}_v{v}.pth\"\n",
    "\n",
    "torch.save(model, model_save_path)\n",
    "    \n",
    "# Save the state dict of the model, not the entire model\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "torch.save(model, model_save_path)\n",
    "\n",
    "\n",
    "# model.train()\n",
    "# # Epoch loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     # For each batch in your training data\n",
    "#     for batch in data_loader_train:\n",
    "#         img_tuple, target_dict_tuple = batch\n",
    "#         img = img_tuple[0]\n",
    "# #         print(img.shape)\n",
    "#         target = target_dict_tuple[0]\n",
    "#         img = img.to(device)\n",
    "#         vertices_gt = target['keypoints'].to(device)\n",
    "#         num_vertices = vertices_gt.shape[0]\n",
    "#         print(num_vertices)\n",
    "#         vertices_gt[:, :, 2] = torch.arange(1, num_vertices+1).unsqueeze(1).to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         vertices_pred, edges_prob, edges_gt = model(img)\n",
    "        \n",
    "#         # Compute the losses\n",
    "#         trifocal_loss = criterion(vertices_pred, vertices_gt)\n",
    "#         ce_loss = edge_loss(edges_prob, edges_gt)\n",
    "\n",
    "#         # Combined loss\n",
    "#         loss = trifocal_loss + ce_loss\n",
    "# #         loss = trifocal_loss\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "        \n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Print loss for each epoch\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfef6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save(img, vertices, filename):\n",
    "    print(\"type of image befor conversion\",type(img))    \n",
    "    print(\"type of vertices before conversion\", type(vertices))\n",
    "    print(img)\n",
    "    img = (img.permute(1,2,0).cpu().numpy() * 255).astype(np.uint8)\n",
    "#     img = (img * 255).astype(np.uint8)  # Convert back from [0, 1] range to [0, 255]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    vertices = vertices.cpu().numpy()\n",
    "\n",
    "    print(f\"Image shape before saving: {img.shape}\")  # print the image shape\n",
    "    print(\"type of vertices\", type(vertices))\n",
    "#     print(\"entered vertices\", vertices)\n",
    "#     print(\"entered image\", img)\n",
    "\n",
    "    # Convert grayscale to BGR if necessary\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "    for i in range(vertices.shape[0]):\n",
    "        img = cv2.circle(img, (int(vertices[i, 0]), int(vertices[i, 1])), radius=2, color=(0, 0, 255), thickness=-1)\n",
    "        \n",
    "    result = cv2.imwrite(filename, img)\n",
    "    print(f\"Image saved at {filename}: {result}\")  # print if save was successful\n",
    "\n",
    "    # If the image didn't save correctly, save the image data to a text file for examination\n",
    "    if not result:\n",
    "        with open(filename + \".txt\", \"w\") as f:\n",
    "            np.savetxt(f, img.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_save_model(model, data_loader_test):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_trifocal_loss = 0.0\n",
    "    total_ce_loss = 0.0\n",
    "    total_vis_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # We don't need to track gradients during evaluation\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(data_loader_test):\n",
    "            img_tuple, target_dict_tuple, img_file = batch\n",
    "\n",
    "            total_batch_loss = 0.0\n",
    "            total_batch_trifocal_loss = 0.0\n",
    "            total_batch_ce_loss = 0.0\n",
    "            total_batch_vis_loss = 0.0\n",
    "\n",
    "            for i in range(len(img_tuple)):\n",
    "                img = img_tuple[i].to(device)\n",
    "                target = target_dict_tuple[i]\n",
    "\n",
    "                # Prepare ground truth vertices for the image\n",
    "                keypoints = target['keypoints'].to(device)\n",
    "                visibility = torch.ones((keypoints.shape[0], keypoints.shape[1], 1)).to(device)\n",
    "                vertices_gt = torch.cat((keypoints, visibility), dim=2).unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "                vertices_gt = vertices_gt.squeeze()\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(img.unsqueeze(0))\n",
    "                vertices_pred = output[0]\n",
    "                edges_prob = model.enc_e\n",
    "                edges_gt = model.edges\n",
    "\n",
    "                # Print the shapes for debugging\n",
    "                print(f\"img shape: {img.shape}, vertices_pred shape: {vertices_pred.shape}\")\n",
    "\n",
    "                # Compute the losses\n",
    "                trifocal_loss = criterion(vertices_pred, vertices_gt)\n",
    "                ce_loss = edge_loss(edges_prob, edges_gt)\n",
    "                vis_loss = visibility_loss(vertices_pred, vertices_gt)\n",
    "\n",
    "                # Combined loss\n",
    "                loss = trifocal_loss + ce_loss + vis_loss\n",
    "\n",
    "                total_batch_loss += loss.item()\n",
    "                total_batch_trifocal_loss += trifocal_loss.item()\n",
    "                total_batch_ce_loss += ce_loss.item()\n",
    "                total_batch_vis_loss += vis_loss.item()\n",
    "\n",
    "                # Visualize and save the prediction\n",
    "                filename = f'/home/jc-merlab/Pictures/Data/occ_vis_data/image_{idx}_{i}.jpg'\n",
    "                visualize_and_save(img, vertices_pred, filename)\n",
    "                print(f\"Image saved at {filename}\")  # Print statement to confirm image save\n",
    "\n",
    "            total_loss += total_batch_loss / len(img_tuple)\n",
    "            total_trifocal_loss += total_batch_trifocal_loss / len(img_tuple)\n",
    "            total_ce_loss += total_batch_ce_loss / len(img_tuple)\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_trifocal_loss = total_trifocal_loss / num_batches\n",
    "    avg_ce_loss = total_ce_loss / num_batches\n",
    "    \n",
    "    print(f'Avg. Test Loss: {avg_loss}, Avg. Trifocal Loss: {avg_trifocal_loss}, Avg. Cross Entropy Loss: {avg_ce_loss}')\n",
    "    return avg_loss, avg_trifocal_loss, avg_ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4397900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_loader_test):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_trifocal_loss = 0.0\n",
    "    total_ce_loss = 0.0\n",
    "    total_vis_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    all_vertices_pred = []  # List to store all predicted vertices\n",
    "    \n",
    "    # We don't need to track gradients during evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader_test:\n",
    "            img_tuple, target_dict_tuple, img_file = batch\n",
    "            \n",
    "            total_batch_loss = 0.0\n",
    "            total_batch_trifocal_loss = 0.0\n",
    "            total_batch_ce_loss = 0.0\n",
    "            \n",
    "\n",
    "            for i in range(len(img_tuple)):\n",
    "                img = img_tuple[i].to(device)\n",
    "                target = target_dict_tuple[i]\n",
    "\n",
    "                vertices_gt = target['keypoints'].to(device)\n",
    "                num_vertices = vertices_gt.shape[0]\n",
    "                vertices_gt[:, :, 2] = torch.arange(1, num_vertices+1).unsqueeze(1).to(device)\n",
    "                \n",
    "\n",
    "                # Forward pass\n",
    "                output = model(img.unsqueeze(0))\n",
    "                print(\"Output per img\", output[0])\n",
    "                vertices_pred, edges_prob, edges_gt = output[0]\n",
    "\n",
    "                # Compute the losses\n",
    "                trifocal_loss = criterion(vertices_pred, vertices_gt)\n",
    "                ce_loss = edge_loss(vertices_pred, vertices_gt)\n",
    "                vis_loss = \n",
    "\n",
    "                # Combined loss\n",
    "                loss = trifocal_loss + ce_loss + vis_loss\n",
    "\n",
    "                total_batch_loss += loss.item()\n",
    "                total_batch_trifocal_loss += trifocal_loss.item()\n",
    "                total_batch_ce_loss += ce_loss.item()\n",
    "\n",
    "                # Save the predictions for this image\n",
    "                all_vertices_pred.append(vertices_pred.cpu().numpy())\n",
    "            \n",
    "            total_loss += total_batch_loss / len(img_tuple)\n",
    "            total_trifocal_loss += total_batch_trifocal_loss / len(img_tuple)\n",
    "            total_ce_loss += total_batch_ce_loss / len(img_tuple)\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_trifocal_loss = total_trifocal_loss / num_batches\n",
    "    avg_ce_loss = total_ce_loss / num_batches\n",
    "    \n",
    "    print(f'Avg. Test Loss: {avg_loss}, Avg. Trifocal Loss: {avg_trifocal_loss}, Avg. Cross Entropy Loss: {avg_ce_loss}, All Predicted Vertices: {all_vertices_pred}')\n",
    "    return avg_loss, avg_trifocal_loss, avg_ce_loss, all_vertices_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_loss, avg_trifocal_loss, avg_ce_loss, all_preds = test_and_save_model(model, data_loader_test)\n",
    "\n",
    "avg_loss, avg_trifocal_loss, avg_ce_loss = test_and_save_model(model, data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save(img, vertices, filename):    \n",
    "    \n",
    "    img = img.squeeze().cpu().numpy()\n",
    "    img = (img * 255).astype(np.uint8)  # Convert back from [0, 1] range to [0, 255]\n",
    "    vertices = vertices.cpu().numpy()\n",
    "\n",
    "    # Convert grayscale to BGR if necessary\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "    for i in range(vertices.shape[0]):\n",
    "        img = cv2.circle(img, (int(vertices[i, 0]), int(vertices[i, 1])), radius=2, color=(0, 0, 255), thickness=-1)\n",
    "        \n",
    "    cv2.imwrite(filename, img)\n",
    "\n",
    "# Iterate over batches\n",
    "for idx, batch in enumerate(data_loader_test):\n",
    "    img_tuple, target_dict_tuple = batch\n",
    "\n",
    "    # Only visualize the first image of each batch\n",
    "    img = img_tuple[0].to(device)\n",
    "    outputs = model([img])\n",
    "    filename = f'image_with_vertices_batch_{idx}.jpg'  # unique filename for each batch\n",
    "    visualize_and_save(img, outputs[0][0], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40b993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58949932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Directory containing images\n",
    "dir_path = '/home/jc-merlab/Pictures/Data/occ_vis_data/'\n",
    "images = []\n",
    "\n",
    "# Ensure the images are sorted by name\n",
    "for f in sorted(os.listdir(dir_path)):\n",
    "    if f.endswith('.jpg') or f.endswith('.png'):  # Check for image file extension\n",
    "        images.append(f)\n",
    "\n",
    "# Determine the width and height from the first image\n",
    "image_path = os.path.join(dir_path, images[0])\n",
    "frame = cv2.imread(image_path)\n",
    "cv2.imshow('video',frame)\n",
    "height, width, channels = frame.shape\n",
    "\n",
    "# Define the codec and create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Be sure to use the correct codec\n",
    "video_filename = 'output.mp4'\n",
    "video = cv2.VideoWriter(video_filename, fourcc, 3.0, (width, height))\n",
    "\n",
    "for image in images:\n",
    "    image_path = os.path.join(dir_path, image)\n",
    "    frame = cv2.imread(image_path)\n",
    "    video.write(frame)  # Write out frame to video\n",
    "\n",
    "# Release everything when job is finished\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"The output video is\", video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0007f54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_occ_b8_e25_v4.pth'\n",
    "\n",
    "model = torch.load(weights_path).to(device)\n",
    "\n",
    "\n",
    "image = Image.open(\"/home/jc-merlab/Pictures/Data/occluded_results_mi20_ma80_n2/occluded_000027.rgb.jpg\")\n",
    "print(type(image))\n",
    "\n",
    "img = F.to_tensor(image).to(device)\n",
    "img.unsqueeze_(0)\n",
    "# print(image.shape)\n",
    "# image = list(image)\n",
    "# print(type(images))\n",
    "# images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(img)\n",
    "    \n",
    "keypoints = output[0]\n",
    "\n",
    "print(keypoints)\n",
    "plt.imshow(image)\n",
    "\n",
    "# Assuming each keypoint is a tensor representing (x, y)\n",
    "for i, keypoint in enumerate(keypoints):\n",
    "    print(f'Key point {i}: {keypoint}')\n",
    "    keypoint = keypoint.cpu().numpy()\n",
    "    plt.plot(keypoint[0], keypoint[1], 'ro')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the image\n",
    "\n",
    "# plt.imshow(image)\n",
    "\n",
    "# for keypoint in output[0]:\n",
    "#     plt.plot(keypoint[0], keypoint[1], 'ro')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d23048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7eb2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

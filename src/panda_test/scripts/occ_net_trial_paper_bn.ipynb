{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bedf1768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16908615680\n",
      "490733568\n",
      "293975040\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b355d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "# root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "root_dir = parent_path + \"occ_sim_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "237a5cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2189cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "#         A.Resize(640, 480)  # Resize all images to be 640x480\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aaae8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(0.95, 0.025, 0.025), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9395a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1df0e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch_geometric.nn as pyg\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "_EPS = 1e-10\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Two-layer fully-connected ELU net with batch norm.\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def batch_norm(self, inputs):\n",
    "        x = inputs.view(inputs.size(0) * inputs.size(1), -1)\n",
    "        x = self.bn(x)\n",
    "        return x.view(inputs.size(0), inputs.size(1), -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "#         print(\"Input shape before any operations: \", inputs.shape)\n",
    "\n",
    "        # Flatten the last two dimensions for the linear layer input\n",
    "#         x = inputs.view(inputs.size(0), -1)\n",
    "        x = func.elu(self.fc1(inputs))\n",
    "        x = func.dropout(x, self.dropout_prob, training=self.training)\n",
    "        x = func.elu(self.fc2(x))\n",
    "        \n",
    "        return self.batch_norm(x)\n",
    "\n",
    "        # Assuming you want to maintain the second dimension for some reason\n",
    "        # (like temporal sequence in a RNN), you would reshape the output\n",
    "        # back to the desired shape. If not, this step is unnecessary.\n",
    "        # output = x.view(inputs.size(0), inputs.size(1), -1)\n",
    "        # print(\"Output shape after forward pass: \", output.shape)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_out=4, do_prob=0., factor=True):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "\n",
    "        self.factor = factor\n",
    "\n",
    "        self.mlp1 = MLP(n_in, n_hid, n_hid, do_prob)\n",
    "        self.mlp2 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "        self.mlp3 = MLP(n_hid, n_hid, n_hid, do_prob)\n",
    "        if self.factor:\n",
    "            self.mlp4 = MLP(n_hid * 3, n_hid, n_hid, do_prob)\n",
    "            print(\"Using factor graph MLP encoder.\")\n",
    "        else:\n",
    "            self.mlp4 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "            print(\"mlp4\", self.mlp4)\n",
    "            print(\"Using MLP graph encoder.\")\n",
    "        self.fc_out = nn.Linear(n_hid, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def edge2node(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        incoming = torch.matmul(rel_rec.t(), x)\n",
    "        return incoming / incoming.size(1)\n",
    "\n",
    "    def node2edge(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        receivers = torch.matmul(rel_rec, x)\n",
    "        senders = torch.matmul(rel_send, x)\n",
    "        edges = torch.cat([receivers, senders], dim=2)\n",
    "        return edges\n",
    "\n",
    "    def forward(self, inputs, rel_rec, rel_send):\n",
    "        # Input shape: [num_sims, num_atoms, num_timesteps, num_dims]\n",
    "        x = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "#         print(\"x shape:\", x.shape)\n",
    "#         print(\"rel_rec shape:\", rel_rec.shape)\n",
    "#         print(\"rel_send shape:\", rel_send.shape)\n",
    "\n",
    "        # New shape: [num_sims, num_atoms, num_timesteps*num_dims]\n",
    "        x = self.mlp1(x)  # 2-layer ELU net per node\n",
    "\n",
    "        x = self.node2edge(x, rel_rec, rel_send)\n",
    "        x = self.mlp2(x)\n",
    "        x_skip = x    \n",
    "        \n",
    "        if self.factor:\n",
    "            x = self.edge2node(x, rel_rec, rel_send)\n",
    "            x = self.mlp3(x)\n",
    "            x = self.node2edge(x, rel_rec, rel_send)\n",
    "            x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "            x = self.mlp4(x)\n",
    "        else:\n",
    "            x = self.mlp3(x)\n",
    "            x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "            x = self.mlp4(x)\n",
    "\n",
    "        return self.fc_out(x)    \n",
    "    \n",
    "class GraphDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in_node, edge_types, msg_hid, msg_out, n_hid,\n",
    "                 do_prob=0., skip_first=False):\n",
    "        super(GraphDecoder, self).__init__()\n",
    "        self.msg_fc1 = nn.ModuleList(\n",
    "            [nn.Linear(2 * n_in_node, msg_hid) for _ in range(edge_types)])\n",
    "        self.msg_fc2 = nn.ModuleList(\n",
    "            [nn.Linear(msg_hid, msg_out) for _ in range(edge_types)])\n",
    "        self.msg_out_shape = msg_out\n",
    "        self.skip_first_edge_type = skip_first\n",
    "\n",
    "        self.out_fc1 = nn.Linear(n_in_node + msg_out, n_hid)\n",
    "        self.out_fc2 = nn.Linear(n_hid, n_hid)\n",
    "        self.out_fc3 = nn.Linear(n_hid, n_in_node)\n",
    "\n",
    "        print('Using learned graph decoder.')\n",
    "\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "    def single_step_forward(self, single_timestep_inputs, rel_rec, rel_send,\n",
    "                            single_timestep_rel_type):\n",
    "\n",
    "        # single_timestep_inputs has shape\n",
    "        # [batch_size, num_timesteps, num_atoms, num_dims]\n",
    "\n",
    "        # single_timestep_rel_type has shape:\n",
    "        # [batch_size, num_timesteps, num_atoms*(num_atoms-1), num_edge_types]\n",
    "\n",
    "        # Node2edge\n",
    "        receivers = torch.matmul(rel_rec, single_timestep_inputs)\n",
    "        senders = torch.matmul(rel_send, single_timestep_inputs)\n",
    "        pre_msg = torch.cat([receivers, senders], dim=-1)\n",
    "\n",
    "        all_msgs = Variable(torch.zeros(pre_msg.size(0), pre_msg.size(1),self.msg_out_shape))\n",
    "        if single_timestep_inputs.is_cuda:\n",
    "            all_msgs = all_msgs.cuda()\n",
    "\n",
    "        if self.skip_first_edge_type:\n",
    "            start_idx = 1\n",
    "        else:\n",
    "            start_idx = 0\n",
    "\n",
    "        # Run separate MLP for every edge type\n",
    "        # NOTE: To exlude one edge type, simply offset range by 1\n",
    "        for i in range(start_idx, len(self.msg_fc2)):\n",
    "            msg = func.relu(self.msg_fc1[i](pre_msg))\n",
    "            msg = func.dropout(msg, p=self.dropout_prob)\n",
    "            msg = func.relu(self.msg_fc2[i](msg))\n",
    "            msg = msg * single_timestep_rel_type[:, :, i:i + 1]\n",
    "            all_msgs += msg\n",
    "\n",
    "        # Aggregate all msgs to receiver\n",
    "        agg_msgs = all_msgs.transpose(-2, -1).matmul(rel_rec).transpose(-2, -1)\n",
    "        agg_msgs = agg_msgs.contiguous()\n",
    "\n",
    "        # Skip connection\n",
    "        aug_inputs = torch.cat([single_timestep_inputs, agg_msgs], dim=-1)\n",
    "\n",
    "        # Output MLP\n",
    "        pred = func.dropout(func.relu(self.out_fc1(aug_inputs)), p=self.dropout_prob)\n",
    "        pred = func.dropout(func.relu(self.out_fc2(pred)), p=self.dropout_prob)\n",
    "        pred = self.out_fc3(pred)\n",
    "#        print(pred.shape,single_timestep_inputs.shape)\n",
    "\n",
    "        # Predict position/velocity difference\n",
    "        return single_timestep_inputs + pred\n",
    "\n",
    "    def forward(self, inputs, rel_type, rel_rec, rel_send, pred_steps=1):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "\n",
    "\n",
    "        # Only take n-th timesteps as starting points (n: pred_steps)\n",
    "        last_pred = inputs[:, :, :]\n",
    "        #asa\n",
    "        curr_rel_type = rel_type[:, :, :]\n",
    "        preds=[]\n",
    "        #print(curr_rel_type.shape)\n",
    "        # NOTE: Assumes rel_type is constant (i.e. same across all time steps).\n",
    "\n",
    "        # Run n prediction steps\n",
    "        #for step in range(0, pred_steps):\n",
    "        last_pred = self.single_step_forward(last_pred, rel_rec, rel_send,\n",
    "                                                 curr_rel_type)\n",
    "        preds.append(last_pred)\n",
    "\n",
    "        sizes = [preds[0].size(0), preds[0].size(1),\n",
    "                 preds[0].size(2)]\n",
    "\n",
    "        output = Variable(torch.zeros(sizes))\n",
    "        if inputs.is_cuda:\n",
    "            output = output.cuda()\n",
    "\n",
    "        # Re-assemble correct timeline\n",
    "        for i in range(len(preds)):\n",
    "            output[:, :, :] = preds[i]\n",
    "\n",
    "        pred_all = output[:, :, :]\n",
    "\n",
    "        # NOTE: We potentially over-predicted (stored in future_pred). Unused.\n",
    "        # future_pred = output[:, (inputs.size(1) - 1):, :, :]\n",
    "\n",
    "        return pred_all#.transpose(1, 2).contiguous()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d61bf273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(input, axis=1):\n",
    "    trans_input = input.transpose(axis, 0).contiguous()\n",
    "    soft_max_1d = func.softmax(trans_input,dim=0)\n",
    "    return soft_max_1d.transpose(axis, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a4b6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path):\n",
    "        super(KeypointPipeline, self).__init__()  \n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.encoder = GraphEncoder(4,512,4,0.5,True)\n",
    "        self.decoder = GraphDecoder(n_in_node=4,\n",
    "                                 edge_types=2,\n",
    "                                 msg_hid=512,\n",
    "                                 msg_out=512,\n",
    "                                 n_hid=512,\n",
    "                                 do_prob=0.5,\n",
    "                                 skip_first=False)\n",
    "        \n",
    "#         self.off_diag = np.ones([6,6]) - np.eye(6)\n",
    "\n",
    "#         self.rel_rec = np.array(encode_onehot(np.where(self.off_diag)[1]), dtype=np.float32)\n",
    "#         self.rel_send = np.array(encode_onehot(np.where(self.off_diag)[0]), dtype=np.float32)\n",
    "#         self.rel_rec = torch.FloatTensor(self.rel_rec)\n",
    "#         self.rel_send = torch.FloatTensor(self.rel_send)\n",
    "\n",
    "        num_nodes = 6\n",
    "        self.off_diag = np.zeros([num_nodes, num_nodes])        \n",
    "#         # Creating a cycle: 1->2, 2->3, ..., 6->1\n",
    "#         for i in range(num_nodes):\n",
    "#             self.off_diag[i, (i + 1) % num_nodes] = 1\n",
    "\n",
    "#         # Creating a bidirectional cycle\n",
    "#         for i in range(num_nodes):\n",
    "#             # Forward connection: i -> (i + 1) % num_nodes\n",
    "#             self.off_diag[i, (i + 1) % num_nodes] = 1\n",
    "\n",
    "#             # Backward connection: i -> (i - 1 + num_nodes) % num_nodes\n",
    "#             # The addition of num_nodes before modulo ensures a positive index\n",
    "#             self.off_diag[i, (i - 1 + num_nodes) % num_nodes] = 1\n",
    "\n",
    "        # Creating a bidirectional, non-cyclic graph\n",
    "        for i in range(num_nodes):\n",
    "            # Forward connection: i -> (i + 1), except for the last node\n",
    "            if i < num_nodes - 1:  # This prevents connecting the last node to the first\n",
    "                self.off_diag[i, i + 1] = 1\n",
    "\n",
    "            # Backward connection: i -> (i - 1), except for the first node\n",
    "            if i > 0:  # This prevents connecting the first node to the last\n",
    "                self.off_diag[i, i - 1] = 1\n",
    "\n",
    "        # Update rel_rec and rel_send based on the new off_diag\n",
    "        self.rel_rec = np.array(encode_onehot(np.where(self.off_diag)[1]), dtype=np.float32)\n",
    "        self.rel_send = np.array(encode_onehot(np.where(self.off_diag)[0]), dtype=np.float32)\n",
    "        self.rel_rec = torch.FloatTensor(self.rel_rec).to(device)\n",
    "        self.rel_send = torch.FloatTensor(self.rel_send).to(device)\n",
    "\n",
    "        self.encoder= self.encoder.cuda()\n",
    "        self.decoder = self.decoder.cuda()\n",
    "        self.rel_rec = self.rel_rec.cuda()\n",
    "        self.rel_send = self.rel_send.cuda()\n",
    "    \n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \n",
    "                                            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "\n",
    "        confidence = output[0]['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0,0:2])) + [confidence[idx]] + [labels[idx]])\n",
    "        \n",
    "        # Sort keypoints based on label\n",
    "        keypoints.sort(key=lambda x: x[-1])\n",
    "        return keypoints\n",
    "    \n",
    "    def keypoints_to_graph(self, keypoints, image_width, image_height):\n",
    "        # keypoints is expected to be a tensor with shape (num_keypoints, 4),\n",
    "        # where each keypoint is (x, y, score, label).\n",
    "        # Convert all elements in keypoints to tensors if they are not already\n",
    "        keypoints = [torch.tensor(kp, dtype=torch.float32).to(device) if not isinstance(kp, torch.Tensor) else kp for kp in keypoints]\n",
    "\n",
    "        # Then stack them\n",
    "        keypoints = torch.stack(keypoints).to(device)        \n",
    "        \n",
    "        # Remove duplicates: Only keep the keypoint with the highest score for each label\n",
    "        unique_labels, best_keypoint_indices = torch.unique(keypoints[:, 3], return_inverse=True)\n",
    "        best_scores, best_indices = torch.max(keypoints[:, 2].unsqueeze(0) * (best_keypoint_indices == torch.arange(len(unique_labels)).unsqueeze(1).cuda()), dim=1)\n",
    "        keypoints = keypoints[best_indices]\n",
    "        \n",
    "#         print(\"init keypoints in graph features\", keypoints)\n",
    "\n",
    "        # Normalize x and y to be in the range [-1, 1]\n",
    "        keypoints[:, 0] = (keypoints[:, 0] - image_width / 2) / (image_width / 2)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] - image_height / 2) / (image_height / 2)\n",
    "\n",
    "        # Use only x, y, and score for the graph features\n",
    "        graph_features = keypoints[:, :4]  # Now shape is (num_keypoints, 3)\n",
    "        \n",
    "        # Ensure the shape is [num_keypoints, 3] before returning\n",
    "        graph_features = graph_features.view(-1, 4)  # Reshape to ensure it's [num_keypoints, 3]\n",
    "#         print(\"graph features\", graph_features)\n",
    "        print(\"graph features shape\", graph_features.shape)\n",
    "\n",
    "        return graph_features\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        # Temporarily set the keypoint model to evaluation mode\n",
    "        keypoint_model_training = self.keypoint_model.training\n",
    "        self.keypoint_model.eval()\n",
    "\n",
    "        # Process each image in the batch\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = [self.keypoint_model(img.unsqueeze(0).to(device)) for img in imgs]\n",
    "\n",
    "        # Set the keypoint model back to its original training mode\n",
    "        self.keypoint_model.train(mode=keypoint_model_training)\n",
    "\n",
    "        # Process model outputs to get labeled keypoints\n",
    "        batch_labeled_keypoints = [self.process_model_output(output) for output in batch_outputs]\n",
    "        # Generate graph input tensor for each image and handle varying number of keypoints\n",
    "        batch_x = []\n",
    "        for labeled_keypoints in batch_labeled_keypoints:\n",
    "            keypoints = self.keypoints_to_graph(labeled_keypoints, 640, 480)\n",
    "\n",
    "            # Initialize x with zeros for 6 nodes with 4 features each\n",
    "            x = torch.zeros(1, 6, 4, device=device)\n",
    "\n",
    "            # Ensure that keypoints are on the correct device and fill in x\n",
    "            num_keypoints_detected = keypoints.size(0)\n",
    "            if num_keypoints_detected <= 6:\n",
    "                x[0, :num_keypoints_detected, :] = keypoints\n",
    "            else:\n",
    "                raise ValueError(\"Number of keypoints detected exceeds the maximum of 6.\")\n",
    "\n",
    "            batch_x.append(x)\n",
    "\n",
    "        # Stack the batch of x tensors for batch processing\n",
    "        batch_x = torch.cat(batch_x, dim=0)\n",
    "\n",
    "        # Forward pass through the encoder and decoder\n",
    "        logits = self.encoder(batch_x, self.rel_rec, self.rel_send)\n",
    "        edges = my_softmax(logits, -1)\n",
    "        KGNN2D = self.decoder(batch_x, edges, self.rel_rec, self.rel_send)\n",
    "\n",
    "        return logits, KGNN2D, batch_labeled_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90ea42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_edges(valid_points, edges):\n",
    "#     num_nodes = 6\n",
    "#     off_diag = np.zeros([num_nodes, num_nodes])\n",
    "    \n",
    "#     # Creating a bidirectional cycle\n",
    "#     for i in range(num_nodes):\n",
    "#         # Forward connection: i -> (i + 1) % num_nodes\n",
    "#         off_diag[i, (i + 1) % num_nodes] = 1\n",
    "\n",
    "#         # Backward connection: i -> (i - 1 + num_nodes) % num_nodes\n",
    "#         # The addition of num_nodes before modulo ensures a positive index\n",
    "#         off_diag[i, (i - 1 + num_nodes) % num_nodes] = 1\n",
    "\n",
    "#     idx = torch.LongTensor(np.where(off_diag)[1]).cuda()\n",
    "#     if valid_points.ndim == 1:\n",
    "#         valid_points = valid_points.unsqueeze(0)  # Reshape to 2D if necessary\n",
    "\n",
    "#     # Initialize relations tensor for the cyclical edges\n",
    "#     relations = torch.zeros(valid_points.shape[0], num_nodes*num_nodes).cuda()\n",
    "#     for count, vis in enumerate(valid_points):\n",
    "#         vis = vis.view(-1, 1).float()\n",
    "# #         vis = vis * vis.t()\n",
    "#         vis_matrix = vis @ vis.t()\n",
    "#         # Extract the relations corresponding to the cyclical edges\n",
    "# #         relations[count] = vis[idx, (idx + 1) % num_nodes]\n",
    "#         relations[count] = vis_matrix.view(-1) * torch.from_numpy(off_diag).view(-1).float().cuda()\n",
    "\n",
    "#     relations = relations.type(torch.LongTensor).cuda()\n",
    "#     # Reshape relations to match the shape of edges\n",
    "#     # Assuming each row in edges corresponds to an edge in off_diag\n",
    "#     relations_reshaped = relations.view(-1, num_nodes**2)[:, :num_nodes]\n",
    "# #     relations_expanded = relations.repeat_interleave(2)\n",
    "#     loss_edges = func.cross_entropy(edges.view(-1, 2), relations_reshaped.view(-1))\n",
    "#     return loss_edges\n",
    "    \n",
    "# def kgnn2d_loss(gt_keypoints, pred_keypoints, labels):\n",
    "#     # Define a weight for keypoints with label 0 (higher weight)\n",
    "#     high_weight = 2.0  # Adjust this weight as needed\n",
    "#     # Default weight for other keypoints\n",
    "#     default_weight = 1.0\n",
    "\n",
    "#     # Create a weight tensor based on the labels\n",
    "#     weights = torch.full_like(labels, default_weight)\n",
    "#     weights[labels == 0] = high_weight\n",
    "#     print(\"Weights\", weights)    \n",
    "\n",
    "#     # Calculate weighted MSE loss\n",
    "#     loss = func.mse_loss(pred_keypoints, gt_keypoints, reduction='none')\n",
    "#     weighted_loss = loss * weights.unsqueeze(-1)  # Apply weights\n",
    "#     return torch.mean(weighted_loss)\n",
    "\n",
    "# def kgnn2d_loss(gt_keypoints, pred_keypoints, visibility):\n",
    "#     # Assuming visibility is 0 for occluded and 1 for visible keypoints\n",
    "#     visibility=visibility.unsqueeze(1)\n",
    "#     weights = torch.ones_like(visibility)\n",
    "#     weights[visibility == 0] = 2  # Increase weight for occluded keypoints\n",
    "# #     weights.unsqueeze(-1)\n",
    "#     print(\"Weights\", weights)\n",
    "#     loss = func.smooth_l1_loss(pred_keypoints * weights, gt_keypoints * weights)\n",
    "#     return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62626123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_edges(valid_points, edges):\n",
    "#     num_nodes = 6\n",
    "#     off_diag = np.zeros([num_nodes, num_nodes])\n",
    "    \n",
    "#     # Creating a bidirectional cycle\n",
    "#     for i in range(num_nodes):\n",
    "#         off_diag[i, (i + 1) % num_nodes] = 1\n",
    "#         off_diag[i, (i - 1 + num_nodes) % num_nodes] = 1\n",
    "\n",
    "#     if valid_points.ndim == 1:\n",
    "#         valid_points = valid_points.unsqueeze(0)  # Reshape to 2D if necessary\n",
    "\n",
    "#     # Initialize relations tensor for the cyclical edges\n",
    "#     relations = torch.zeros(valid_points.shape[0], num_nodes, num_nodes).cuda()\n",
    "#     for count, vis in enumerate(valid_points):\n",
    "#         vis = vis.view(-1, 1).float()  # Convert to float for matrix multiplication\n",
    "#         vis_matrix = vis @ vis.t()     # Compute visibility matrix\n",
    "#         relations[count] = vis_matrix * torch.from_numpy(off_diag).float().cuda()\n",
    "\n",
    "#     relations = relations.type(torch.LongTensor).cuda()\n",
    "\n",
    "#     # Reshape relations to match the shape of edges\n",
    "#     # Flatten the relations tensor to a 1D tensor\n",
    "#     relations_flat = relations.view(-1)[:edges.numel()]\n",
    "\n",
    "#     # Calculate the loss\n",
    "#     loss_edges = func.cross_entropy(edges.view(-1, 2), relations_flat)\n",
    "#     return loss_edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb9ff304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_edges(valid_points, edges):\n",
    "    num_nodes = 6\n",
    "    batch_size = valid_points.shape[0]\n",
    "    off_diag = np.zeros([num_nodes, num_nodes])\n",
    "\n",
    "    # Creating a bidirectional, non-cyclic graph\n",
    "    for i in range(num_nodes):\n",
    "        if i < num_nodes - 1:\n",
    "            off_diag[i, i + 1] = 1\n",
    "        if i > 0:\n",
    "            off_diag[i, i - 1] = 1\n",
    "\n",
    "    # Convert off_diag to a tensor and get the number of relations\n",
    "    off_diag = torch.tensor(off_diag, dtype=torch.bool).cuda()\n",
    "    num_relations = off_diag.sum().item()\n",
    "\n",
    "    # Initialize relations tensor\n",
    "    relations = torch.zeros(batch_size, num_relations).cuda()\n",
    "\n",
    "    for count, vis in enumerate(valid_points):\n",
    "        vis = vis.view(-1, 1)\n",
    "        vis_matrix = vis * vis.t()\n",
    "        selected_relations = vis_matrix[off_diag].view(-1)\n",
    "\n",
    "        # Assigning selected_relations to relations\n",
    "        relations[count, :num_relations] = selected_relations\n",
    "\n",
    "    relations = relations.view(-1).type(torch.long).cuda()\n",
    "\n",
    "    # Calculate the correct number of repetitions for edges\n",
    "    num_repetitions = num_relations // (edges.size(2) // 2)\n",
    "    edges_reshaped = edges.repeat_interleave(num_repetitions, dim=2).view(batch_size, -1, 2).view(-1, 2)\n",
    "\n",
    "    loss_edges = func.cross_entropy(edges_reshaped, relations)\n",
    "    return loss_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9ff3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_gaussian(preds, target, variance, add_const=False):\n",
    "    neg_log_p = ((preds - target) ** 2 / (2 * variance))\n",
    "    if add_const:\n",
    "        const = 0.5 * np.log(2 * np.pi * variance)\n",
    "        neg_log_p += const\n",
    "    return neg_log_p.sum() / (target.size(0) * target.size(1))\n",
    "\n",
    "def kgnn2d_loss(keypoints_gt, valid_points, keypoints_logits):\n",
    "    # Ensure data types are consistent and move tensors to the appropriate device\n",
    "    keypoints_gt = keypoints_gt.type(torch.FloatTensor).cuda()\n",
    "    keypoints_logits = keypoints_logits.type(torch.FloatTensor).cuda()\n",
    "    valid_points = valid_points.type(torch.FloatTensor).cuda()\n",
    "\n",
    "    # Print shapes for debugging\n",
    "    print(f\"keypoints_gt.shape: {keypoints_gt.shape}\")\n",
    "    print(f\"keypoints_logits.shape: {keypoints_logits.shape}\")\n",
    "    print(f\"valid_points.shape: {valid_points.shape}\")\n",
    "    keypoints_gt = keypoints_gt.type(torch.FloatTensor)*valid_points.unsqueeze(2).type(torch.FloatTensor)\n",
    "    keypoints_logits = keypoints_logits.type(torch.FloatTensor)*valid_points.unsqueeze(2).type(torch.FloatTensor)\n",
    "    keypoints_gt = keypoints_gt.cuda()\n",
    "    keypoints_logits = keypoints_logits.cuda()\n",
    "    loss_occ = nll_gaussian(keypoints_gt[:,:,0:2], keypoints_logits[:,:,0:2] , 0.1)\n",
    "    return loss_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "411072cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "# def process_keypoints(keypoints):\n",
    "#     # Assuming keypoints is a list of Nx3 tensors where N is the number of keypoints\n",
    "#     # and each keypoint is represented as [x, y, visibility]\n",
    "#     # Remove the unnecessary middle dimension\n",
    "#     keypoints = [kp.squeeze(1) for kp in keypoints]\n",
    "#     visibilities = [kp[:, 2] for kp in keypoints]  # Extract visibility flags\n",
    "#     valid_vis_all = torch.cat([v == 1 for v in visibilities]).long().cuda()\n",
    "#     valid_invis_all = torch.cat([v == 0 for v in visibilities]).long().cuda()\n",
    "\n",
    "#     keypoints_gt = torch.cat([kp[:, :2] for kp in keypoints]).float().cuda()  # Gather all keypoints and discard visibility flags\n",
    "#     keypoints_gt = keypoints_gt.view(-1, 2).unsqueeze(0)  # Add an extra dimension to match expected shape for loss_edges\n",
    "\n",
    "#     return keypoints_gt, valid_vis_all, valid_invis_all\n",
    "\n",
    "# def process_batch_keypoints(batch_keypoints):\n",
    "#     # Assuming batch_keypoints is a batch of keypoints tensors\n",
    "#     # Each tensor in the batch has shape [N, 3] where N is the number of keypoints\n",
    "#     # and each keypoint is represented as [x, y, visibility]\n",
    "\n",
    "#     # Concatenate all keypoints and visibilities from the batch\n",
    "#     all_keypoints = torch.cat([kp for kp in batch_keypoints])\n",
    "#     visibilities = all_keypoints[:, 2]  # Extract visibility flags\n",
    "\n",
    "#     valid_vis_all = (visibilities == 1).long().cuda()\n",
    "#     valid_invis_all = (visibilities == 0).long().cuda()\n",
    "\n",
    "#     keypoints_gt = all_keypoints[:, :2].float().cuda()  # Discard visibility flags\n",
    "#     keypoints_gt = keypoints_gt.view(-1, 2)  # Reshape for consistency\n",
    "\n",
    "#     return keypoints_gt, valid_vis_all, valid_invis_all\n",
    "\n",
    "def process_batch_keypoints(target_dicts):\n",
    "    # This function now expects target_dicts, a list of dictionaries containing keypoints information\n",
    "    batch_size = len(target_dicts)\n",
    "    print(batch_size)\n",
    "\n",
    "    # Initialize lists to store keypoints and visibilities for each image in the batch\n",
    "    keypoints_list = []\n",
    "    visibilities_list = []\n",
    "\n",
    "    for dict_ in target_dicts:\n",
    "        print(dict_)\n",
    "        # Each keypoints tensor in the dict is expected to have a shape [num_keypoints, 3]\n",
    "        keypoints = dict_['keypoints'].squeeze(1).to(device)\n",
    "        print(f\"Original shape of keypoints in dict: {keypoints.shape}\")\n",
    "\n",
    "        # Extract x, y coordinates and visibility flags\n",
    "        xy_coords = keypoints[:, :2]  # Keep only x, y coordinates\n",
    "        visibilities = keypoints[:, 2]  # Extract visibility flags\n",
    "\n",
    "        keypoints_list.append(xy_coords)\n",
    "        visibilities_list.append(visibilities)\n",
    "\n",
    "    # Concatenate keypoints and visibilities for the entire batch\n",
    "    # The final shape of keypoints_gt should be [batch_size, num_keypoints, 2]\n",
    "    keypoints_gt = torch.stack(keypoints_list).float().cuda()\n",
    "    visibilities = torch.stack(visibilities_list).cuda()\n",
    "\n",
    "    # Create valid visibility masks\n",
    "    valid_vis_all = (visibilities == 1).long().cuda()\n",
    "    valid_invis_all = (visibilities == 0).long().cuda()\n",
    "\n",
    "    print(f\"Shape of keypoints_gt after processing: {keypoints_gt.shape}\")\n",
    "    return keypoints_gt, valid_vis_all, valid_invis_all\n",
    "\n",
    "def reorder_batch_keypoints(batch_keypoints):\n",
    "    # Assuming batch_keypoints is a tensor of shape [batch_size, num_keypoints, num_features]\n",
    "    batch_size, num_keypoints, num_features = batch_keypoints.shape\n",
    "    reordered_keypoints_batch = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Directly use the normalized keypoints\n",
    "        normalized_keypoints = batch_keypoints[i]\n",
    "\n",
    "        # Initialize a tensor for reordered keypoints with only x, y coordinates\n",
    "        reordered_normalized_keypoints = torch.zeros(num_keypoints, 2, device=batch_keypoints.device)\n",
    "\n",
    "        # Reordering logic\n",
    "        rounded_labels = torch.round(normalized_keypoints[:, -1]).int()\n",
    "        used_indices = []\n",
    "        for label in range(1, 7):\n",
    "            valid_idx = (rounded_labels == label).nonzero(as_tuple=True)[0]\n",
    "            if valid_idx.numel() > 0:\n",
    "                reordered_normalized_keypoints[label - 1] = normalized_keypoints[valid_idx[0], :2]\n",
    "            else:\n",
    "                invalid_idx = ((rounded_labels < 1) | (rounded_labels > 6)).nonzero(as_tuple=True)[0]\n",
    "                invalid_idx = [idx for idx in invalid_idx if idx not in used_indices]\n",
    "                if invalid_idx:\n",
    "                    reordered_normalized_keypoints[label - 1] = normalized_keypoints[invalid_idx[0], :2]\n",
    "                    used_indices.append(invalid_idx[0])\n",
    "\n",
    "        reordered_keypoints_batch.append(reordered_normalized_keypoints)\n",
    "\n",
    "    return torch.stack(reordered_keypoints_batch)\n",
    "\n",
    "def denormalize_keypoints(batch_keypoints, width=640, height=480):\n",
    "    # Assuming batch_keypoints is a batch of normalized keypoints tensors\n",
    "    # Denormalize each keypoint in the batch\n",
    "    print(batch_keypoints.shape)\n",
    "    denormalized_keypoints = []\n",
    "    for kp in batch_keypoints:\n",
    "        denormalized_x = (kp[:, 0] * (width / 2)) + (width / 2)\n",
    "        denormalized_y = (kp[:, 1] * (height / 2)) + (height / 2)\n",
    "        denormalized_kp = torch.stack((denormalized_x, denormalized_y), dim=1)\n",
    "        denormalized_keypoints.append(denormalized_kp)\n",
    "        \n",
    "    denormalized_keypoints = torch.stack(denormalized_keypoints)\n",
    "    print(\"denormalized_keypoints.shape\", denormalized_keypoints.shape)\n",
    "    return denormalized_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4c0cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using factor graph MLP encoder.\n",
      "Using learned graph decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 13310 files [00:00, 19520.21 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph features shape torch.Size([3, 4])\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([2, 6, 4])\n",
      "2\n",
      "{'boxes': tensor([[247.9522, 356.9199, 267.9522, 376.9199],\n",
      "        [247.9597, 273.0131, 267.9597, 293.0131],\n",
      "        [222.3994, 197.5902, 242.3994, 217.5902],\n",
      "        [242.0816, 190.9170, 262.0816, 210.9170],\n",
      "        [284.9065, 101.5169, 304.9065, 121.5169],\n",
      "        [289.8304,  79.1918, 309.8304,  99.1918]]), 'labels': tensor([1, 2, 3, 4, 5, 6]), 'image_id': tensor([6294]), 'area': tensor([400.0003, 400.0003, 400.0000, 400.0003, 400.0000, 400.0000]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0]), 'keypoints': tensor([[[257.9522, 366.9199,   0.0000]],\n",
      "\n",
      "        [[257.9597, 283.0131,   1.0000]],\n",
      "\n",
      "        [[232.3994, 207.5902,   1.0000]],\n",
      "\n",
      "        [[252.0816, 200.9170,   1.0000]],\n",
      "\n",
      "        [[294.9065, 111.5169,   1.0000]],\n",
      "\n",
      "        [[299.8304,  89.1918,   0.0000]]])}\n",
      "Original shape of keypoints in dict: torch.Size([6, 3])\n",
      "{'boxes': tensor([[247.9522, 356.9199, 267.9522, 376.9199],\n",
      "        [247.9597, 273.0131, 267.9597, 293.0131],\n",
      "        [272.0515, 197.1493, 292.0515, 217.1493],\n",
      "        [291.8434, 203.4183, 311.8434, 223.4183],\n",
      "        [390.8515, 206.6182, 410.8515, 226.6182],\n",
      "        [402.6869, 188.2229, 422.6869, 208.2229]]), 'labels': tensor([1, 2, 3, 4, 5, 6]), 'image_id': tensor([970]), 'area': tensor([400.0003, 400.0003, 400.0000, 400.0000, 400.0000, 400.0000]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0]), 'keypoints': tensor([[[257.9522, 366.9199,   1.0000]],\n",
      "\n",
      "        [[257.9597, 283.0131,   1.0000]],\n",
      "\n",
      "        [[282.0515, 207.1493,   1.0000]],\n",
      "\n",
      "        [[301.8434, 213.4183,   1.0000]],\n",
      "\n",
      "        [[400.8515, 216.6182,   1.0000]],\n",
      "\n",
      "        [[412.6869, 198.2229,   1.0000]]])}\n",
      "Original shape of keypoints in dict: torch.Size([6, 3])\n",
      "Shape of keypoints_gt after processing: torch.Size([2, 6, 2])\n",
      "keypoints_gt.shape in training torch.Size([2, 6, 2])\n",
      "valid_invis_all.shape torch.Size([2, 6])\n",
      "torch.Size([2, 6, 2])\n",
      "denormalized_keypoints.shape torch.Size([2, 6, 2])\n",
      "keypoints_logits.shape torch.Size([2, 6, 2])\n",
      "torch.Size([2, 6])\n",
      "torch.Size([2, 10, 4])\n",
      "keypoints_gt.shape: torch.Size([2, 6, 2])\n",
      "keypoints_logits.shape: torch.Size([2, 6, 2])\n",
      "valid_points.shape: torch.Size([2, 6])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (200) to match target batch_size (20).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         loss_kgnn2d \u001b[38;5;241m=\u001b[39m kgnn2d_loss(keypoints_gt, valid_invis_all, denormalized_keypoints)\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# Compute batch losses\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m         edge_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_vis_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#         loss_kgnn2d = kgnn2d_loss(keypoints_gt, denormalized_keypoints, valid_vis_all)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# Combine the losses\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         total_batch_loss \u001b[38;5;241m=\u001b[39m edge_loss \u001b[38;5;241m+\u001b[39m loss_kgnn2d\n",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36mloss_edges\u001b[0;34m(valid_points, edges)\u001b[0m\n\u001b[1;32m     31\u001b[0m num_repetitions \u001b[38;5;241m=\u001b[39m num_relations \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (edges\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     32\u001b[0m edges_reshaped \u001b[38;5;241m=\u001b[39m edges\u001b[38;5;241m.\u001b[39mrepeat_interleave(num_repetitions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m loss_edges \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43medges_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_edges\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (200) to match target batch_size (20)."
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = KeypointPipeline(weights_path)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 2 # Define your number of epochs\n",
    "batch_size = 2\n",
    "\n",
    "split_folder_path = train_test_split(root_dir)\n",
    "KEYPOINTS_FOLDER_TRAIN = split_folder_path +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = split_folder_path +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = split_folder_path +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "v = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, target_dicts, _ in data_loader_train:\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass for batch\n",
    "        logits, KGNN2D, batch_labeled_keypoints = model(imgs)\n",
    "        print(KGNN2D.shape)\n",
    "\n",
    "        # Process keypoints for the entire batch\n",
    "        keypoints_gt, valid_vis_all, valid_invis_all = process_batch_keypoints(target_dicts)\n",
    "        print(\"keypoints_gt.shape in training\", keypoints_gt.shape)\n",
    "        print(\"valid_invis_all.shape\", valid_invis_all.shape)\n",
    "        \n",
    "        # Normalize and reorder keypoints as per your existing logic\n",
    "        # Ensure this logic works on the batch level\n",
    "        \n",
    "        reordered_normalized_keypoints = reorder_batch_keypoints(KGNN2D)\n",
    "#         print(\"keypoints_logits.shape\", reordered_normalized_keypoints.shape)\n",
    "        # Denormalize the reordered keypoints for the entire batch\n",
    "        denormalized_keypoints = denormalize_keypoints(reordered_normalized_keypoints)\n",
    "        print(\"keypoints_logits.shape\", denormalized_keypoints.shape)\n",
    "        \n",
    "        print(valid_vis_all.shape)\n",
    "        print(logits.shape)\n",
    "        \n",
    "        loss_kgnn2d = kgnn2d_loss(keypoints_gt, valid_invis_all, denormalized_keypoints)\n",
    "\n",
    "        # Compute batch losses\n",
    "        edge_loss = loss_edges(valid_vis_all, logits)\n",
    "        \n",
    "#         loss_kgnn2d = kgnn2d_loss(keypoints_gt, denormalized_keypoints, valid_vis_all)\n",
    "\n",
    "        # Combine the losses\n",
    "        total_batch_loss = edge_loss + loss_kgnn2d\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += total_batch_loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64334397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "945ce7b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp4 MLP(\n",
      "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Using MLP graph encoder.\n",
      "Using learned graph decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 13310 files [00:00, 16725.11 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "graph features shape torch.Size([3, 4])\n",
      "graph features shape torch.Size([6, 4])\n",
      "gt keypoints [tensor([[[257.9522, 366.9199,   1.0000]],\n",
      "\n",
      "        [[257.9597, 283.0131,   1.0000]],\n",
      "\n",
      "        [[282.3140, 207.2354,   1.0000]],\n",
      "\n",
      "        [[302.0853, 213.5749,   0.0000]],\n",
      "\n",
      "        [[376.4050, 148.3055,   1.0000]],\n",
      "\n",
      "        [[381.0244, 126.1484,   0.0000]]])]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'process_keypoints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m [target_dicts[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt keypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m, keypoints)\n\u001b[0;32m---> 43\u001b[0m keypoints_gt, valid_vis_all, valid_invis_all \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_keypoints\u001b[49m([target_dicts[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)])\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_keypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_labeled_keypoints[i])\n\u001b[1;32m     45\u001b[0m normalized_keypoints \u001b[38;5;241m=\u001b[39m KGNN2D[i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_keypoints' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd043d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218df109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

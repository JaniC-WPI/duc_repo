{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bedf1768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16908615680\n",
      "754974720\n",
      "646005760\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4b355d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "# root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "root_dir = parent_path + \"occ_sim_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "237a5cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a2189cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "#         A.Resize(640, 480)  # Resize all images to be 640x480\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aaae8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(0.95, 0.025, 0.025), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b9395a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1df0e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch_geometric.nn as pyg\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "_EPS = 1e-10\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Two-layer fully-connected ELU net with batch norm.\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def batch_norm(self, inputs):\n",
    "        x = inputs.view(inputs.size(0) * inputs.size(1), -1)\n",
    "        x = self.bn(x)\n",
    "        return x.view(inputs.size(0), inputs.size(1), -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "#         print(\"Input shape before any operations: \", inputs.shape)\n",
    "\n",
    "        # Flatten the last two dimensions for the linear layer input\n",
    "#         x = inputs.view(inputs.size(0), -1)\n",
    "        x = func.elu(self.fc1(inputs))\n",
    "        x = func.dropout(x, self.dropout_prob, training=self.training)\n",
    "        x = func.elu(self.fc2(x))\n",
    "        \n",
    "        return self.batch_norm(x)\n",
    "\n",
    "        # Assuming you want to maintain the second dimension for some reason\n",
    "        # (like temporal sequence in a RNN), you would reshape the output\n",
    "        # back to the desired shape. If not, this step is unnecessary.\n",
    "        # output = x.view(inputs.size(0), inputs.size(1), -1)\n",
    "        # print(\"Output shape after forward pass: \", output.shape)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_out=4, do_prob=0., factor=True):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "\n",
    "        self.factor = factor\n",
    "\n",
    "        self.mlp1 = MLP(n_in, n_hid, n_hid, do_prob)\n",
    "        self.mlp2 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "        self.mlp3 = MLP(n_hid, n_hid, n_hid, do_prob)\n",
    "        if self.factor:\n",
    "            self.mlp4 = MLP(n_hid * 3, n_hid, n_hid, do_prob)\n",
    "            print(\"Using factor graph MLP encoder.\")\n",
    "        else:\n",
    "            self.mlp4 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "            print(\"mlp4\", self.mlp4)\n",
    "            print(\"Using MLP graph encoder.\")\n",
    "        self.fc_out = nn.Linear(n_hid, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def edge2node(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        incoming = torch.matmul(rel_rec.t(), x)\n",
    "        return incoming / incoming.size(1)\n",
    "\n",
    "    def node2edge(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        receivers = torch.matmul(rel_rec, x)\n",
    "        senders = torch.matmul(rel_send, x)\n",
    "        edges = torch.cat([receivers, senders], dim=2)\n",
    "        return edges\n",
    "\n",
    "    def forward(self, inputs, rel_rec, rel_send):\n",
    "        # Input shape: [num_sims, num_atoms, num_timesteps, num_dims]\n",
    "        x = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "#         print(\"x shape:\", x.shape)\n",
    "#         print(\"rel_rec shape:\", rel_rec.shape)\n",
    "#         print(\"rel_send shape:\", rel_send.shape)\n",
    "\n",
    "        # New shape: [num_sims, num_atoms, num_timesteps*num_dims]\n",
    "        x = self.mlp1(x)  # 2-layer ELU net per node\n",
    "\n",
    "        x = self.node2edge(x, rel_rec, rel_send)\n",
    "        x = self.mlp2(x)\n",
    "        x_skip = x    \n",
    "        \n",
    "        if self.factor:\n",
    "            x = self.edge2node(x, rel_rec, rel_send)\n",
    "            x = self.mlp3(x)\n",
    "            x = self.node2edge(x, rel_rec, rel_send)\n",
    "            x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "            x = self.mlp4(x)\n",
    "        else:\n",
    "            x = self.mlp3(x)\n",
    "            x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "            x = self.mlp4(x)\n",
    "\n",
    "        return self.fc_out(x)    \n",
    "    \n",
    "class GraphDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in_node, edge_types, msg_hid, msg_out, n_hid,\n",
    "                 do_prob=0., skip_first=False):\n",
    "        super(GraphDecoder, self).__init__()\n",
    "        self.msg_fc1 = nn.ModuleList(\n",
    "            [nn.Linear(2 * n_in_node, msg_hid) for _ in range(edge_types)])\n",
    "        self.msg_fc2 = nn.ModuleList(\n",
    "            [nn.Linear(msg_hid, msg_out) for _ in range(edge_types)])\n",
    "        self.msg_out_shape = msg_out\n",
    "        self.skip_first_edge_type = skip_first\n",
    "\n",
    "        self.out_fc1 = nn.Linear(n_in_node + msg_out, n_hid)\n",
    "        self.out_fc2 = nn.Linear(n_hid, n_hid)\n",
    "        self.out_fc3 = nn.Linear(n_hid, n_in_node)\n",
    "\n",
    "        print('Using learned graph decoder.')\n",
    "\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "    def single_step_forward(self, single_timestep_inputs, rel_rec, rel_send,\n",
    "                            single_timestep_rel_type):\n",
    "\n",
    "        # single_timestep_inputs has shape\n",
    "        # [batch_size, num_timesteps, num_atoms, num_dims]\n",
    "\n",
    "        # single_timestep_rel_type has shape:\n",
    "        # [batch_size, num_timesteps, num_atoms*(num_atoms-1), num_edge_types]\n",
    "\n",
    "        # Node2edge\n",
    "        receivers = torch.matmul(rel_rec, single_timestep_inputs)\n",
    "        senders = torch.matmul(rel_send, single_timestep_inputs)\n",
    "        pre_msg = torch.cat([receivers, senders], dim=-1)\n",
    "\n",
    "        all_msgs = Variable(torch.zeros(pre_msg.size(0), pre_msg.size(1),self.msg_out_shape))\n",
    "        if single_timestep_inputs.is_cuda:\n",
    "            all_msgs = all_msgs.cuda()\n",
    "\n",
    "        if self.skip_first_edge_type:\n",
    "            start_idx = 1\n",
    "        else:\n",
    "            start_idx = 0\n",
    "\n",
    "        # Run separate MLP for every edge type\n",
    "        # NOTE: To exlude one edge type, simply offset range by 1\n",
    "        for i in range(start_idx, len(self.msg_fc2)):\n",
    "            msg = func.relu(self.msg_fc1[i](pre_msg))\n",
    "            msg = func.dropout(msg, p=self.dropout_prob)\n",
    "            msg = func.relu(self.msg_fc2[i](msg))\n",
    "            msg = msg * single_timestep_rel_type[:, :, i:i + 1]\n",
    "            all_msgs += msg\n",
    "\n",
    "        # Aggregate all msgs to receiver\n",
    "        agg_msgs = all_msgs.transpose(-2, -1).matmul(rel_rec).transpose(-2, -1)\n",
    "        agg_msgs = agg_msgs.contiguous()\n",
    "\n",
    "        # Skip connection\n",
    "        aug_inputs = torch.cat([single_timestep_inputs, agg_msgs], dim=-1)\n",
    "\n",
    "        # Output MLP\n",
    "        pred = func.dropout(func.relu(self.out_fc1(aug_inputs)), p=self.dropout_prob)\n",
    "        pred = func.dropout(func.relu(self.out_fc2(pred)), p=self.dropout_prob)\n",
    "        pred = self.out_fc3(pred)\n",
    "#        print(pred.shape,single_timestep_inputs.shape)\n",
    "\n",
    "        # Predict position/velocity difference\n",
    "        return single_timestep_inputs + pred\n",
    "\n",
    "    def forward(self, inputs, rel_type, rel_rec, rel_send, pred_steps=1):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "\n",
    "\n",
    "        # Only take n-th timesteps as starting points (n: pred_steps)\n",
    "        last_pred = inputs[:, :, :]\n",
    "        #asa\n",
    "        curr_rel_type = rel_type[:, :, :]\n",
    "        preds=[]\n",
    "        #print(curr_rel_type.shape)\n",
    "        # NOTE: Assumes rel_type is constant (i.e. same across all time steps).\n",
    "\n",
    "        # Run n prediction steps\n",
    "        #for step in range(0, pred_steps):\n",
    "        last_pred = self.single_step_forward(last_pred, rel_rec, rel_send,\n",
    "                                                 curr_rel_type)\n",
    "        preds.append(last_pred)\n",
    "\n",
    "        sizes = [preds[0].size(0), preds[0].size(1),\n",
    "                 preds[0].size(2)]\n",
    "\n",
    "        output = Variable(torch.zeros(sizes))\n",
    "        if inputs.is_cuda:\n",
    "            output = output.cuda()\n",
    "\n",
    "        # Re-assemble correct timeline\n",
    "        for i in range(len(preds)):\n",
    "            output[:, :, :] = preds[i]\n",
    "\n",
    "        pred_all = output[:, :, :]\n",
    "\n",
    "        # NOTE: We potentially over-predicted (stored in future_pred). Unused.\n",
    "        # future_pred = output[:, (inputs.size(1) - 1):, :, :]\n",
    "\n",
    "        return pred_all#.transpose(1, 2).contiguous()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d61bf273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(input, axis=1):\n",
    "    trans_input = input.transpose(axis, 0).contiguous()\n",
    "    soft_max_1d = func.softmax(trans_input,dim=0)\n",
    "    return soft_max_1d.transpose(axis, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0a4b6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path):\n",
    "        super(KeypointPipeline, self).__init__()  \n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.encoder = GraphEncoder(4,512,4,0.5,False)\n",
    "        self.decoder = GraphDecoder(n_in_node=4,\n",
    "                                 edge_types=2,\n",
    "                                 msg_hid=512,\n",
    "                                 msg_out=512,\n",
    "                                 n_hid=512,\n",
    "                                 do_prob=0.5,\n",
    "                                 skip_first=False)\n",
    "        \n",
    "#         self.off_diag = np.ones([6,6]) - np.eye(6)\n",
    "\n",
    "#         self.rel_rec = np.array(encode_onehot(np.where(self.off_diag)[1]), dtype=np.float32)\n",
    "#         self.rel_send = np.array(encode_onehot(np.where(self.off_diag)[0]), dtype=np.float32)\n",
    "#         self.rel_rec = torch.FloatTensor(self.rel_rec)\n",
    "#         self.rel_send = torch.FloatTensor(self.rel_send)\n",
    "\n",
    "        num_nodes = 6\n",
    "        self.off_diag = np.zeros([num_nodes, num_nodes])        \n",
    "#         # Creating a cycle: 1->2, 2->3, ..., 6->1\n",
    "#         for i in range(num_nodes):\n",
    "#             self.off_diag[i, (i + 1) % num_nodes] = 1\n",
    "\n",
    "        # Creating a bidirectional cycle\n",
    "        for i in range(num_nodes):\n",
    "            # Forward connection: i -> (i + 1) % num_nodes\n",
    "            self.off_diag[i, (i + 1) % num_nodes] = 1\n",
    "\n",
    "            # Backward connection: i -> (i - 1 + num_nodes) % num_nodes\n",
    "            # The addition of num_nodes before modulo ensures a positive index\n",
    "            self.off_diag[i, (i - 1 + num_nodes) % num_nodes] = 1\n",
    "\n",
    "        # Update rel_rec and rel_send based on the new off_diag\n",
    "        self.rel_rec = np.array(encode_onehot(np.where(self.off_diag)[1]), dtype=np.float32)\n",
    "        self.rel_send = np.array(encode_onehot(np.where(self.off_diag)[0]), dtype=np.float32)\n",
    "        self.rel_rec = torch.FloatTensor(self.rel_rec).to(device)\n",
    "        self.rel_send = torch.FloatTensor(self.rel_send).to(device)\n",
    "\n",
    "        self.encoder= self.encoder.cuda()\n",
    "        self.decoder = self.decoder.cuda()\n",
    "        self.rel_rec = self.rel_rec.cuda()\n",
    "        self.rel_send = self.rel_send.cuda()\n",
    "    \n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \n",
    "                                            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "\n",
    "        confidence = output[0]['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0,0:2])) + [confidence[idx]] + [labels[idx]])\n",
    "        \n",
    "        # Sort keypoints based on label\n",
    "        keypoints.sort(key=lambda x: x[-1])\n",
    "        return keypoints\n",
    "    \n",
    "    def keypoints_to_graph(self, keypoints, image_width, image_height):\n",
    "        # keypoints is expected to be a tensor with shape (num_keypoints, 4),\n",
    "        # where each keypoint is (x, y, score, label).\n",
    "        # Convert all elements in keypoints to tensors if they are not already\n",
    "        keypoints = [torch.tensor(kp, dtype=torch.float32).to(device) if not isinstance(kp, torch.Tensor) else kp for kp in keypoints]\n",
    "\n",
    "        # Then stack them\n",
    "        keypoints = torch.stack(keypoints).to(device)        \n",
    "        \n",
    "        # Remove duplicates: Only keep the keypoint with the highest score for each label\n",
    "        unique_labels, best_keypoint_indices = torch.unique(keypoints[:, 3], return_inverse=True)\n",
    "        best_scores, best_indices = torch.max(keypoints[:, 2].unsqueeze(0) * (best_keypoint_indices == torch.arange(len(unique_labels)).unsqueeze(1).cuda()), dim=1)\n",
    "        keypoints = keypoints[best_indices]\n",
    "        \n",
    "#         print(\"init keypoints in graph features\", keypoints)\n",
    "\n",
    "        # Normalize x and y to be in the range [-1, 1]\n",
    "        keypoints[:, 0] = (keypoints[:, 0] - image_width / 2) / (image_width / 2)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] - image_height / 2) / (image_height / 2)\n",
    "\n",
    "        # Use only x, y, and score for the graph features\n",
    "        graph_features = keypoints[:, :4]  # Now shape is (num_keypoints, 3)\n",
    "        \n",
    "        # Ensure the shape is [num_keypoints, 3] before returning\n",
    "        graph_features = graph_features.view(-1, 4)  # Reshape to ensure it's [num_keypoints, 3]\n",
    "#         print(\"graph features\", graph_features)\n",
    "        print(\"graph features shape\", graph_features.shape)\n",
    "\n",
    "        return graph_features\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        # Temporarily set the keypoint model to evaluation mode\n",
    "        keypoint_model_training = self.keypoint_model.training\n",
    "        self.keypoint_model.eval()\n",
    "\n",
    "        # Process each image in the batch\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = [self.keypoint_model(img.unsqueeze(0).to(device)) for img in imgs]\n",
    "\n",
    "        # Set the keypoint model back to its original training mode\n",
    "        self.keypoint_model.train(mode=keypoint_model_training)\n",
    "\n",
    "        # Process model outputs to get labeled keypoints\n",
    "        batch_labeled_keypoints = [self.process_model_output(output) for output in batch_outputs]\n",
    "        # Generate graph input tensor for each image and handle varying number of keypoints\n",
    "        batch_x = []\n",
    "        for labeled_keypoints in batch_labeled_keypoints:\n",
    "            keypoints = self.keypoints_to_graph(labeled_keypoints, 640, 480)\n",
    "\n",
    "            # Initialize x with zeros for 6 nodes with 4 features each\n",
    "            x = torch.zeros(1, 6, 4, device=device)\n",
    "\n",
    "            # Ensure that keypoints are on the correct device and fill in x\n",
    "            num_keypoints_detected = keypoints.size(0)\n",
    "            if num_keypoints_detected <= 6:\n",
    "                x[0, :num_keypoints_detected, :] = keypoints\n",
    "            else:\n",
    "                raise ValueError(\"Number of keypoints detected exceeds the maximum of 6.\")\n",
    "\n",
    "            batch_x.append(x)\n",
    "\n",
    "        # Stack the batch of x tensors for batch processing\n",
    "        batch_x = torch.cat(batch_x, dim=0)\n",
    "\n",
    "        # Forward pass through the encoder and decoder\n",
    "        logits = self.encoder(batch_x, self.rel_rec, self.rel_send)\n",
    "        edges = my_softmax(logits, -1)\n",
    "        KGNN2D = self.decoder(batch_x, edges, self.rel_rec, self.rel_send)\n",
    "\n",
    "        return logits, KGNN2D, batch_labeled_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "90ea42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_edges(valid_points, edges):\n",
    "#     num_nodes = 6\n",
    "#     off_diag = np.zeros([num_nodes, num_nodes])\n",
    "    \n",
    "#     # Creating a bidirectional cycle\n",
    "#     for i in range(num_nodes):\n",
    "#         # Forward connection: i -> (i + 1) % num_nodes\n",
    "#         off_diag[i, (i + 1) % num_nodes] = 1\n",
    "\n",
    "#         # Backward connection: i -> (i - 1 + num_nodes) % num_nodes\n",
    "#         # The addition of num_nodes before modulo ensures a positive index\n",
    "#         off_diag[i, (i - 1 + num_nodes) % num_nodes] = 1\n",
    "\n",
    "#     idx = torch.LongTensor(np.where(off_diag)[1]).cuda()\n",
    "#     if valid_points.ndim == 1:\n",
    "#         valid_points = valid_points.unsqueeze(0)  # Reshape to 2D if necessary\n",
    "\n",
    "#     # Initialize relations tensor for the cyclical edges\n",
    "#     relations = torch.zeros(valid_points.shape[0], num_nodes*num_nodes).cuda()\n",
    "#     for count, vis in enumerate(valid_points):\n",
    "#         vis = vis.view(-1, 1).float()\n",
    "# #         vis = vis * vis.t()\n",
    "#         vis_matrix = vis @ vis.t()\n",
    "#         # Extract the relations corresponding to the cyclical edges\n",
    "# #         relations[count] = vis[idx, (idx + 1) % num_nodes]\n",
    "#         relations[count] = vis_matrix.view(-1) * torch.from_numpy(off_diag).view(-1).float().cuda()\n",
    "\n",
    "#     relations = relations.type(torch.LongTensor).cuda()\n",
    "#     # Reshape relations to match the shape of edges\n",
    "#     # Assuming each row in edges corresponds to an edge in off_diag\n",
    "#     relations_reshaped = relations.view(-1, num_nodes**2)[:, :num_nodes]\n",
    "# #     relations_expanded = relations.repeat_interleave(2)\n",
    "#     loss_edges = func.cross_entropy(edges.view(-1, 2), relations_reshaped.view(-1))\n",
    "#     return loss_edges\n",
    "    \n",
    "# def kgnn2d_loss(gt_keypoints, pred_keypoints, labels):\n",
    "#     # Define a weight for keypoints with label 0 (higher weight)\n",
    "#     high_weight = 2.0  # Adjust this weight as needed\n",
    "#     # Default weight for other keypoints\n",
    "#     default_weight = 1.0\n",
    "\n",
    "#     # Create a weight tensor based on the labels\n",
    "#     weights = torch.full_like(labels, default_weight)\n",
    "#     weights[labels == 0] = high_weight\n",
    "#     print(\"Weights\", weights)    \n",
    "\n",
    "#     # Calculate weighted MSE loss\n",
    "#     loss = func.mse_loss(pred_keypoints, gt_keypoints, reduction='none')\n",
    "#     weighted_loss = loss * weights.unsqueeze(-1)  # Apply weights\n",
    "#     return torch.mean(weighted_loss)\n",
    "\n",
    "def kgnn2d_loss(gt_keypoints, pred_keypoints, visibility):\n",
    "    # Assuming visibility is 0 for occluded and 1 for visible keypoints\n",
    "    visibility=visibility.unsqueeze(1)\n",
    "    weights = torch.ones_like(visibility)\n",
    "    weights[visibility == 0] = 2  # Increase weight for occluded keypoints\n",
    "#     weights.unsqueeze(-1)\n",
    "    print(\"Weights\", weights)\n",
    "    loss = func.smooth_l1_loss(pred_keypoints * weights, gt_keypoints * weights)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "62626123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_edges(valid_points, edges):\n",
    "#     num_nodes = 6\n",
    "#     off_diag = np.zeros([num_nodes, num_nodes])\n",
    "    \n",
    "#     # Creating a bidirectional cycle\n",
    "#     for i in range(num_nodes):\n",
    "#         off_diag[i, (i + 1) % num_nodes] = 1\n",
    "#         off_diag[i, (i - 1 + num_nodes) % num_nodes] = 1\n",
    "\n",
    "#     if valid_points.ndim == 1:\n",
    "#         valid_points = valid_points.unsqueeze(0)  # Reshape to 2D if necessary\n",
    "\n",
    "#     # Initialize relations tensor for the cyclical edges\n",
    "#     relations = torch.zeros(valid_points.shape[0], num_nodes, num_nodes).cuda()\n",
    "#     for count, vis in enumerate(valid_points):\n",
    "#         vis = vis.view(-1, 1).float()  # Convert to float for matrix multiplication\n",
    "#         vis_matrix = vis @ vis.t()     # Compute visibility matrix\n",
    "#         relations[count] = vis_matrix * torch.from_numpy(off_diag).float().cuda()\n",
    "\n",
    "#     relations = relations.type(torch.LongTensor).cuda()\n",
    "\n",
    "#     # Reshape relations to match the shape of edges\n",
    "#     # Flatten the relations tensor to a 1D tensor\n",
    "#     relations_flat = relations.view(-1)[:edges.numel()]\n",
    "\n",
    "#     # Calculate the loss\n",
    "#     loss_edges = func.cross_entropy(edges.view(-1, 2), relations_flat)\n",
    "#     return loss_edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bb9ff304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_edges(valid_points, edges):\n",
    "    num_nodes = 6\n",
    "    off_diag = np.zeros([num_nodes, num_nodes])\n",
    "    \n",
    "    # Creating a bidirectional cycle\n",
    "    for i in range(num_nodes):\n",
    "        off_diag[i, (i + 1) % num_nodes] = 1\n",
    "        off_diag[i, (i - 1 + num_nodes) % num_nodes] = 1\n",
    "\n",
    "    idx = torch.LongTensor(np.where(off_diag)).cuda()\n",
    "\n",
    "    # Ensure valid_points is of the correct shape\n",
    "    if valid_points.shape[0] != num_nodes:\n",
    "        raise ValueError(f\"Unexpected shape of valid_points: {valid_points.shape}. Expected length: {num_nodes}\")\n",
    "\n",
    "    vis = valid_points.view(-1, 1).float()\n",
    "    vis_matrix = vis @ vis.t()\n",
    "\n",
    "    # Gathering the elements corresponding to the edges\n",
    "    relations = vis_matrix[idx[0], idx[1]]\n",
    "    relations = relations.type(torch.LongTensor).cuda()\n",
    "    \n",
    "    # Flatten relations to match the total number of rows in edges\n",
    "    num_edge_pairs = edges.shape[0] * edges.shape[1] // 2\n",
    "    relations_flat = relations.view(-1)[:num_edge_pairs]\n",
    "\n",
    "    # Compute the cross-entropy loss\n",
    "    loss_edges = func.cross_entropy(edges.view(-1, 2), relations_flat)\n",
    "    return loss_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "411072cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "def process_keypoints(keypoints):\n",
    "    # Assuming keypoints is a list of Nx3 tensors where N is the number of keypoints\n",
    "    # and each keypoint is represented as [x, y, visibility]\n",
    "    # Remove the unnecessary middle dimension\n",
    "    keypoints = [kp.squeeze(1) for kp in keypoints]\n",
    "    visibilities = [kp[:, 2] for kp in keypoints]  # Extract visibility flags\n",
    "    valid_vis_all = torch.cat([v == 1 for v in visibilities]).long().cuda()\n",
    "    valid_invis_all = torch.cat([v == 0 for v in visibilities]).long().cuda()\n",
    "\n",
    "    keypoints_gt = torch.cat([kp[:, :2] for kp in keypoints]).float().cuda()  # Gather all keypoints and discard visibility flags\n",
    "    keypoints_gt = keypoints_gt.view(-1, 2).unsqueeze(0)  # Add an extra dimension to match expected shape for loss_edges\n",
    "\n",
    "    return keypoints_gt, valid_vis_all, valid_invis_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "945ce7b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp4 MLP(\n",
      "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Using MLP graph encoder.\n",
      "Using learned graph decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 13310 files [00:00, 19452.64 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "graph features shape torch.Size([5, 4])\n",
      "graph features shape torch.Size([3, 4])\n",
      "gt keypoints [tensor([[[257.9522, 366.9199,   1.0000]],\n",
      "\n",
      "        [[257.9597, 283.0131,   0.0000]],\n",
      "\n",
      "        [[183.5314, 254.3787,   1.0000]],\n",
      "\n",
      "        [[190.9853, 234.9648,   1.0000]],\n",
      "\n",
      "        [[124.5713, 160.9595,   1.0000]],\n",
      "\n",
      "        [[135.5736, 142.8366,   1.0000]]])]\n",
      "init_keypoints [[258, 367, 0.9999602, 1], [183, 254, 0.99970406, 3], [191, 235, 0.9998319, 4], [124, 161, 0.995751, 5], [136, 142, 0.9947752, 6]]\n",
      "rounded labels tensor([1, 3, 4, 5, 6, 0], device='cuda:0', dtype=torch.int32)\n",
      "valid vis all tensor([1, 0, 1, 1, 1, 1], device='cuda:0')\n",
      "labels tensor([[1],\n",
      "        [0],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6]], device='cuda:0', dtype=torch.int32)\n",
      "Predicted keypoints tensor([[252.9207, 366.9734],\n",
      "        [341.2433, 242.6013],\n",
      "        [219.6614, 240.6557],\n",
      "        [154.0688, 270.7302],\n",
      "        [ 96.1871, 131.6719],\n",
      "        [171.2694, 120.4384]], device='cuda:0', grad_fn=<StackBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (24) to match target batch_size (12).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [140]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted keypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m, denormalized_keypoints)\n\u001b[1;32m     79\u001b[0m             \u001b[38;5;66;03m# Compute loss for each image\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m             edge_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_vis_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m             loss_kgnn2d \u001b[38;5;241m=\u001b[39m kgnn2d_loss(keypoints_gt, denormalized_keypoints, valid_vis_all)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#             print(\"Edge loss\", edge_loss)\u001b[39;00m\n",
      "Input \u001b[0;32mIn [138]\u001b[0m, in \u001b[0;36mloss_edges\u001b[0;34m(valid_points, edges)\u001b[0m\n\u001b[1;32m     25\u001b[0m relations_flat \u001b[38;5;241m=\u001b[39m relations\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[:num_edge_pairs]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Compute the cross-entropy loss\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m loss_edges \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelations_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_edges\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (24) to match target batch_size (12)."
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = KeypointPipeline(weights_path)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 2 # Define your number of epochs\n",
    "batch_size = 2\n",
    "\n",
    "split_folder_path = train_test_split(root_dir)\n",
    "KEYPOINTS_FOLDER_TRAIN = split_folder_path +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = split_folder_path +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = split_folder_path +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "v = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader_train:\n",
    "        imgs, target_dicts, _ = batch\n",
    "        print(type(imgs))\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        logits, KGNN2D, batch_labeled_keypoints = model([img.to(device) for img in imgs])\n",
    "\n",
    "        # Prepare batch loss computation\n",
    "        batch_losses = []\n",
    "        for i in range(len(imgs)):\n",
    "            # Process each image in the batch\n",
    "            keypoints = [target_dicts[i]['keypoints']]\n",
    "            print(\"gt keypoints\", keypoints)\n",
    "            keypoints_gt, valid_vis_all, valid_invis_all = process_keypoints([target_dicts[i]['keypoints'].to(device)])\n",
    "            print(\"init_keypoints\", batch_labeled_keypoints[i])\n",
    "            normalized_keypoints = KGNN2D[i]\n",
    "            # Round labels to the nearest integers\n",
    "            rounded_labels = torch.round(normalized_keypoints[:, 3]).int()\n",
    "            print(\"rounded labels\", rounded_labels)\n",
    "            # Initialize tensors to hold the reordered keypoints\n",
    "            reordered_normalized_keypoints = torch.zeros_like(normalized_keypoints)\n",
    "            reordered_denormalized_keypoints = torch.zeros_like(normalized_keypoints)\n",
    "            # Track used indices for invalid labels\n",
    "            used_indices = []\n",
    "            # Iterate through the expected label range (1 to 6)\n",
    "            for label in range(1, 7):\n",
    "                # Find the index of the current label\n",
    "                valid_idx = (rounded_labels == label).nonzero(as_tuple=True)[0]\n",
    "                if valid_idx.numel() > 0:\n",
    "                    # Assign the keypoint to the correct position\n",
    "                    reordered_normalized_keypoints[label - 1, :] = normalized_keypoints[valid_idx[0], :]\n",
    "                else:\n",
    "                    # If the label is missing, find a keypoint with invalid label\n",
    "                    invalid_idx = ((rounded_labels < 1) | (rounded_labels > 6)).nonzero(as_tuple=True)[0]\n",
    "                    invalid_idx = [idx for idx in invalid_idx if idx not in used_indices]\n",
    "                    if invalid_idx:\n",
    "                        reordered_normalized_keypoints[label - 1, :] = normalized_keypoints[invalid_idx[0], :]\n",
    "                        used_indices.append(invalid_idx[0])\n",
    "            print(\"valid vis all\", valid_vis_all)\n",
    "            labels = torch.round(reordered_normalized_keypoints[:,3:4]).int()\n",
    "            print(\"labels\", labels)\n",
    "            # Denormalize the reordered keypoints\n",
    "            denormalized_x = (reordered_normalized_keypoints[:, 0] * (640 / 2)) + (640 / 2)\n",
    "            denormalized_y = (reordered_normalized_keypoints[:, 1] * (480 / 2)) + (480 / 2)\n",
    "            reordered_denormalized_keypoints[:, 0] = denormalized_x\n",
    "            reordered_denormalized_keypoints[:, 1] = denormalized_y\n",
    "            # Stack the denormalized x and y coordinates together to form [n, 2] tensor\n",
    "            denormalized_keypoints = torch.stack((denormalized_x, denormalized_y), dim=1)\n",
    "            print(\"Predicted keypoints\", denormalized_keypoints)\n",
    "            # Compute loss for each image\n",
    "            edge_loss = loss_edges(valid_vis_all, logits[i])\n",
    "            loss_kgnn2d = kgnn2d_loss(keypoints_gt, denormalized_keypoints, valid_vis_all)\n",
    "#             print(\"Edge loss\", edge_loss)\n",
    "            print(\"kgnn2d loss\", loss_kgnn2d)\n",
    "            loss = edge_loss + loss_kgnn2d\n",
    "            batch_losses.append(loss)\n",
    "#             batch_losses.append(loss_kgnn2d)\n",
    "            \n",
    "\n",
    "        # Average loss over the batch and backpropagation\n",
    "        batch_loss = torch.mean(torch.stack(batch_losses))\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader_train)}')\n",
    "\n",
    "# # Save the model after training\n",
    "# torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "model_save_path = f\"/home/jc-merlab/Pictures/Data/trained_models/krcnn_occ_b{batch_size}_e{num_epochs}_v{v}.pth\"\n",
    "\n",
    "torch.save(model, model_save_path)\n",
    "    \n",
    "# Save the state dict of the model, not the entire model\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd043d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218df109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b355d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "# root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "root_dir = parent_path + \"occ_sim_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2189cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "#         A.Resize(640, 480)  # Resize all images to be 640x480\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9395a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import networkx as nx\n",
    "import torch_geometric.nn as pyg\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return func.relu(self.fc(x))\n",
    "\n",
    "# Graph Encoder\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, node_feature_size):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.f_enc = GNNLayer(node_feature_size, 128)\n",
    "        self.f_e1 = GNNLayer(256, 128)\n",
    "        self.f_v = GNNLayer(128, 128)\n",
    "        self.f_e2 = GNNLayer(256, 128)\n",
    "\n",
    "    def forward(self, V):\n",
    "        # Implementing the encoder part\n",
    "        # Node to edge\n",
    "        H1 = self.f_enc(V)\n",
    "        H_edge1 = self.f_e1(torch.cat((H1, H1), dim=1))  # Example implementation\n",
    "        # Edge to node\n",
    "        H2 = self.f_v(H_edge1.sum(dim=0, keepdim=True))  # Summing for simplicity\n",
    "        # Node to edge again\n",
    "        H_edge2 = self.f_e2(torch.cat((H2, H2), dim=1))  # Example implementation\n",
    "\n",
    "        # Softmax over edges to get occlusion statistics\n",
    "        edge_predictions = func.softmax(H_edge2, dim=1)\n",
    "        return edge_predictions\n",
    "\n",
    "# Graph Decoder\n",
    "class GraphDecoder(nn.Module):\n",
    "    def __init__(self, node_feature_size):\n",
    "        super(GraphDecoder, self).__init__()\n",
    "        self.f_e_p = GNNLayer(node_feature_size * 2, 128)\n",
    "        self.f_v = GNNLayer(128, node_feature_size)\n",
    "\n",
    "    def forward(self, V, E):\n",
    "        # Implementing the decoder part\n",
    "        # Vertex to Edge\n",
    "        H_edge = self.f_e_p(torch.cat((V, V), dim=1)) * E\n",
    "        # Edge to Vertex\n",
    "        mu_g = V + self.f_v(H_edge.sum(dim=0, keepdim=True))\n",
    "\n",
    "        return mu_g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_loss(pred_edges, gt_edges):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss for edge predictions.\n",
    "\n",
    "    Args:\n",
    "    - pred_edges (Tensor): Predicted probabilities of edges being visible, \n",
    "                           shape [#edges, 2] where second column is visibility probability.\n",
    "    - gt_edges (Tensor): Ground truth for edges, binary values (0 or 1), \n",
    "                         shape [#edges].\n",
    "\n",
    "    Returns:\n",
    "    - loss (Tensor): Computed cross-entropy loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    gt_edges_tensor = torch.tensor(gt_edges, dtype=torch.float32).to(pred_edges.device)  # Convert gt_edges to tensor and move to the correct device\n",
    "\n",
    "    # Extract the probabilities corresponding to the edges being visible.\n",
    "    visible_prob = pred_edges[:, 1]\n",
    "    \n",
    "    # Compute the binary cross-entropy loss.\n",
    "    loss = -torch.sum(gt_edges_tensor * torch.log(visible_prob + 1e-10))  # Adding a small value to avoid log(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# def kp_loss(predictions, targets):\n",
    "#     return torch.mean((predictions - targets) ** 2)\n",
    "\n",
    "def compute_loss(pred_keypoints, gt_keypoints):\n",
    "    # Assuming pred_keypoints and gt_keypoints are tensors of shape [num_keypoints, 3]\n",
    "    # where the last dimension is (x, y, confidence)\n",
    "\n",
    "    # Create a distance matrix between all predicted and ground truth keypoints\n",
    "    distance_matrix = torch.cdist(pred_keypoints[:, :2], gt_keypoints[:, :2])\n",
    "\n",
    "    # Match predicted keypoints with ground truth keypoints\n",
    "    # .detach() is used to convert the tensor to a numpy array without requiring grad\n",
    "    pred_idx, gt_idx = linear_sum_assignment(distance_matrix.detach().cpu().numpy())\n",
    "\n",
    "    # Compute loss for matched keypoints\n",
    "    matched_loss = func.mse_loss(pred_keypoints[pred_idx, :2], gt_keypoints[gt_idx, :2])\n",
    "\n",
    "    return matched_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0841a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground truth edge creation\n",
    "def create_gt_edges(keypoints, edges_definition):\n",
    "    \"\"\"\n",
    "    Create ground truth edges based on keypoints visibility.\n",
    "\n",
    "    Args:\n",
    "    - keypoints: A NumPy array or PyTorch tensor of shape [#keypoints, 3] where each row represents a keypoint \n",
    "                 (x, y, visibility). Visibility is 1 if the keypoint is visible, else 0.\n",
    "    - edges_definition (list of tuples): Each tuple contains the indices of the keypoints that \n",
    "                                         form an edge, e.g., (0, 1) for an edge between the first \n",
    "                                         and second keypoints.\n",
    "\n",
    "    Returns:\n",
    "    - gt_edges (np.array): Array of shape [#edges] where each element is 1 if the edge is visible, else 0.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch tensor to NumPy array if necessary\n",
    "    if isinstance(keypoints, torch.Tensor):\n",
    "        # Move tensor to CPU if it's on CUDA\n",
    "        keypoints = keypoints.cpu().numpy()\n",
    "    \n",
    "    gt_edges = []\n",
    "    for start_idx, end_idx in edges_definition:\n",
    "        # An edge is visible if both its keypoints are visible.\n",
    "        edge_visible = keypoints[start_idx, 2] and keypoints[end_idx, 2]\n",
    "        gt_edges.append(edge_visible)\n",
    "\n",
    "    return np.array(gt_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path):\n",
    "        super().__init__()\n",
    "\n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.gnn_encoder = GraphEncoder(node_feature_size=4)\n",
    "        self.gnn_decoder = GraphDecoder(node_feature_size=4)\n",
    "\n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \n",
    "                                            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "\n",
    "        confidence = output[0]['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0,0:2])) + [confidence[idx]] + [labels[idx]])\n",
    "        \n",
    "        # Sort keypoints based on label\n",
    "        keypoints.sort(key=lambda x: x[-1])\n",
    "        print(\"initial keypoints\", keypoints)\n",
    "        keypoints_tensor = torch.tensor(keypoints, dtype=torch.float32).to(device)\n",
    "        self.enc_edges = self.gnn_encoder(keypoints_tensor)\n",
    "        print(\"edges encoded\", self.enc_edges)\n",
    "        vertices_pred = self.gnn_decoder(keypoints_tensor, self.enc_edges)\n",
    "        print(\"decoder keypoints\", vertices_pred)\n",
    "        return vertices_pred       \n",
    "    \n",
    "\n",
    "    def process_image(self, img):\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        # Temporarily set the keypoint model to evaluation mode\n",
    "        keypoint_model_training = self.keypoint_model.training  # Save the current mode\n",
    "        self.keypoint_model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.keypoint_model(img)\n",
    "        # Set the keypoint model back to its previous mode\n",
    "        self.keypoint_model.train(keypoint_model_training)\n",
    "        img = (img[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        labeled_keypoints = self.process_model_output(output)\n",
    "\n",
    "        return labeled_keypoints\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(imgs.shape[0]):\n",
    "            labeled_keypoints = self.process_image(imgs[i])\n",
    "            outputs.append(labeled_keypoints)\n",
    "\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ce7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = KeypointPipeline(weights_path)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 25  # Define your number of epochs\n",
    "batch_size = 8\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "edges_def = [(0,1),(0,2),(0,3),(0,4),(0,5),(1,2),(1,3),(1,4),(1,5),(2,3),(2,4),(2,5),(3,4),(3,5),(4,5)]\n",
    "\n",
    "v = 1\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for i, batch in enumerate(data_loader_train):\n",
    "        img_tuple, target_dict_tuple, img_files = batch\n",
    "        print(f\"Processing batch {i+1} with images:\", img_files)\n",
    "        \n",
    "        imgs = [img.to(device) for img in img_tuple]  # Create list of images\n",
    "\n",
    "        # Process each image individually\n",
    "        losses = []\n",
    "        for i in range(len(imgs)):\n",
    "            img = imgs[i].unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "\n",
    "            # Prepare ground truth vertices for the image\n",
    "            gt_keypoints = target_dict_tuple[i]['keypoints'].to(device).squeeze()\n",
    "            print(gt_keypoints.shape)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(img)\n",
    "            pred_keypoints = output[0]\n",
    "            \n",
    "            print(\"predicted keypoints\", pred_keypoints)\n",
    "            \n",
    "            edges_prob = model.enc_edges\n",
    "            \n",
    "            edges_gt = create_gt_edges(gt_keypoints,edges_def)\n",
    "\n",
    "            # Compute loss for the image\n",
    "            kp_loss = compute_loss(pred_keypoints, gt_keypoints)\n",
    "            ce_loss = edge_loss(edges_prob, edges_gt)\n",
    "\n",
    "            loss = kp_loss + ce_loss\n",
    "            losses.append(loss)  # Store loss for the image\n",
    "            \n",
    "        # Average loss over all images in the batch\n",
    "        total_loss = torch.mean(torch.stack(losses))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    eta = epoch_time * (num_epochs - epoch - 1)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, ETA: {eta} seconds')\n",
    "\n",
    "model_save_path = f\"/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_occ_b{batch_size}_e{num_epochs}_v{v}.pth\"\n",
    "\n",
    "torch.save(model, model_save_path)\n",
    "    \n",
    "# Save the state dict of the model, not the entire model\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "torch.save(model, model_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfef6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save(img, vertices, filename):\n",
    "    print(\"type of image befor conversion\",type(img))    \n",
    "    print(\"type of vertices before conversion\", type(vertices))\n",
    "    print(img)\n",
    "    img = (img.permute(1,2,0).cpu().numpy() * 255).astype(np.uint8)\n",
    "#     img = (img * 255).astype(np.uint8)  # Convert back from [0, 1] range to [0, 255]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    vertices = vertices.cpu().numpy()\n",
    "\n",
    "    print(f\"Image shape before saving: {img.shape}\")  # print the image shape\n",
    "    print(\"type of vertices\", type(vertices))\n",
    "#     print(\"entered vertices\", vertices)\n",
    "#     print(\"entered image\", img)\n",
    "\n",
    "    # Convert grayscale to BGR if necessary\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "    for i in range(vertices.shape[0]):\n",
    "        img = cv2.circle(img, (int(vertices[i, 0]), int(vertices[i, 1])), radius=2, color=(0, 0, 255), thickness=-1)\n",
    "        \n",
    "    result = cv2.imwrite(filename, img)\n",
    "    print(f\"Image saved at {filename}: {result}\")  # print if save was successful\n",
    "\n",
    "    # If the image didn't save correctly, save the image data to a text file for examination\n",
    "    if not result:\n",
    "        with open(filename + \".txt\", \"w\") as f:\n",
    "            np.savetxt(f, img.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_save_model(model, data_loader_test):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_trifocal_loss = 0.0\n",
    "    total_ce_loss = 0.0\n",
    "    total_vis_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    y_true_sequence = []\n",
    "    y_pred_sequence = []\n",
    "\n",
    "    # We don't need to track gradients during evaluation\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(data_loader_test):\n",
    "            img_tuple, target_dict_tuple, img_files = batch\n",
    "\n",
    "            total_batch_loss = 0.0\n",
    "            total_batch_trifocal_loss = 0.0\n",
    "            total_batch_ce_loss = 0.0\n",
    "            total_batch_vis_loss = 0.0\n",
    "\n",
    "            # Process each image individually\n",
    "            for i in range(len(img_tuple)):\n",
    "                img = img_tuple[i].to(device)\n",
    "                target = target_dict_tuple[i]\n",
    "\n",
    "                # Prepare ground truth vertices for the image\n",
    "                keypoints = target['keypoints'].to(device)\n",
    "                visibility = torch.ones((keypoints.shape[0], keypoints.shape[1], 1)).to(device)\n",
    "                vertices_gt = torch.cat((keypoints, visibility), dim=2).unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "                vertices_gt = vertices_gt.squeeze()\n",
    "                y_true_sequence.append(vertices_gt)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(img.unsqueeze(0))\n",
    "                vertices_pred = output[0]\n",
    "                y_pred_sequence.append(vertices_pred)\n",
    "\n",
    "                edges_prob = model.enc_e\n",
    "                edges = model.edges.T\n",
    "                edge_features = model.edge_features\n",
    "                edges_gt = torch.cat((edges, edge_features), dim=1) \n",
    "\n",
    "                trifocal_loss = criterion(vertices_pred, vertices_gt)\n",
    "                ce_loss = edge_loss(edges_prob, edges_gt)\n",
    "                vis_loss = visibility_loss(vertices_pred, vertices_gt)\n",
    "                loss = trifocal_loss + ce_loss + vis_loss\n",
    "\n",
    "                total_batch_loss += loss.item()\n",
    "                total_batch_trifocal_loss += trifocal_loss.item()\n",
    "                total_batch_ce_loss += ce_loss.item()\n",
    "                total_batch_vis_loss += vis_loss.item()\n",
    "\n",
    "                # Visualize and save the prediction\n",
    "                filename = f'/home/jc-merlab/Pictures/Data/occ_vis_data/image_{idx}_{i}.jpg'\n",
    "                visualize_and_save(img, vertices_pred, filename)\n",
    "                print(f\"Image saved at {filename}\")  # Print statement to confirm image save\n",
    "\n",
    "            # Convert true and predicted sequences to tensors\n",
    "            y_true_tensor = torch.stack(y_true_sequence)\n",
    "            y_pred_tensor = torch.stack(y_pred_sequence)\n",
    "\n",
    "            # Compute temporal consistency loss\n",
    "            temporal_loss = temporal_consistency_loss(y_true_tensor, y_pred_tensor)\n",
    "\n",
    "            total_loss += (total_batch_loss + temporal_loss) / len(img_tuple)\n",
    "            total_trifocal_loss += total_batch_trifocal_loss / len(img_tuple)\n",
    "            total_ce_loss += total_batch_ce_loss / len(img_tuple)\n",
    "            num_batches += 1\n",
    "\n",
    "            # Clear the sequences for the next batch\n",
    "            y_true_sequence.clear()\n",
    "            y_pred_sequence.clear()\n",
    "    \n",
    "    # Average the loss over all batches\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_trifocal_loss = total_trifocal_loss / num_batches\n",
    "    avg_ce_loss = total_ce_loss / num_batches\n",
    "    \n",
    "    print(f'Avg. Test Loss: {avg_loss}, Avg. Trifocal Loss: {avg_trifocal_loss}, Avg. Cross Entropy Loss: {avg_ce_loss}')\n",
    "    return avg_loss, avg_trifocal_loss, avg_ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_loss, avg_trifocal_loss, avg_ce_loss, all_preds = test_and_save_model(model, data_loader_test)\n",
    "\n",
    "avg_loss, avg_trifocal_loss, avg_ce_loss = test_and_save_model(model, data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58949932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Directory containing images\n",
    "dir_path = '/home/jc-merlab/Pictures/Data/occ_vis_data/'\n",
    "images = []\n",
    "\n",
    "# Ensure the images are sorted by name\n",
    "for f in sorted(os.listdir(dir_path)):\n",
    "    if f.endswith('.jpg') or f.endswith('.png'):  # Check for image file extension\n",
    "        images.append(f)\n",
    "\n",
    "# Determine the width and height from the first image\n",
    "image_path = os.path.join(dir_path, images[0])\n",
    "frame = cv2.imread(image_path)\n",
    "cv2.imshow('video',frame)\n",
    "height, width, channels = frame.shape\n",
    "\n",
    "# Define the codec and create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Be sure to use the correct codec\n",
    "video_filename = 'output.mp4'\n",
    "video = cv2.VideoWriter(video_filename, fourcc, 3.0, (width, height))\n",
    "\n",
    "for image in images:\n",
    "    image_path = os.path.join(dir_path, image)\n",
    "    frame = cv2.imread(image_path)\n",
    "    video.write(frame)  # Write out frame to video\n",
    "\n",
    "# Release everything when job is finished\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"The output video is\", video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0007f54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_occ_b16_e25_v1.pth'\n",
    "\n",
    "# model = torch.load(model_path).to(device)\n",
    "\n",
    "\n",
    "image = Image.open(\"/home/jc-merlab/Pictures/Data/occluded_results_mi20_ma80_n2/occluded_000041.rgb.jpg\")\n",
    "print(type(image))\n",
    "\n",
    "img = F.to_tensor(image).to(device)\n",
    "img.unsqueeze_(0)\n",
    "# print(image.shape)\n",
    "# image = list(image)\n",
    "# print(type(images))\n",
    "# images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(img)\n",
    "    \n",
    "keypoints = output[0]\n",
    "\n",
    "print(keypoints)\n",
    "plt.imshow(image)\n",
    "\n",
    "# Assuming each keypoint is a tensor representing (x, y)\n",
    "for i, keypoint in enumerate(keypoints):\n",
    "    print(f'Key point {i}: {keypoint}')\n",
    "    keypoint = keypoint.cpu().numpy()\n",
    "    plt.plot(keypoint[0], keypoint[1], 'ro')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the image\n",
    "\n",
    "# plt.imshow(image)\n",
    "\n",
    "# for keypoint in output[0]:\n",
    "#     plt.plot(keypoint[0], keypoint[1], 'ro')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d23048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7eb2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

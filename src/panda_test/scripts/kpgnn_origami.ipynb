{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f533ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27dc182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "# parent_path =  path.home + \"/Workspace/WPI/Summer2023/ws/duc_repo/src/panda_test/\" + \"data/sim_marker/\"\n",
    "parent_path = path.home + \"/lama/results/09_04_2023/\"\n",
    "\n",
    "# root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "root_dir = parent_path + \"rcnn\" + \"/\"\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fucntion tranforms an input image for diverseifying data for training\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba3144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to split the dataset into train, test and validation folder.\n",
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo \n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the arm\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_kp')\n",
    "            bboxes_labels_original.append('kp2')\n",
    "            bboxes_labels_original.append('kp3')\n",
    "            bboxes_labels_original.append('ee_kp')\n",
    "\n",
    "        if self.transform:\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "            \n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj):\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original  \n",
    "\n",
    "            # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "#         labels = [1, 2, 3, 4, 5, 6]   \n",
    "        labels = [1, 2, 3, 4]\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" \n",
    "dataset = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=True)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "# print(batch[2])\n",
    "\n",
    "# print(\"Original targets:\\n\", batch[3], \"\\n\\n\")\n",
    "# print(\"Transformed targets:\\n\", batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaeafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize how the transformed data looks \n",
    "\n",
    "keypoints_classes_ids2names = {0: 'base_joint', 1: 'joint2', 2: 'joint3', 3: 'joint4', 4: 'joint5', 5: 'joint6',\\\n",
    "                              6:'joint7', 7:'joint8', 8:'panda_finger_1', 9:'panda_finger_2'}\n",
    "\n",
    "def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):\n",
    "    fontsize = 18\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        start_point = (bbox[0], bbox[1])\n",
    "        end_point = (bbox[2], bbox[3])\n",
    "        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n",
    "    \n",
    "    for idx, kps in enumerate(keypoints):\n",
    "        for kp in kps:\n",
    "            image = cv2.circle(image.copy(), tuple(kp), 2, (255,0,0), 10)\n",
    "#         image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "    if image_original is None and keypoints_original is None:\n",
    "        plt.figure(figsize=(40,40))\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "    else:\n",
    "        for bbox in bboxes_original:\n",
    "            start_point = (bbox[0], bbox[1])\n",
    "            end_point = (bbox[2], bbox[3])\n",
    "            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)\n",
    "        \n",
    "        print(keypoints_original)\n",
    "        for idx, kps in enumerate(keypoints_original):\n",
    "            print(idx)\n",
    "            print(kps)\n",
    "            for kp in kps:\n",
    "                print(kp)\n",
    "                image_original = cv2.circle(image_original, tuple(kp), 5, (255,0,0), 2)\n",
    "#             image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n",
    "\n",
    "        ax[0].imshow(image_original)\n",
    "        ax[0].set_title('Original image', fontsize=fontsize)\n",
    "\n",
    "        ax[1].imshow(image)\n",
    "        ax[1].set_title('Transformed image', fontsize=fontsize)\n",
    "        \n",
    "        return None\n",
    "        \n",
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "# for kps in batch1[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "#     keypoints.append([kp[:2] for kp in [kps]])\n",
    "    \n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp[:2] for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "# for kps in batch1[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "#     keypoints_original.append([kp[:2] for kp in [kps]])\n",
    "    \n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp[:2] for kp in kps])\n",
    "\n",
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f0585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model(num_keypoints, weights_path=None):\n",
    "    \n",
    "#     anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n",
    "#     model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "#                                                                    pretrained_backbone=True,\n",
    "#                                                                    num_keypoints=num_keypoints,\n",
    "#                                                                    num_classes = 4, # Background is the first class, object is the second class\n",
    "#                                                                    rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "#     if weights_path:\n",
    "#         state_dict = torch.load(weights_path)\n",
    "#         model.load_state_dict(state_dict)        \n",
    "        \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93038e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.fc = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        out = torch.matmul(adj, x)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.keypoint_rcnn = torchvision.models.detection.keypointrcnn_resnet50_fpn(\n",
    "            pretrained=False, pretrained_backbone=True, num_keypoints=4, num_classes=5)\n",
    "        self.graph_conv1 = GraphConv(3, 128)\n",
    "        self.graph_conv2 = GraphConv(128, 3)\n",
    "\n",
    "    def forward(self, images, boxes, adj_matrix=None, targets=None, train=False):\n",
    "        if train:\n",
    "            output = self.keypoint_rcnn(images, targets)\n",
    "            return output  # This contains the losses during training\n",
    "        else:\n",
    "            with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n",
    "                self.keypoint_rcnn.eval()  # Set to evaluation mode\n",
    "                output = self.keypoint_rcnn(images)\n",
    "                self.keypoint_rcnn.train()  # Revert to training mode\n",
    "                \n",
    "            print(output)\n",
    "\n",
    "            keypoints = output[0]['keypoints']\n",
    "\n",
    "            keypoints = self.graph_conv1(keypoints, adj_matrix)\n",
    "            keypoints = nn.functional.relu(keypoints)\n",
    "            keypoints = self.graph_conv2(keypoints, adj_matrix)\n",
    "\n",
    "            return keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ecb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(predicted_keypoints, gt_keypoints, predicted_boxes, gt_boxes, adj_matrix, loss_keypoint):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    keypoint_loss = mse_loss(predicted_keypoints, gt_keypoints)\n",
    "    box_loss = mse_loss(predicted_boxes, gt_boxes)\n",
    "    \n",
    "    graph_loss = torch.sum((1 - adj_matrix) * (predicted_keypoints - gt_keypoints)**2)\n",
    "    \n",
    "    total_loss = keypoint_loss + 0.1 * box_loss + 0.01 * graph_loss + loss_keypoint\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5bdccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(num_keypoints):\n",
    "    # Initialize a zero matrix\n",
    "    adj_matrix = torch.zeros((num_keypoints, num_keypoints))\n",
    "    \n",
    "    # Fill the diagonal above the main diagonal with ones\n",
    "    for i in range(num_keypoints - 1):\n",
    "        adj_matrix[i, i + 1] = 1\n",
    "        \n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37937cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "model = CombinedModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create adjacency matrix\n",
    "num_keypoints = 4\n",
    "adj_matrix = create_adjacency_matrix(num_keypoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8ea37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_val = ClassDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(num_epochs):  # for 50 epochs\n",
    "    for batch in data_loader_train:\n",
    "#         print(\"Batch:\", batch)\n",
    "        images, targets = batch  \n",
    "#         print(images.size(0))\n",
    "#         print(images[0].shape)\n",
    "#         print(\"keys in target dict\", targets[0].keys())\n",
    "        ground_truth_keypoints = [target['keypoints'] for target in targets]\n",
    "        ground_truth_boxes = [target['boxes'] for target in targets]\n",
    "        \n",
    "        images = torch.stack(images)\n",
    "        ground_truth_keypoints = torch.stack(ground_truth_keypoints)\n",
    "        ground_truth_boxes = torch.stack(ground_truth_boxes)\n",
    "        \n",
    "        print(\"gt keypoints\", ground_truth_keypoints.shape)\n",
    "        print(\"Adj matrix\", adj_matrix, adj_matrix.shape)\n",
    "\n",
    "#         Create a batched adjacency matrix with the same batch size\n",
    "        batch_adj_matrix = adj_matrix.repeat(len(ground_truth_keypoints), 1, 1)\n",
    "    \n",
    "        print(\"Batch adjacency_matrix\", batch_adj_matrix, batch_adj_matrix.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass for training\n",
    "        output_train = model(images, ground_truth_boxes, adj_matrix=batch_adj_matrix, targets=targets, train=True)\n",
    "        print(\"Output keypoints shape\", output_train.keys())\n",
    "        \n",
    "        #Forward pass for loss\n",
    "        predicted_keypoints = model(images, ground_truth_boxes, adj_matrix=batch_adj_matrix, train=False)\n",
    "        \n",
    "        \n",
    "        print(\"predicted keypoints\", predicted_keypoints.shape)\n",
    "                \n",
    "        loss_keypoint = output_train['loss_keypoint']\n",
    "        \n",
    "        # Compute loss and backpropagate\n",
    "        loss = custom_loss(predicted_keypoints, ground_truth_keypoints, \n",
    "                           predicted_boxes=None, gt_boxes=ground_truth_boxes, \n",
    "                           adj_matrix=batch_adj_matrix, loss_keypoint=loss_keypoint)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bff46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_keypoints = 4\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "# dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = ClassDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "batch_sizes = [1,2,3]\n",
    "epochs_lst = [25,30,50]\n",
    "\n",
    "# batch_sizes = [3, 2, 1]\n",
    "# epochs_lst = [25, 30]\n",
    "# batch_sizes = [3]\n",
    "# epochs_lst = [50]\n",
    "\n",
    "v = 1\n",
    "\n",
    "for b_size in batch_sizes:\n",
    "    for epochs in epochs_lst:\n",
    "        data_loader_train = DataLoader(dataset_train, batch_size=b_size, shuffle=True, collate_fn=collate_fn)\n",
    "        data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "        data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        model = get_model(num_keypoints = total_keypoints)\n",
    "        model.to(device)\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.9, weight_decay=0.0005)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "        num_epochs = epochs\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=1000)\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "#             if epoch == 25 or epoch == 30 :\n",
    "#                 PATH = f\"/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_ld_b{b_size}_e{epoch}_v{v}.pth\"        \n",
    "#                 torch.save(model, PATH)\n",
    "#                 v+=1 \n",
    "            \n",
    "        \n",
    "        PATH = f\"/home/jc-merlab/lama/results/09_04_2023/trained_models/keypointsrcnn_weights_origami_b{b_size}_e{epochs}_v{v}.pth\"\n",
    "            \n",
    "            \n",
    "        torch.save(model, PATH)\n",
    "        \n",
    "           \n",
    "        \n",
    "\n",
    "\n",
    "#     evaluate(model, data_loader_val, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights after training\n",
    "# torch.save(model.state_dict(), 'keypointsrcnn_weights_120.pth')\n",
    "# torch.save(model, '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_ld_b1_e30_v3.pth')\n",
    "            \n",
    "# torch.save(model, PATH)\n",
    "PATH = os.path.join(parent_path, f\"trained_models/keypointsrcnn_weights_ld_b{b_size}_e{epochs}_v{v}.pth\")\n",
    "# PATH = os.path.join(parent_path, f\"trained_models/keypointsrcnn_weights_ld_b{1}_e{25}_v{'origami'}.pth\")            \n",
    "            \n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e5f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_VAL = parent_path + \"split_folder_output-2023-07-14\" +\"/val\"\n",
    "dataset_val = ClassDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "iterator = iter(data_loader_val)\n",
    "len(data_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, data_loader_val, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fad317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_path = '/home/user/Workspace/WPI/Summer2023/ws/duc_repo/src/panda_test/data/trained_models/keypointsrcnn_weights_ld_b1_e25_v9.pth'\n",
    "weights_path = PATH\n",
    "model = torch.load(weights_path).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = iter(data_loader_val)\n",
    "i = 1\n",
    "while True:\n",
    "    try:\n",
    "        images, targets = next(data_iterator)\n",
    "        images = list(img.to(device) for img in images)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            images = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "            scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "            high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "            post_nms_idxs = torchvision.ops.nms(outputs[0]['boxes'][high_scores_idxs], outputs[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "            keypoints = []\n",
    "            for kps in outputs[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "                keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "            bboxes = []\n",
    "            for bbox in outputs[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "                bboxes.append(list(map(int, bbox.tolist())))\n",
    "            img = visualize(images, bboxes, keypoints)\n",
    "#             cv2.imwrite(\"/home/jc-merlab/Pictures/Data/video_results_01/out_image_\" + str(i) + \".jpg\", img)\n",
    "            cv2.imshow(f'image{i}', img)\n",
    "\n",
    "            cv2.waitKey(2000)\n",
    "            cv2.destroyWindow(f'image{i}')\n",
    "\n",
    "            \n",
    "            i = i+1\n",
    "\n",
    "            # Calculate loss and metrics for evaluation here.\n",
    "    except StopIteration:\n",
    "        break         \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_path = '/home/user/Workspace/WPI/Summer2023/ws/duc_repo/src/panda_test/data/trained_models/keypointsrcnn_weights_ld_b1_e25_v1.pth'\n",
    "weights_path = PATH\n",
    "model = torch.load(weights_path).to(device)\n",
    "\n",
    "images1, targets1 = next(iterator)\n",
    "images2, targets2 = next(iterator)\n",
    "images3, targets3 = next(iterator)\n",
    "images4, targets4 = next(iterator)\n",
    "# images, targets = next(iterator)\n",
    "\n",
    "print(type(images1))\n",
    "\n",
    "\n",
    "images1 = list(image1.to(device) for image1 in images1)\n",
    "images2 = list(image2.to(device) for image2 in images2)\n",
    "images3 = list(image3.to(device) for image3 in images3)\n",
    "images4 = list(image4.to(device) for image4 in images4)\n",
    "# images = list(image.to(device) for image in images)\n",
    "\n",
    "print(type(images1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output1 = model(images1)\n",
    "    output2 = model(images2)\n",
    "    output3 = model(images3)\n",
    "    output4 = model(images4)\n",
    "#     output = model(image)\n",
    "\n",
    "# print(\"Predictions: \\n\", output1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2300e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions: \\n\", output1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca000731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, targets) in enumerate(data_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_ld_b1_e25_v2.pth'\n",
    "model = torch.load(weights_path).to(device)\n",
    "# model = get_model(num_keypoints=6, weights_path=weights_path)\n",
    "# model.load_state_dict(torch.load('keypointsrcnn_weights.pth'))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# print(type(model))\n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "cap = cv2.VideoCapture('/home/jc-merlab/Pictures/Data/inference_data/test_video_3d.avi')\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "i = 0\n",
    "print(type(i))\n",
    "while(cap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "    print(i)\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:        \n",
    "#         img = cv2.imread(frame)\n",
    "        image = Image.fromarray(frame)\n",
    "\n",
    "        image = F.to_tensor(image).to(device)\n",
    "        image.unsqueeze_(0)\n",
    "        image = list(image)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            start = time.time(now)\n",
    "            output = model(image)\n",
    "            stop = time.time(now)\n",
    "            print(\"time\", (stop - start))\n",
    "\n",
    "        image = (image[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "        # Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "        # Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "        # Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "        keypoints = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "        bboxes = []\n",
    "        for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            bboxes.append(list(map(int, bbox.tolist())))\n",
    "        img = visualize(image, bboxes, keypoints)\n",
    "        \n",
    "        cv2.imwrite(\"/home/jc-merlab/Pictures/Data/video_results_01/out_image_\" + str(i) + \".jpg\", img)\n",
    "    \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "    i = i+1\n",
    "    \n",
    "cap.release()\n",
    " \n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc57ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = (images1[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores1 = output1[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores1 > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output1[0]['boxes'][high_scores_idxs], output1[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output1[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "    \n",
    "print(keypoints)\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output1[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "image = visualize(image1, bboxes, keypoints)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b19363",
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = (images2[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores = output2[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output2[0]['boxes'][high_scores_idxs], output2[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output2[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output2[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image2, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image3 = (images3[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores3 = output3[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output3[0]['boxes'][high_scores_idxs], output3[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output3[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output3[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image3, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image4 = (images4[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores4 = output4[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output4[0]['boxes'][high_scores_idxs], output4[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output4[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output4[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image4, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"/home/jc-merlab/lama/results/08_14_2023/test_set/2023-08-14_13_55_39/13.jpg\")\n",
    "print(type(image))\n",
    "\n",
    "image = F.to_tensor(image).to(device)\n",
    "image.unsqueeze_(0)\n",
    "print(image.shape)\n",
    "image = list(image)\n",
    "# print(type(images))\n",
    "# images = list(image.to(device) for image in images)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(image)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (image[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "high_scores_idxs = np.where(scores > 0.1)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "keypoints = []\n",
    "for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "bboxes = []\n",
    "for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "    bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "visualize(image, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077377c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "weights_path = 'keypointsrcnn_weights.pth'\n",
    "model = get_model(num_keypoints=6, weights_path=weights_path)\n",
    "model.load_state_dict(torch.load('keypointsrcnn_weights.pth'))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# print(type(model))\n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "cap = cv2.VideoCapture('/home/jc-merlab/nov1_v1.avi')\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "i = 0\n",
    "print(type(i))\n",
    "while(cap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "    print(i)\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:        \n",
    "#         img = cv2.imread(frame)\n",
    "        image = Image.fromarray(frame)\n",
    "\n",
    "        image = F.to_tensor(image).to(device)\n",
    "        image.unsqueeze_(0)\n",
    "        image = list(image)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            output = model(image)\n",
    "\n",
    "        image = (image[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "        # Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "        # Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "        # Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "\n",
    "        keypoints = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append([list(map(int, kp[:2])) for kp in kps])\n",
    "\n",
    "        bboxes = []\n",
    "        for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            bboxes.append(list(map(int, bbox.tolist())))\n",
    "        img = visualize(image, bboxes, keypoints)\n",
    "        \n",
    "        cv2.imwrite(\"/home/jc-merlab/Pictures/Data/video_results/out_image_\" + str(i) + \".jpg\", img)\n",
    "        \n",
    "    i = i+1\n",
    "    \n",
    "cap.release()\n",
    " \n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99134e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import moviepy.video.io.ImageSequenceClip\n",
    "image_folder=\"/home/jc-merlab/Pictures/Data/video_results/\"\n",
    "\n",
    "fps=1\n",
    "\n",
    "image_files = [os.path.join(image_folder,img)\n",
    "               for img in os.listdir(image_folder)\n",
    "               if img.endswith(\".jpg\")]\n",
    "clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(image_files, fps=fps)\n",
    "clip.write_videofile('my_video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4af23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "2500//72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1070c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bedf1768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16908615680\n",
      "354418688\n",
      "257395712\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "237a5cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1df0e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch_geometric.nn as pyg\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "_EPS = 1e-10\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Two-layer fully-connected ELU net with batch norm.\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def batch_norm(self, inputs):\n",
    "        x = inputs.view(inputs.size(0) * inputs.size(1), -1)\n",
    "        x = self.bn(x)\n",
    "        return x.view(inputs.size(0), inputs.size(1), -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "#         print(\"Input shape before any operations: \", inputs.shape)\n",
    "\n",
    "        # Flatten the last two dimensions for the linear layer input\n",
    "#         x = inputs.view(inputs.size(0), -1)\n",
    "        x = func.elu(self.fc1(inputs))\n",
    "        x = func.dropout(x, self.dropout_prob, training=self.training)\n",
    "        x = func.elu(self.fc2(x))\n",
    "        \n",
    "        return self.batch_norm(x)\n",
    "\n",
    "        # Assuming you want to maintain the second dimension for some reason\n",
    "        # (like temporal sequence in a RNN), you would reshape the output\n",
    "        # back to the desired shape. If not, this step is unnecessary.\n",
    "        # output = x.view(inputs.size(0), inputs.size(1), -1)\n",
    "        # print(\"Output shape after forward pass: \", output.shape)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_out=4, do_prob=0., factor=True):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "\n",
    "        self.factor = factor\n",
    "\n",
    "        self.mlp1 = MLP(n_in, n_hid, n_hid, do_prob)\n",
    "        self.mlp2 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "        self.mlp3 = MLP(n_hid, n_hid, n_hid, do_prob)\n",
    "        if self.factor:\n",
    "            self.mlp4 = MLP(n_hid * 3, n_hid, n_hid, do_prob)\n",
    "            print(\"Using factor graph MLP encoder.\")\n",
    "        else:\n",
    "            self.mlp4 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "            print(\"mlp4\", self.mlp4)\n",
    "            print(\"Using MLP graph encoder.\")\n",
    "        self.fc_out = nn.Linear(n_hid, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def edge2node(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        incoming = torch.matmul(rel_rec.t(), x)\n",
    "        return incoming / incoming.size(1)\n",
    "\n",
    "    def node2edge(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        receivers = torch.matmul(rel_rec, x)\n",
    "        senders = torch.matmul(rel_send, x)\n",
    "        edges = torch.cat([receivers, senders], dim=2)\n",
    "        return edges\n",
    "\n",
    "    def forward(self, inputs, rel_rec, rel_send):\n",
    "        # Input shape: [num_sims, num_atoms, num_timesteps, num_dims]\n",
    "        x = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "#         print(\"x shape:\", x.shape)\n",
    "#         print(\"rel_rec shape:\", rel_rec.shape)\n",
    "#         print(\"rel_send shape:\", rel_send.shape)\n",
    "\n",
    "        # New shape: [num_sims, num_atoms, num_timesteps*num_dims]\n",
    "        x = self.mlp1(x)  # 2-layer ELU net per node\n",
    "\n",
    "        x = self.node2edge(x, rel_rec, rel_send)\n",
    "        x = self.mlp2(x)\n",
    "        x_skip = x    \n",
    "        \n",
    "        if self.factor:\n",
    "            x = self.edge2node(x, rel_rec, rel_send)\n",
    "            x = self.mlp3(x)\n",
    "            x = self.node2edge(x, rel_rec, rel_send)\n",
    "            x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "            x = self.mlp4(x)\n",
    "        else:\n",
    "            x = self.mlp3(x)\n",
    "            x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "            x = self.mlp4(x)\n",
    "\n",
    "        return self.fc_out(x)    \n",
    "    \n",
    "class GraphDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in_node, edge_types, msg_hid, msg_out, n_hid,\n",
    "                 do_prob=0., skip_first=False):\n",
    "        super(GraphDecoder, self).__init__()\n",
    "        self.msg_fc1 = nn.ModuleList(\n",
    "            [nn.Linear(2 * n_in_node, msg_hid) for _ in range(edge_types)])\n",
    "        self.msg_fc2 = nn.ModuleList(\n",
    "            [nn.Linear(msg_hid, msg_out) for _ in range(edge_types)])\n",
    "        self.msg_out_shape = msg_out\n",
    "        self.skip_first_edge_type = skip_first\n",
    "\n",
    "        self.out_fc1 = nn.Linear(n_in_node + msg_out, n_hid)\n",
    "        self.out_fc2 = nn.Linear(n_hid, n_hid)\n",
    "        self.out_fc3 = nn.Linear(n_hid, n_in_node)\n",
    "\n",
    "        print('Using learned graph decoder.')\n",
    "\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "    def single_step_forward(self, single_timestep_inputs, rel_rec, rel_send,\n",
    "                            single_timestep_rel_type):\n",
    "\n",
    "        # single_timestep_inputs has shape\n",
    "        # [batch_size, num_timesteps, num_atoms, num_dims]\n",
    "\n",
    "        # single_timestep_rel_type has shape:\n",
    "        # [batch_size, num_timesteps, num_atoms*(num_atoms-1), num_edge_types]\n",
    "\n",
    "        # Node2edge\n",
    "        receivers = torch.matmul(rel_rec, single_timestep_inputs)\n",
    "        senders = torch.matmul(rel_send, single_timestep_inputs)\n",
    "        pre_msg = torch.cat([receivers, senders], dim=-1)\n",
    "\n",
    "        all_msgs = Variable(torch.zeros(pre_msg.size(0), pre_msg.size(1),self.msg_out_shape))\n",
    "        if single_timestep_inputs.is_cuda:\n",
    "            all_msgs = all_msgs.cuda()\n",
    "\n",
    "        if self.skip_first_edge_type:\n",
    "            start_idx = 1\n",
    "        else:\n",
    "            start_idx = 0\n",
    "\n",
    "        # Run separate MLP for every edge type\n",
    "        # NOTE: To exlude one edge type, simply offset range by 1\n",
    "        for i in range(start_idx, len(self.msg_fc2)):\n",
    "            msg = func.relu(self.msg_fc1[i](pre_msg))\n",
    "            msg = func.dropout(msg, p=self.dropout_prob)\n",
    "            msg = func.relu(self.msg_fc2[i](msg))\n",
    "            msg = msg * single_timestep_rel_type[:, :, i:i + 1]\n",
    "            all_msgs += msg\n",
    "\n",
    "        # Aggregate all msgs to receiver\n",
    "        agg_msgs = all_msgs.transpose(-2, -1).matmul(rel_rec).transpose(-2, -1)\n",
    "        agg_msgs = agg_msgs.contiguous()\n",
    "\n",
    "        # Skip connection\n",
    "        aug_inputs = torch.cat([single_timestep_inputs, agg_msgs], dim=-1)\n",
    "\n",
    "        # Output MLP\n",
    "        pred = func.dropout(func.relu(self.out_fc1(aug_inputs)), p=self.dropout_prob)\n",
    "        pred = func.dropout(func.relu(self.out_fc2(pred)), p=self.dropout_prob)\n",
    "        pred = self.out_fc3(pred)\n",
    "#        print(pred.shape,single_timestep_inputs.shape)\n",
    "\n",
    "        # Predict position/velocity difference\n",
    "        return single_timestep_inputs + pred\n",
    "\n",
    "    def forward(self, inputs, rel_type, rel_rec, rel_send, pred_steps=1):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "\n",
    "\n",
    "        # Only take n-th timesteps as starting points (n: pred_steps)\n",
    "        last_pred = inputs[:, :, :]\n",
    "        #asa\n",
    "        curr_rel_type = rel_type[:, :, :]\n",
    "        preds=[]\n",
    "        #print(curr_rel_type.shape)\n",
    "        # NOTE: Assumes rel_type is constant (i.e. same across all time steps).\n",
    "\n",
    "        # Run n prediction steps\n",
    "        #for step in range(0, pred_steps):\n",
    "        last_pred = self.single_step_forward(last_pred, rel_rec, rel_send,\n",
    "                                                 curr_rel_type)\n",
    "        preds.append(last_pred)\n",
    "\n",
    "        sizes = [preds[0].size(0), preds[0].size(1),\n",
    "                 preds[0].size(2)]\n",
    "\n",
    "        output = Variable(torch.zeros(sizes))\n",
    "        if inputs.is_cuda:\n",
    "            output = output.cuda()\n",
    "\n",
    "        # Re-assemble correct timeline\n",
    "        for i in range(len(preds)):\n",
    "            output[:, :, :] = preds[i]\n",
    "\n",
    "        pred_all = output[:, :, :]\n",
    "\n",
    "        # NOTE: We potentially over-predicted (stored in future_pred). Unused.\n",
    "        # future_pred = output[:, (inputs.size(1) - 1):, :, :]\n",
    "\n",
    "        return pred_all#.transpose(1, 2).contiguous()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d61bf273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(input, axis=1):\n",
    "    trans_input = input.transpose(axis, 0).contiguous()\n",
    "    soft_max_1d = func.softmax(trans_input,dim=0)\n",
    "    return soft_max_1d.transpose(axis, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a4b6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path):\n",
    "        super(KeypointPipeline, self).__init__()  \n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.encoder = GraphEncoder(4,512,4,0.5,True)\n",
    "        self.decoder = GraphDecoder(n_in_node=4,\n",
    "                                 edge_types=2,\n",
    "                                 msg_hid=512,\n",
    "                                 msg_out=512,\n",
    "                                 n_hid=512,\n",
    "                                 do_prob=0.5,\n",
    "                                 skip_first=False)\n",
    "        \n",
    "#         self.off_diag = np.ones([6,6]) - np.eye(6)\n",
    "\n",
    "#         self.rel_rec = np.array(encode_onehot(np.where(self.off_diag)[1]), dtype=np.float32)\n",
    "#         self.rel_send = np.array(encode_onehot(np.where(self.off_diag)[0]), dtype=np.float32)\n",
    "#         self.rel_rec = torch.FloatTensor(self.rel_rec)\n",
    "#         self.rel_send = torch.FloatTensor(self.rel_send)\n",
    "\n",
    "        # Define a unidirectional cyclical graph\n",
    "        num_nodes = 6\n",
    "        self.off_diag = np.zeros([num_nodes, num_nodes])\n",
    "        \n",
    "        # Creating a cycle: 1->2, 2->3, ..., 6->1\n",
    "        for i in range(num_nodes):\n",
    "            self.off_diag[i, (i + 1) % num_nodes] = 1\n",
    "\n",
    "        # Update rel_rec and rel_send based on the new off_diag\n",
    "        self.rel_rec = np.array(encode_onehot(np.where(self.off_diag)[1]), dtype=np.float32)\n",
    "        self.rel_send = np.array(encode_onehot(np.where(self.off_diag)[0]), dtype=np.float32)\n",
    "        self.rel_rec = torch.FloatTensor(self.rel_rec).to(device)\n",
    "        self.rel_send = torch.FloatTensor(self.rel_send).to(device)\n",
    "\n",
    "        self.encoder= self.encoder.cuda()\n",
    "        self.decoder = self.decoder.cuda()\n",
    "        self.rel_rec = self.rel_rec.cuda()\n",
    "        self.rel_send = self.rel_send.cuda()\n",
    "    \n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \n",
    "                                            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "\n",
    "        confidence = output[0]['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0,0:2])) + [confidence[idx]] + [labels[idx]])\n",
    "        \n",
    "        # Sort keypoints based on label\n",
    "        keypoints.sort(key=lambda x: x[-1])\n",
    "        return keypoints\n",
    "    \n",
    "    def keypoints_to_graph(self, keypoints, image_width, image_height):\n",
    "        # keypoints is expected to be a tensor with shape (num_keypoints, 4),\n",
    "        # where each keypoint is (x, y, score, label).\n",
    "        # Convert all elements in keypoints to tensors if they are not already\n",
    "        keypoints = [torch.tensor(kp, dtype=torch.float32).to(device) if not isinstance(kp, torch.Tensor) else kp for kp in keypoints]\n",
    "\n",
    "        # Then stack them\n",
    "        keypoints = torch.stack(keypoints).to(device)        \n",
    "        \n",
    "        # Remove duplicates: Only keep the keypoint with the highest score for each label\n",
    "        unique_labels, best_keypoint_indices = torch.unique(keypoints[:, 3], return_inverse=True)\n",
    "        best_scores, best_indices = torch.max(keypoints[:, 2].unsqueeze(0) * (best_keypoint_indices == torch.arange(len(unique_labels)).unsqueeze(1).cuda()), dim=1)\n",
    "        keypoints = keypoints[best_indices]\n",
    "        \n",
    "#         print(\"init keypoints in graph features\", keypoints)\n",
    "\n",
    "        # Normalize x and y to be in the range [-1, 1]\n",
    "        keypoints[:, 0] = (keypoints[:, 0] - image_width / 2) / (image_width / 2)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] - image_height / 2) / (image_height / 2)\n",
    "\n",
    "        # Use only x, y, and score for the graph features\n",
    "        graph_features = keypoints[:, :4]  # Now shape is (num_keypoints, 3)\n",
    "        \n",
    "        # Ensure the shape is [num_keypoints, 3] before returning\n",
    "        graph_features = graph_features.view(-1, 4)  # Reshape to ensure it's [num_keypoints, 3]\n",
    "#         print(\"graph features\", graph_features)\n",
    "        print(\"graph features shape\", graph_features.shape)\n",
    "\n",
    "        return graph_features\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        # Temporarily set the keypoint model to evaluation mode\n",
    "        keypoint_model_training = self.keypoint_model.training\n",
    "        self.keypoint_model.eval()\n",
    "\n",
    "        # Process each image in the batch\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = [self.keypoint_model(img.unsqueeze(0).to(device)) for img in imgs]\n",
    "\n",
    "        # Set the keypoint model back to its original training mode\n",
    "        self.keypoint_model.train(mode=keypoint_model_training)\n",
    "\n",
    "        # Process model outputs to get labeled keypoints\n",
    "        batch_labeled_keypoints = [self.process_model_output(output) for output in batch_outputs]\n",
    "        # Generate graph input tensor for each image and handle varying number of keypoints\n",
    "        batch_x = []\n",
    "        for labeled_keypoints in batch_labeled_keypoints:\n",
    "            keypoints = self.keypoints_to_graph(labeled_keypoints, 640, 480)\n",
    "\n",
    "            # Initialize x with zeros for 6 nodes with 4 features each\n",
    "            x = torch.zeros(1, 6, 4, device=device)\n",
    "\n",
    "            # Ensure that keypoints are on the correct device and fill in x\n",
    "            num_keypoints_detected = keypoints.size(0)\n",
    "            if num_keypoints_detected <= 6:\n",
    "                x[0, :num_keypoints_detected, :] = keypoints\n",
    "            else:\n",
    "                raise ValueError(\"Number of keypoints detected exceeds the maximum of 6.\")\n",
    "\n",
    "            batch_x.append(x)\n",
    "\n",
    "        # Stack the batch of x tensors for batch processing\n",
    "        batch_x = torch.cat(batch_x, dim=0)\n",
    "\n",
    "        # Forward pass through the encoder and decoder\n",
    "        logits = self.encoder(batch_x, self.rel_rec, self.rel_send)\n",
    "        edges = my_softmax(logits, -1)\n",
    "        KGNN2D = self.decoder(batch_x, edges, self.rel_rec, self.rel_send)\n",
    "\n",
    "        return logits, KGNN2D, batch_labeled_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90ea42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_edges(valid_points, edges):\n",
    "#     off_diag = np.ones([6, 6]) - np.eye(6)\n",
    "#     idx =  torch.LongTensor(np.where(off_diag)[1].reshape(6,5)).cuda()\n",
    "#     if valid_points.ndim == 1:\n",
    "#         valid_points = valid_points.unsqueeze(0)  # Reshape to 2D if necessary\n",
    "\n",
    "#     relations = torch.zeros(valid_points.shape[0],valid_points.shape[1]*(valid_points.shape[1]-1)).cuda()\n",
    "#     for count,vis in enumerate(valid_points):\n",
    "#         vis = vis.view(-1,1) \n",
    "#         vis = vis*vis.t()\n",
    "#         vis = torch.gather(vis,1,idx)\n",
    "#         relations[count] = vis.view(-1)\n",
    "#     relations = relations.type(torch.LongTensor).cuda() \n",
    "#     loss_edges = func.cross_entropy(edges.view(-1, 2), relations.view(-1))\n",
    "#     return loss_edges\n",
    "\n",
    "# def loss_kp(gt_keypoints, pred_keypoints):\n",
    "#     # Convert pred_keypoints to tensor if it's a list\n",
    "    \n",
    "#     if isinstance(pred_keypoints, list):\n",
    "#         pred_keypoints = torch.stack([torch.tensor(kp, device=gt_keypoints.device, dtype=torch.float32) if isinstance(kp, list) else kp for kp in pred_keypoints])\n",
    "\n",
    "#     # Ensure gt_keypoints is a tensor\n",
    "#     if not isinstance(gt_keypoints, torch.Tensor):\n",
    "#         gt_keypoints = torch.tensor(gt_keypoints, dtype=torch.float32, device=pred_keypoints.device)\n",
    "\n",
    "#     # Check if the shape of gt_keypoints is as expected, it should be [N, M] where N is the number of keypoints and M is the properties of each keypoint (like x, y, visibility, etc.)\n",
    "#     if gt_keypoints.dim() != 2 or gt_keypoints.size(-1) < 3:\n",
    "#         raise ValueError(\"gt_keypoints must be a 2D tensor with shape [N, M] where M >= 3.\")\n",
    "\n",
    "#     # Initialize a mask for selecting valid keypoints in gt_keypoints\n",
    "#     valid_gt_mask = (gt_keypoints[:, -1] == 1)\n",
    "\n",
    "#     # Ensure the mask is one-dimensional\n",
    "#     if valid_gt_mask.dim() != 1:\n",
    "#         raise ValueError(\"The mask must be one-dimensional\")\n",
    "\n",
    "#     # Filter the gt_keypoints and pred_keypoints based on the mask\n",
    "#     filtered_gt_keypoints = gt_keypoints[valid_gt_mask][:, :2]  # x, y columns\n",
    "#     filtered_pred_keypoints = pred_keypoints[valid_gt_mask][:, :2]  # x, y columns\n",
    "\n",
    "#     # Compute the loss using Smooth L1 Loss on the filtered keypoints\n",
    "#     loss = func.smooth_l1_loss(filtered_pred_keypoints, filtered_gt_keypoints, reduction='none')\n",
    "\n",
    "#     # Apply the mask to the loss to consider only valid keypoints\n",
    "#     valid_loss = loss[valid_gt_mask]\n",
    "#     return valid_loss.sum() / valid_gt_mask.float().sum()  \n",
    "    \n",
    "# def kgnn2d_loss(gt_keypoints, pred_keypoints):\n",
    "#     loss = func.mse_loss(pred_keypoints, gt_keypoints)\n",
    "    \n",
    "#     return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "411072cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "def process_keypoints(keypoints):\n",
    "    # Assuming keypoints is a list of Nx3 tensors where N is the number of keypoints\n",
    "    # and each keypoint is represented as [x, y, visibility]\n",
    "    # Remove the unnecessary middle dimension\n",
    "    keypoints = [kp.squeeze(1) for kp in keypoints]\n",
    "    visibilities = [kp[:, 2] for kp in keypoints]  # Extract visibility flags\n",
    "    valid_vis_all = torch.cat([v == 1 for v in visibilities]).long().cuda()\n",
    "    valid_invis_all = torch.cat([v == 0 for v in visibilities]).long().cuda()\n",
    "\n",
    "    keypoints_gt = torch.cat([kp[:, :2] for kp in keypoints]).float().cuda()  # Gather all keypoints and discard visibility flags\n",
    "    keypoints_gt = keypoints_gt.view(-1, 2).unsqueeze(0)  # Add an extra dimension to match expected shape for loss_edges\n",
    "\n",
    "    return keypoints_gt, valid_vis_all, valid_invis_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58d23048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torchvision\n",
    "# import numpy as np\n",
    "# from torchvision.transforms import functional as F\n",
    "# from PIL import Image\n",
    "# import cv2\n",
    "\n",
    "# model = KeypointPipeline(weights_path)\n",
    "# model = model.to(device)\n",
    "\n",
    "# new_weights_path = '/home/jc-merlab/Pictures/Data/trained_models/krcnn_occ_b32_e200_v0.pth'\n",
    "# model = torch.load(weights_path).to(device)\n",
    "\n",
    "# # image = Image.open(\"/home/jc-merlab/Pictures/Data/split_folder_output-2023-10-22/test/images/002510.rgb.jpg\")\n",
    "# image = Image.open(\"/home/jc-merlab/Pictures/Data/2023-08-14-Occluded/000207.rgb.jpg\")\n",
    "# print(type(image))\n",
    "\n",
    "# image = F.to_tensor(image).to(device)\n",
    "# # image.unsqueeze_(0)\n",
    "# print(image.shape)\n",
    "# # image = list(image)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     output = model(image)\n",
    "    \n",
    "# print(output[1])\n",
    "    \n",
    "# pred_kp = output[1]\n",
    "\n",
    "# # Replace these with your actual image dimensions\n",
    "# image_width = 640\n",
    "# image_height = 480\n",
    "\n",
    "\n",
    "# # Denormalize keypoints\n",
    "# # denormalized_x = (pred_kp[:, :, 0] * (image_width / 2)) + (image_width / 2)\n",
    "# # denormalized_y = (pred_kp[:, :, 1] * (image_height / 2)) + (image_height / 2)\n",
    "\n",
    "# # Denormalize keypoints\n",
    "# denormalized_x = (pred_kp[:, :, 0] + 1) * (image_width / 2)\n",
    "# denormalized_y = (pred_kp[:, :, 1] + 1) * (image_height / 2)\n",
    "\n",
    "# # Stack the denormalized x and y coordinates together to form [n, 2] tensor\n",
    "# denormalized_keypoints = torch.stack((denormalized_x, denormalized_y), dim=2)\n",
    "\n",
    "# print(\"Denormalized Keypoints:\", denormalized_keypoints)\n",
    "\n",
    "# kp_numpy = denormalized_keypoints.cpu().numpy()\n",
    "# kp_flat_list = kp_numpy.reshape(-1).tolist()  # Convert to a flat list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a7eb2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_keypoints(image_path, all_keypoints, point_radius=5, keypoint_color=(255, 0, 0)):\n",
    "#     \"\"\"\n",
    "#     Visualize keypoints on the given image. Expects keypoints in the format of a list with a flat structure [x1,y1,x2,y2,...]\n",
    "#     for each keypoint set associated with an object.\n",
    "#     \"\"\"\n",
    "#     image = cv2.imread(image_path)\n",
    "    \n",
    "# #     if bboxes:\n",
    "# #         for bbox in bboxes:\n",
    "# #             cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), bbox_color, 2)\n",
    "    \n",
    "#     for keypoints in all_keypoints:\n",
    "#         for i in range(0, len(keypoints), 2):\n",
    "#             x, y = int(keypoints[i]), int(keypoints[i+1])\n",
    "#             cv2.circle(image, (x, y), point_radius, keypoint_color, -1)\n",
    "    \n",
    "#     return image\n",
    "\n",
    "# image = \"/home/jc-merlab/Pictures/Data/2023-08-14-Occluded/000207.rgb.jpg\"\n",
    "\n",
    "# output_image = visualize_keypoints(image,[kp_flat_list])\n",
    "\n",
    "# # Display or save the output image as needed\n",
    "# cv2.imshow(\"Keypoints\", output_image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37862bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def calculate_accuracy(pred_kps, gt_kps, threshold=10):\n",
    "    \"\"\"Calculate accuracy based on distance threshold.\"\"\"\n",
    "    distances = torch.norm(pred_kps - gt_kps, dim=2)\n",
    "    correct = torch.le(distances, threshold).all(dim=1)\n",
    "    accuracy = torch.mean(correct.float())\n",
    "    return accuracy.item()\n",
    "\n",
    "def visualize_keypoints(image, pred_keypoints, gt_keypoints, pred_color=(0, 255, 0), gt_color=(255, 0, 0), pred_radius=7, gt_radius=5):\n",
    "    \"\"\"Visualize predicted and ground truth keypoints.\"\"\"\n",
    "    for x, y in pred_keypoints:\n",
    "        cv2.circle(image, (int(x), int(y)), pred_radius, pred_color, -1)\n",
    "    for x, y in gt_keypoints:\n",
    "        cv2.circle(image, (int(x), int(y)), gt_radius, gt_color, -1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbd043d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004211.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([2, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006465.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004503.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004906.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001323.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004819.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003334.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005915.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001755.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002030.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004393.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003698.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000580.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004398.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([2, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006173.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004002.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004988.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005995.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005651.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001503.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002769.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005972.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000058.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006627.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001973.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000443.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005169.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005895.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005904.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000913.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005480.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001352.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000916.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001990.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([2, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005342.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([1, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005979.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005404.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000552.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005398.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004428.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006639.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001961.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004508.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001629.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000482.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003531.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004820.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006241.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005502.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003059.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002779.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004363.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003063.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000772.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006419.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004418.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001148.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([2, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002036.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001463.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004344.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006273.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000581.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003268.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001051.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005925.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002706.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003367.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001647.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002404.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003675.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002046.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003716.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001559.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001083.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001936.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000701.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004647.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006151.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000617.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001875.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002553.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003943.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000806.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000004.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005234.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000764.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005798.rgb.jpg: 0.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000895.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003976.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001735.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002655.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001885.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005293.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004417.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003285.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006593.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000257.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001030.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000159.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001949.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002444.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001569.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003825.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004100.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001840.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006296.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006382.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000026.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000794.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005840.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000566.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006230.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002973.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002185.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001362.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003298.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([1, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005353.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003978.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002157.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006187.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003874.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000524.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003469.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001295.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000871.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002281.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003981.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004677.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005572.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005965.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001804.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000938.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002171.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002519.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001751.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000474.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003561.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005122.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003893.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004158.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000496.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003630.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000414.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004969.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006503.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005516.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003466.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005250.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003789.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004537.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002786.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003872.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003588.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004322.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 005584.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([1, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006378.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000172.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([2, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 006246.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 001558.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004720.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000802.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000991.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([5, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 002902.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 003329.rgb.jpg: 0.00%\n",
      "graph features shape torch.Size([6, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 000120.rgb.jpg: 100.00%\n",
      "graph features shape torch.Size([4, 4])\n",
      "torch.Size([6, 4])\n",
      "Accuracy for 004239.rgb.jpg: 0.00%\n",
      "Mean Accuracy across all images: 22.29%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load your model\n",
    "new_weights_path = '/home/jc-merlab/Pictures/Data/trained_models/krcnn_occ_b128_e200_v0.pth'\n",
    "model = torch.load(new_weights_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# source_folder = '/home/jc-merlab/Pictures/Data/only_occ_data/'\n",
    "source_folder = '/home/jc-merlab/Pictures/Data/split_folder_output-2023-11-17/val/folder'\n",
    "output_folder = '/home/jc-merlab/Pictures/Data/occ_results/13/'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "total_accuracy = 0\n",
    "num_images = 0\n",
    "image_width = 640\n",
    "image_height = 480\n",
    "\n",
    "for filename in os.listdir(source_folder):\n",
    "    if filename.endswith(\".rgb.jpg\"):\n",
    "        image_path = os.path.join(source_folder, filename)\n",
    "        annotation_path = os.path.join(source_folder, filename.replace('.rgb.jpg','.json'))\n",
    "\n",
    "        # Load image and annotation\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        # Preprocess image\n",
    "        input_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Predict keypoints\n",
    "        with torch.no_grad():\n",
    "            _, pred_kps, _ = model([input_tensor.squeeze(0)])\n",
    "            \n",
    "        print(pred_kps[0].shape)\n",
    "\n",
    "        # pred_kps is a batch. Since batch size is 1, take the first element.\n",
    "        pred_kps = pred_kps[0]\n",
    "\n",
    "        # Denormalize keypoints\n",
    "        denormalized_pred_kps = torch.stack(((pred_kps[:,0] * (image_width / 2)) + (image_width / 2), \n",
    "                                             (pred_kps[:,1] * (image_height / 2)) + (image_height / 2)), dim=1)\n",
    "\n",
    "        # Extract ground truth keypoints and format them correctly\n",
    "        gt_kps = torch.tensor([kp[0][:2] for kp in annotation['keypoints']], dtype=torch.float32)\n",
    "\n",
    "        # Calculate accuracy (Implement calculate_accuracy according to your requirements)\n",
    "        accuracy = calculate_accuracy(denormalized_pred_kps.to(device), gt_kps.unsqueeze(0).to(device))\n",
    "        total_accuracy += accuracy\n",
    "        num_images += 1\n",
    "\n",
    "        # Visualize keypoints (Implement visualize_keypoints according to your requirements)\n",
    "        image_np = np.array(image)\n",
    "        output_image = visualize_keypoints(image_np, denormalized_pred_kps.cpu().numpy(), gt_kps.numpy())\n",
    "\n",
    "        print(f'Accuracy for {filename}: {accuracy * 100:.2f}%')\n",
    "        \n",
    "        # Save the output image\n",
    "        output_image_path = os.path.join(output_folder, f'visualized_{filename}')\n",
    "        cv2.imwrite(output_image_path, cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# Calculate mean accuracy\n",
    "if num_images > 0:\n",
    "    mean_accuracy = total_accuracy / num_images\n",
    "    print(f'Mean Accuracy across all images: {mean_accuracy * 100:.2f}%')\n",
    "else:\n",
    "    print('No images were processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac31b34c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

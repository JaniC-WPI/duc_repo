{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as func\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac7fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd833b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fucntion tranforms an input image for diverseifying data for training\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), \n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1), \n",
    "        ], p=1),\n",
    "        A.Resize(640, 480),  # Resize every image to 640x480 after all other transformations\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'),\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to split the dataset into train, test and validation folder.\n",
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo \n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the arm\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_kp')\n",
    "            bboxes_labels_original.append('joint1')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "\n",
    "        if self.transform:\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "            \n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj):\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original  \n",
    "\n",
    "            # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]   \n",
    "#         labels = [1, 2, 3, 4]\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)                     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c16e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph_for_training(gt_keypoints):\n",
    "    N = gt_keypoints.shape[0]\n",
    "    edge_index = [(i, (i + 1) % N) for i in range(N)]\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    x = torch.tensor(gt_keypoints[:, :2], dtype=torch.float)  # assuming keypoints are (x, y, visibility)\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# def construct_graph_for_training(gt_keypoints):\n",
    "#     N = gt_keypoints.shape[0]\n",
    "#     edge_index = []\n",
    "#     for i in range(N):\n",
    "#         for j in range(N):\n",
    "#             edge_index.append((i, j))\n",
    "\n",
    "#     edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "#     x = torch.tensor(gt_keypoints[:, :2], dtype=torch.float)\n",
    "#     return Data(x=x, edge_index=edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph_for_prediction(keypoints, total_keypoints=6):\n",
    "    if len(keypoints) < total_keypoints:\n",
    "        dummy_keypoints = np.zeros((total_keypoints - len(keypoints), 2))\n",
    "        keypoints = np.concatenate([keypoints, dummy_keypoints], axis=0)\n",
    "\n",
    "    N = keypoints.shape[0]\n",
    "    edge_index = [(i, (i + 1) % N) for i in range(N)]\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    x = torch.tensor(keypoints[:, :2], dtype=torch.float)\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# def construct_graph_for_prediction(keypoints, total_keypoints=6):\n",
    "#     # If there are missing keypoints, add dummy nodes.\n",
    "#     if len(keypoints) < total_keypoints:\n",
    "#         dummy_keypoints = np.zeros((total_keypoints - len(keypoints), 3))\n",
    "#         keypoints = np.concatenate([keypoints, dummy_keypoints], axis=0)\n",
    "\n",
    "#     N = keypoints.shape[0]\n",
    "#     edge_index = []\n",
    "#     for i in range(N):\n",
    "#         for j in range(N):\n",
    "#             edge_index.append((i, j))\n",
    "\n",
    "#     edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "#     x = torch.tensor(keypoints[:, :2], dtype=torch.float)\n",
    "#     return Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15817fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class SimpleGNNLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SimpleGNNLayer, self).__init__(aggr='add')  # 'add' aggregation\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Add self loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Transform node feature matrix.\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=self.lin(x))\n",
    "\n",
    "    def message(self, x_j, edge_index, size):\n",
    "        # Compute normalization.\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "class SimpleGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        self.layer1 = SimpleGNNLayer(in_channels, hidden_channels)\n",
    "        self.layer2 = SimpleGNNLayer(hidden_channels, hidden_channels)\n",
    "#         self.layer3 = SimpleGNNLayer(hidden_channels, hidden_channels)\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = torch.relu(self.layer1(x, edge_index))\n",
    "        x = torch.relu(self.layer2(x, edge_index))\n",
    "#         x = torch.relu(self.layer3(x, edge_index))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f9fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridModel, self).__init__()\n",
    "        \n",
    "        # Pre-trained ResNet for feature extraction\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # Remove the final FC layer\n",
    "        \n",
    "        # Your Simple GNN model (assuming 2D keypoints)\n",
    "        self.gnn = SimpleGNN(in_channels=2 + 2048, hidden_channels=128, out_channels=2)\n",
    "        \n",
    "    def forward(self, x, data):\n",
    "        # Get feature map from CNN\n",
    "        features = self.resnet(x)\n",
    "\n",
    "        # Reshape and repeat features to match keypoints\n",
    "        n = data.x.size(0)\n",
    "        repeated_features = features.unsqueeze(2).repeat(1, 1, n).transpose(1, 2).reshape(-1, 2048)\n",
    "\n",
    "        # Concatenate features with keypoints\n",
    "        combined_data = torch.cat((data.x, repeated_features), dim=1)\n",
    "        data.x = combined_data\n",
    "\n",
    "        # Pass through GNN\n",
    "        x = self.gnn(data)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d44dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HybridModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0005)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 4\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_val = ClassDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(data_loader_train):\n",
    "        images, targets = batch\n",
    "        images = torch.stack(images).to(device)  \n",
    "        for i in range(len(images)):\n",
    "            gt_keypoints = targets[i]['keypoints'].to(device).squeeze()\n",
    "            data = construct_graph_for_training(gt_keypoints).to(device)\n",
    "            print(\"Gt Keypoints:\", gt_keypoints[:,:2])\n",
    "            optimizer.zero_grad()\n",
    "            out = model(images[i].unsqueeze(0), data)\n",
    "            print(\"Predicted Keypoints:\", out)\n",
    "            loss = criterion(out, gt_keypoints[:, :2].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(f'Epoch:{epoch} and Loss:{loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_model = HybridModel().to(device)\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'\n",
    "cnn_model = torch.load(weights_path).to(device)\n",
    "image = '/home/jc-merlab/Pictures/Data/2023-08-14-Occluded/002654.rgb.jpg'\n",
    "image = Image.open(image).convert(\"RGB\")\n",
    "\n",
    "def predict_keypoints(cnn_model, gnn_model, image):\n",
    "    gnn_model.eval()\n",
    "    cnn_model.eval()\n",
    "    image = F.to_tensor(image).to(device)\n",
    "#     image = list(image)    \n",
    "    with torch.no_grad():\n",
    "        output = cnn_model([image])  \n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() \n",
    "        keypoints = []\n",
    "        labels = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append(list(map(int, kps[0,0:2])))        \n",
    "        for label in output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            labels.append(label)\n",
    "        initial_keypoints = [x for _,x in sorted(zip(labels,keypoints))]\n",
    "        print(initial_keypoints)\n",
    "        data = construct_graph_for_prediction(initial_keypoints)\n",
    "        data = data.to(device)\n",
    "        predicted_keypoints = gnn_model(image.unsqueeze(0), data).cpu().numpy()\n",
    "    print(predicted_keypoints)\n",
    "    return predicted_keypoints, initial_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keypoints, gt_keypoints = predict_keypoints(cnn_model, gnn_model, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_keypoints(image_path, keypoints, gt_keypoints):\n",
    "    \"\"\"\n",
    "    Visualize the keypoints on an image.\n",
    "    \n",
    "    Args:\n",
    "    - image_path (str): Path to the image.\n",
    "    - keypoints (np.array): Array of keypoints, assumed to be in (x, y) format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "#     img = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(image_path)\n",
    "    print(type(keypoints))\n",
    "    # Extract the x and y coordinates\n",
    "    x_coords = keypoints[:, 0]\n",
    "    y_coords = keypoints[:, 1]\n",
    "    \n",
    "    print(type(gt_keypoints))\n",
    "    gt_keypoints = np.array(gt_keypoints)\n",
    "    \n",
    "    x_gt = gt_keypoints[:, 0]\n",
    "    y_gt = gt_keypoints[:, 1]\n",
    "    \n",
    "    # Plot the keypoints\n",
    "    ax.scatter(x_coords, y_coords, c='r', s=40, label=\"Keypoints\")\n",
    "    ax.scatter(x_gt, y_gt, c='b', s=40, label=\"gt_keypoints\")\n",
    "    \n",
    "    # Show the image with keypoints\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_keypoints(image, predicted_keypoints, gt_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660fad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

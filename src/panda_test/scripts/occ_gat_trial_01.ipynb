{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b59f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c5b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "#         A.Resize(640, 480)  # Resize all images to be 640x480\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1938c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96385476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b95de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as functional\n",
    "# from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "# from torch_geometric.nn import GATConv\n",
    "# from torch_geometric.data import Data\n",
    "\n",
    "# class GAT(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(GAT, self).__init__()\n",
    "#         self.conv1 = GATConv(5, 16, heads=4, dropout=0.4)\n",
    "#         self.conv2 = GATConv(64, 16, heads=4, dropout=0.4)\n",
    "#         self.conv3 = GATConv(64, 16, heads=4, dropout=0.4)  # New layer\n",
    "#         self.conv4 = GATConv(64, 32, heads=4, dropout=0.4)  # New layer\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = functional.elu(self.conv1(x, edge_index))\n",
    "#         x = functional.elu(self.conv2(x, edge_index))\n",
    "#         x = functional.elu(self.conv3(x, edge_index))  # Use the new layer\n",
    "#         x = self.conv4(x, edge_index)  # Use the new layer\n",
    "#         return x  # This is the predicted keypoints. Shape should be [num_nodes, num_node_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as func\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class KeyPointGNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeyPointGNN, self).__init__()\n",
    "        # Define graph convolution layers\n",
    "        self.conv1 = GCNConv(5, 256) # 5 features for each keypoint\n",
    "        self.conv2 = GCNConv(256, 128)\n",
    "        self.conv3 = GCNConv(128,64)\n",
    "        self.fc = torch.nn.Linear(64, 5) # Output 5 values (x, y, c, v, label) for each keypoint\n",
    "        \n",
    "        # Linear layers for edge feature prediction\n",
    "        self.edge_fc1 = torch.nn.Linear(2, 128) # Assuming edge_features has 2 elements\n",
    "        self.edge_fc2 = torch.nn.Linear(128, 2) # Output size depends on your edge features\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_features = data.x, data.edge_index, data.edge_attr\n",
    "        # Graph convolution layers\n",
    "        x = func.relu(self.conv1(x, edge_index))\n",
    "        x = func.relu(self.conv2(x, edge_index))\n",
    "        x = func.relu(self.conv3(x, edge_index))\n",
    "        # Fully connected layer to output keypoint locations\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Linear layers for edge feature prediction\n",
    "        edge_features = func.relu(self.edge_fc1(edge_features))\n",
    "        predicted_edge_features = self.edge_fc2(edge_features)\n",
    "        \n",
    "        return x, predicted_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e30a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path, num_vertices, delta=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.num_vertices = num_vertices\n",
    "        self.delta = delta\n",
    "\n",
    "        # Integrate the GAT model\n",
    "        self.gcn = KeyPointGNN().to(device)\n",
    "\n",
    "    def get_node_features(self, keypoints):\n",
    "        print(\"Vertices in Node features\", keypoints)\n",
    "        node_features = []\n",
    "        for keypoint in keypoints:\n",
    "            x, y, confidence, visibility, label = keypoint\n",
    "            node_features.append([x, y, confidence, visibility, label])\n",
    "        return torch.tensor(node_features, dtype=torch.float).to(device)\n",
    "\n",
    "    def get_edge_features(self, keypoints):\n",
    "        edges = [(0,1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)]\n",
    "        edge_features = []\n",
    "        for edge in edges:\n",
    "            k1, k2 = keypoints[edge[0]][:2], keypoints[edge[1]][:2]\n",
    "            distance = torch.norm(k1 - k2)\n",
    "            angle = torch.atan2(k2[1] - k1[1], k2[0] - k1[0])\n",
    "            edge_features.append([distance.item(), angle.item()])\n",
    "        return torch.tensor(edges, dtype=torch.long).t().contiguous().to(device), \\\n",
    "               torch.tensor(edge_features, dtype=torch.float).to(device)    \n",
    "\n",
    "\n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \n",
    "                                            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "\n",
    "        confidence = output[0]['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            # Setting t_i = 1 because label is found\n",
    "            keypoints.append(list(map(int, kps[0,0:2])) + [confidence[idx]] + [1] + [labels[idx]])\n",
    "\n",
    "        # Create a dictionary where the key is the label and the value is the keypoint\n",
    "        label_to_keypoint = {}\n",
    "        for keypoint in keypoints:\n",
    "            label = keypoint[-1]\n",
    "            if label not in label_to_keypoint or label_to_keypoint[label][-2] < keypoint[-2]:\n",
    "                label_to_keypoint[label] = keypoint\n",
    "\n",
    "        # Use a dictionary to keep track of all possible keypoints and their locations.\n",
    "        # Initialize with placeholders for missing keypoints.\n",
    "        all_keypoints = {i: [-1, -1, 0, 0, i] for i in range(1, self.num_vertices+1)}  # added another 0 for t_i\n",
    "\n",
    "        for label, keypoint in label_to_keypoint.items():\n",
    "            all_keypoints[label] = keypoint\n",
    "\n",
    "        keypoints = list(all_keypoints.values())\n",
    "        keypoints = torch.stack([torch.tensor(kp) for kp in keypoints]).float().to(device)\n",
    "#         print(\"processed keypoints\", keypoints)\n",
    "\n",
    "        # Now process these keypoints with GAT\n",
    "        print(\"vertices for graph node features\", keypoints)\n",
    "        node_features = self.get_node_features(keypoints)\n",
    "        edges, edge_features = self.get_edge_features(node_features)      \n",
    "                \n",
    "        data = Data(x=node_features, edge_index=edges, edge_attr=edge_features)\n",
    "#         print(\"graph data\", data)\n",
    "        \n",
    "#         print(\"node feature\", data.x)\n",
    "#         print(\"edges\", data.edge_index)\n",
    "#         print(\"edge feature\", data.edge_attr)\n",
    "        \n",
    "        vertices_pred, edge_pred = self.gcn(data)\n",
    "        return vertices_pred, edge_pred, edge_features \n",
    "    \n",
    "    def process_image(self, img):\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        # Temporarily set the keypoint model to evaluation mode\n",
    "        keypoint_model_training = self.keypoint_model.training  # Save the current mode\n",
    "        self.keypoint_model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.keypoint_model(img)\n",
    "        # Set the keypoint model back to its previous mode\n",
    "        self.keypoint_model.train(keypoint_model_training)\n",
    "        img = (img[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        labeled_keypoints = self.process_model_output(output)\n",
    "        \n",
    "        return labeled_keypoints        \n",
    "\n",
    "    def forward(self, imgs):\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(imgs.shape[0]):\n",
    "            pred_keypoints = self.process_image(imgs[i])\n",
    "            outputs.append((pred_keypoints))  # Each output now contains both predicted and original keypoints\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def loss_function(self, vertices_pred, vertices_orig, pred_edge_features, orig_edge_features):\n",
    "        # Assume both tensors are of the shape [num_nodes, num_node_features] and the first two features are x and y.\n",
    "#         pos_loss = functional.mse_loss(pred[:, :3], orig[:, :3])  # Loss based on position of keypoints\n",
    "        # Compute differences\n",
    "        diff = (vertices_pred[:, :3] - vertices_orig[:, :3]).abs()\n",
    "\n",
    "        # Compute Huber loss\n",
    "        huber_loss = torch.where(diff < self.delta, 0.5 * diff**2, self.delta * (diff - 0.5 * self.delta))\n",
    "        pos_loss = huber_loss.mean()\n",
    "        # Compute loss for edge features\n",
    "        edge_loss = func.mse_loss(pred_edge_features, orig_edge_features)\n",
    "\n",
    "        vis_loss = func.cross_entropy(vertices_pred[:, 3], vertices_orig[:, 3])  # Loss based on visibility of keypoints\n",
    "\n",
    "        return pos_loss + vis_loss + edge_loss\n",
    "#         return pos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd88827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 1  # Define your number of epochs\n",
    "batch_size = 1\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "model = KeypointPipeline(weights_path, num_vertices=6)  # Initialize the model with the provided weights and vertices\n",
    "model = model.to(device)  # Ensure the model is on the right device\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)  # Define the optimizer\n",
    "\n",
    "v=2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()  # Ensure the model is in training mode    \n",
    "\n",
    "    # Training loop\n",
    "    for i, batch in enumerate(data_loader_train):\n",
    "        img_tuple, target_dict_tuple, img_files = batch\n",
    "        \n",
    "        imgs = [img.to(device) for img in img_tuple]  # Create list of images\n",
    "        \n",
    "        # Process each image individually\n",
    "        total_train_loss = 0\n",
    "        for i in range(len(imgs)):\n",
    "            img = imgs[i].unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "            \n",
    "            # Prepare ground truth vertices for the image\n",
    "            keypoints = target_dict_tuple[i]['keypoints'].to(device)\n",
    "            visibility = torch.ones((keypoints.shape[0], keypoints.shape[1], 1)).to(device)\n",
    "            vertices_gt = torch.cat((keypoints, visibility), dim=2).unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "            \n",
    "            vertices_gt = vertices_gt.squeeze()\n",
    "            \n",
    "            print(\"ground truth keypoints\", vertices_gt.shape)\n",
    "                        \n",
    "            optimizer.zero_grad()  # Clear the gradients\n",
    "\n",
    "            outputs = model(img)  # Forward pass\n",
    "            \n",
    "            vertices_pred, edge_pred, edge_features = outputs[0]\n",
    "\n",
    "            loss = model.loss_function(vertices_pred, vertices_gt, edge_pred, edge_features)  # Compute the loss\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "\n",
    "            optimizer.step()  # Update the model parameters\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    eta = epoch_time * (num_epochs - epoch - 1)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, ETA: {eta} seconds')\n",
    "    \n",
    "\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    \n",
    "\n",
    "    # Validation loop\n",
    "    with torch.no_grad():  # No need to track gradients in validation\n",
    "        for i, batch in enumerate(data_loader_val):\n",
    "            img_tuple, target_dict_tuple, img_files = batch\n",
    "            imgs = [img.to(device) for img in img_tuple]  # Create list of images          \n",
    "\n",
    "            \n",
    "            total_val_loss = 0\n",
    "            for i in range(len(imgs)):\n",
    "                img = imgs[i].unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "                # Prepare ground truth vertices for the image\n",
    "                keypoints = target_dict_tuple[i]['keypoints'].to(device)\n",
    "                visibility = torch.ones((keypoints.shape[0], keypoints.shape[1], 1)).to(device)\n",
    "                vertices_gt = torch.cat((keypoints, visibility), dim=2).unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "\n",
    "                vertices_gt = vertices_gt.squeeze()\n",
    "                \n",
    "                outputs = model(img)  # Forward pass\n",
    "            \n",
    "                vertices_pred, edge_pred, edge_features = outputs[0]\n",
    "\n",
    "                loss = model.loss_function(vertices_pred, vertices_gt, edge_pred, edge_features)  # Compute the loss\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        print(f'Validation Loss after epoch {epoch + 1}: {total_val_loss/len(data_loader_val)}')\n",
    "    \n",
    "model_save_path = f\"/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_gcn_b{batch_size}_e{num_epochs}_v{v}.pth\"\n",
    "torch.save(model, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf649c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save(img, vertices, filename):\n",
    "    print(\"type of image befor conversion\",type(img))    \n",
    "    print(\"type of vertices before conversion\", type(vertices))\n",
    "    print(img.shape)\n",
    "    img = (img.permute(1,2,0).cpu().numpy() * 255).astype(np.uint8)\n",
    "#     img = (img * 255).astype(np.uint8)  # Convert back from [0, 1] range to [0, 255]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    vertices = vertices.cpu().numpy()\n",
    "\n",
    "    print(f\"Image shape before saving: {img.shape}\")  # print the image shape\n",
    "    print(\"type of vertices\", type(vertices))\n",
    "#     print(\"entered vertices\", vertices)\n",
    "#     print(\"entered image\", img)\n",
    "\n",
    "    # Convert grayscale to BGR if necessary\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "    for i in range(vertices.shape[0]):\n",
    "        img = cv2.circle(img, (int(vertices[i, 0]), int(vertices[i, 1])), radius=2, color=(0, 0, 255), thickness=-1)\n",
    "        \n",
    "    result = cv2.imwrite(filename, img)\n",
    "    print(f\"Image saved at {filename}: {result}\")  # print if save was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed653c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "predicted_keypoints = []  # List to store predicted keypoints\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients in testing\n",
    "    for idx, batch in enumerate(data_loader_test):\n",
    "        img_tuple, target_dict_tuple, img_files = batch\n",
    "        imgs = [img.to(device) for img in img_tuple]  # Create list of images\n",
    "        total_test_loss = 0\n",
    "        for i in range(len(imgs)):\n",
    "            img = imgs[i].unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "            # Prepare ground truth vertices for the image\n",
    "            keypoints = target_dict_tuple[i]['keypoints'].to(device)\n",
    "            visibility = torch.ones((keypoints.shape[0], keypoints.shape[1], 1)).to(device)\n",
    "            vertices_gt = torch.cat((keypoints, visibility), dim=2).unsqueeze(0)  # Unsqueeze to add batch dimension\n",
    "            vertices_gt = vertices_gt.squeeze()\n",
    "            outputs = model(img)  # Forward pass\n",
    "            \n",
    "            vertices_pred, edge_pred, edge_features = outputs[0]\n",
    "\n",
    "            loss = model.loss_function(vertices_pred, vertices_gt, edge_pred, edge_features)  # Compute the loss\n",
    "            \n",
    "            # Visualize and save the prediction\n",
    "            filename = f'/home/jc-merlab/Pictures/Data/occ_vis_data/image_{idx}_{i}.jpg'\n",
    "            visualize_and_save(img.squeeze(), vertices_pred, filename)\n",
    "            print(f\"Image saved at {filename}\")  # Print statement to confirm image save\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "            predicted_keypoints.extend(outputs)\n",
    "\n",
    "print(f'Test Loss: {total_test_loss/len(data_loader_test)}')\n",
    "\n",
    "# Print predicted keypoints\n",
    "for i, keypoints in enumerate(predicted_keypoints):\n",
    "    print(f'Predicted keypoints for test image {i+1}:')\n",
    "    print(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabedbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f87c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

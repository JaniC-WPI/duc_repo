{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b59f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c5b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "#         A.Resize(640, 480)  # Resize all images to be 640x480\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1938c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96385476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNPipeline(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNPipeline, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "class KeypointRNN(nn.Module):\n",
    "    def __init__(self, weights_path, rnn_hidden_size, num_vertices):\n",
    "        super().__init__()\n",
    "\n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.num_vertices = num_vertices\n",
    "        self.rnn = RNNPipeline(4, rnn_hidden_size, 4)  # Assuming input and output sizes are both 4 (x, y, c, label)\n",
    "\n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \n",
    "                                            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "        \n",
    "        confidence = output[0]['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0,0:2])) + [confidence[idx]] + [labels[idx]])\n",
    "\n",
    "        # Create a dictionary where the key is the label and the value is the keypoint\n",
    "        label_to_keypoint = {}\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0,0:2])) + [confidence[idx]] + [labels[idx]])\n",
    "\n",
    "        # Use a dictionary to keep track of all possible keypoints and their locations.\n",
    "        # Initialize with placeholders for missing keypoints.\n",
    "        all_keypoints = {i: [-1, -1, 0, i] for i in range(1, self.num_vertices+1)}\n",
    "\n",
    "        for keypoint in keypoints:\n",
    "            label = keypoint[-1]\n",
    "            if label not in label_to_keypoint or label_to_keypoint[label][-2] < keypoint[-2]:\n",
    "                all_keypoints[label] = keypoint\n",
    "\n",
    "        # Convert the dictionary values back into a list\n",
    "        keypoints = list(all_keypoints.values())\n",
    "        return keypoints\n",
    "\n",
    "    def process_image(self, img):\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        # Temporarily set the keypoint model to evaluation mode\n",
    "        keypoint_model_training = self.keypoint_model.training  # Save the current mode\n",
    "        self.keypoint_model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.keypoint_model(img)\n",
    "        # Set the keypoint model back to its previous mode\n",
    "        self.keypoint_model.train(keypoint_model_training)\n",
    "        img = (img[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        labeled_keypoints = self.process_model_output(output)\n",
    "\n",
    "        return labeled_keypoints\n",
    "\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        outputs = []\n",
    "        hidden = self.rnn.initHidden()\n",
    "\n",
    "        for i in range(imgs.shape[0]):\n",
    "            labeled_keypoints = self.process_image(imgs[i])\n",
    "            keypoints = torch.stack([torch.tensor(kp) for kp in labeled_keypoints]).float().to(device)\n",
    "\n",
    "            rnn_input = keypoints.view(1, -1)  # Flatten keypoints into a 1D tensor\n",
    "            rnn_output, hidden = self.rnn(rnn_input, hidden)\n",
    "\n",
    "            # Reshape RNN output back into the format of keypoints\n",
    "            updated_keypoints = rnn_output.view(-1, 4)  # Assuming keypoints have 4 values (x, y, c, label)\n",
    "            outputs.append(updated_keypoints)\n",
    "        print(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41330b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, vertices_pred, vertices_gt):\n",
    "        vertices_pred = vertices_pred[:, :3]\n",
    "        vertices_gt = vertices_gt.squeeze()[:, :3]\n",
    "        diff = (vertices_gt - vertices_pred).abs()\n",
    "        loss = torch.where(diff < self.delta, 0.5 * diff.pow(2), self.delta * (diff - 0.5 * self.delta))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeypointRNN(weights_path, rnn_hidden_size=128, num_vertices=6)\n",
    "model.to(device)\n",
    "loss_fn = HuberLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30  # Define your number of epochs\n",
    "batch_size = 4\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "v = 1\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    # For each batch in your training data\n",
    "    for batch in data_loader_train:\n",
    "        img_tuple, target_dict_tuple, img_files = batch\n",
    "        print(\"Processing images:\", img_files)\n",
    "        # Plot first image of the batch\n",
    "        img_to_show = img_tuple[0].numpy().transpose((1, 2, 0))\n",
    "        plt.imshow(img_to_show)\n",
    "        plt.show()\n",
    "        imgs = torch.stack(img_tuple).to(device)  # Stack images into a single tensor\n",
    "        vertices_gt = [target['keypoints'].to(device) for target in target_dict_tuple]  # List of ground truth keypoints for each image\n",
    "        print(\"ground_truth_kp\", vertices_gt)\n",
    "    \n",
    "        # Forward pass\n",
    "        output = model(imgs)  # Output will be a list of tuples, each containing vertices_pred, edges_prob, edges_gt for an image\n",
    "        \n",
    "\n",
    "\n",
    "        # Compute the losses\n",
    "        loss = 0\n",
    "        for i in range(len(output)):\n",
    "            vertices_pred, = output[i]\n",
    "            vertices_gt_i = vertices_gt[i]\n",
    "            loss_fn = criterion(vertices_pred, vertices_gt_i)\n",
    "#             ce_loss = edge_loss(edges_prob, edges_gt)\n",
    "\n",
    "            # Combined loss\n",
    "            loss += huber_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)   \n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for each epoch\n",
    "    end_time = time.time()  # Stop the timer\n",
    "    epoch_time = end_time - start_time\n",
    "    eta = epoch_time * (num_epochs - epoch - 1)  # Calculate ETA (in seconds)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, ETA: {eta} seconds')\n",
    "\n",
    "model_save_path = f\"/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_occ_rnn_b{batch_size}_e{num_epochs}_v{v}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfedd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = KeypointPipeline(weights_path, num_vertices=6)\n",
    "model = model.to(device)\n",
    "\n",
    "# # Define the loss\n",
    "# loss_fn = nn.MSELoss()  # Define the loss function\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30  # Define your number of epochs\n",
    "batch_size = 4\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "v = 1\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    # For each batch in your training data\n",
    "    for batch in data_loader_train:\n",
    "        img_tuple, target_dict_tuple, img_files = batch\n",
    "        print(\"Processing images:\", img_files)\n",
    "        # Plot first image of the batch\n",
    "        img_to_show = img_tuple[0].numpy().transpose((1, 2, 0))\n",
    "        plt.imshow(img_to_show)\n",
    "        plt.show()\n",
    "        imgs = torch.stack(img_tuple).to(device)  # Stack images into a single tensor\n",
    "        vertices_gt = [target['keypoints'].to(device) for target in target_dict_tuple]  # List of ground truth keypoints for each image\n",
    "        print(\"ground_truth_kp\", vertices_gt)\n",
    "    \n",
    "        # Forward pass\n",
    "        output = model(imgs)  # Output will be a list of tuples, each containing vertices_pred, edges_prob, edges_gt for an image\n",
    "        \n",
    "\n",
    "\n",
    "        # Compute the losses\n",
    "        loss = 0\n",
    "        for i in range(len(output)):\n",
    "            vertices_pred, = output[i]\n",
    "            vertices_gt_i = vertices_gt[i]\n",
    "            loss_fn = criterion(vertices_pred, vertices_gt_i)\n",
    "#             ce_loss = edge_loss(edges_prob, edges_gt)\n",
    "\n",
    "            # Combined loss\n",
    "            loss += huber_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)   \n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for each epoch\n",
    "    end_time = time.time()  # Stop the timer\n",
    "    epoch_time = end_time - start_time\n",
    "    eta = epoch_time * (num_epochs - epoch - 1)  # Calculate ETA (in seconds)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, ETA: {eta} seconds')\n",
    "\n",
    "model_save_path = f\"/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_occ_b{batch_size}_e{num_epochs}_v{v}.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedf1768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16908615680\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b355d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "237a5cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2189cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaae8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9395a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1df0e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, vertices_dim=3, hidden_dim=128, num_vertices=6):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        # Define your layers here, e.g., \n",
    "        self.f_enc = nn.Linear(vertices_dim, hidden_dim)  # Assuming 3 features per vertex as per the paper\n",
    "        self.f_e1 = nn.Linear(hidden_dim * 2, hidden_dim)  # Concatenate two vertices features\n",
    "        self.f_v = nn.Linear(hidden_dim, hidden_dim)  # Hidden layer for vertices\n",
    "        self.f_e2 = nn.Linear(hidden_dim * 2, (vertices_dim - 1)) # Concatenate updated vertices features\n",
    "        \n",
    "#         self.relu = nn.ReLU() \n",
    "    \n",
    "    def generate_edges(self, vertices):\n",
    "        # Convert tensor to list for easier manipulation\n",
    "        vertices_list = vertices.tolist()\n",
    "\n",
    "        # Create a graph\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Enumerate over vertices, get index and vertex\n",
    "        for idx, vertex in enumerate(vertices_list):\n",
    "            G.add_node(idx, x=vertex[0], y=vertex[1], t=vertex[2])  # Adding node with attributes\n",
    "\n",
    "            # Add edge to the next node if it exists\n",
    "            if idx < len(vertices_list) - 1:\n",
    "                G.add_edge(idx, idx + 1)\n",
    "\n",
    "        # Create a tensor from the graph's edges\n",
    "        edges = torch.tensor(list(G.edges), dtype=torch.long).to(vertices.device)  # Ensure the edges tensor is on the same device as vertices\n",
    "        return edges\n",
    "    \n",
    "\n",
    "    def forward(self, vertices):\n",
    "        print(vertices)\n",
    "        h1 = self.f_enc(vertices)\n",
    "        \n",
    "        # using predefined edges from predicted keypoints\n",
    "        edges = self.generate_edges(vertices)\n",
    "        print(\"offending vertices\", vertices)\n",
    "        print(edges)\n",
    "        print(\"Offending h1\", h1.shape)\n",
    "        if h1.shape[0] < num_vertices:  # Adjusting for instances where h1's shape is (5,128) instead of (6,128)\n",
    "            h1_source = h1[edges[:-1, 0]]  # Only take until the second to last element\n",
    "            h1_target = h1[edges[:-1, 1]]\n",
    "            h_e1 = self.f_e1(torch.cat((h1_source, h1_target), dim=1))  # Concatenate along the last dimension\n",
    "            h2 = self.f_v(h1)\n",
    "            print(\"shape h2\", h2.shape)\n",
    "            h2_prob = func.softmax(h2)  # vertices probability\n",
    "#             h2_source = h2[edges[:-1, 0]]\n",
    "#             h2_target = h2[edges[:-1, 1]]\n",
    "#             h_e2 = self.f_e2(torch.cat((h2_source, h2_target), dim=1))  # v->e\n",
    "#             print(\"Edge_shape\", h_e2.shape)\n",
    "            return h2_prob, h_e1, edges\n",
    "        else:\n",
    "            h1_source = h1[edges[:, 0]]\n",
    "            h1_target = h1[edges[:, 1]]\n",
    "            h_e1 = self.f_e1(torch.cat((h1_source, h1_target), dim=1))  # Concatenate along the last dimension\n",
    "            h2 = self.f_v(h_e1)\n",
    "            h2_prob = func.softmax(h2)  # vertices probability\n",
    "#             print(\"shape h2\", h2.shape)\n",
    "#             if h2.shape[0] < num_vertices:\n",
    "#                 h2_source = h2[edges[:-1, 0]]\n",
    "#                 h2_target = h2[edges[:-1, 1]]\n",
    "#             else:\n",
    "#                 h2_source = h2[edges[:, 0]]\n",
    "#                 h2_target = h2[edges[:, 1]]            \n",
    "#             h_e2 = self.f_e2(torch.cat((h2_source, h2_target), dim=1))  # v->e\n",
    "#             print(\"Edge_shape\", h_e2.shape)\n",
    "#             print(\"H2 type\", h2_prob)\n",
    "            return h2_prob, h_e1, edges\n",
    "\n",
    "class GNNDecoder(nn.Module):\n",
    "    def __init__(self, vertices_dim=3, hidden_dim=128, num_vertices=6):\n",
    "        super(GNNDecoder, self).__init__()\n",
    "\n",
    "        self.f_ep = nn.Linear(hidden_dim*2, hidden_dim)  # Transformation for edge features\n",
    "        self.f_v = nn.Linear(hidden_dim, hidden_dim)  # For updating vertices\n",
    "        self.f_v_out = nn.Linear(hidden_dim, vertices_dim)\n",
    "\n",
    "#         self.mu = nn.Parameter(torch.zeros(vertices_dim))  # Mean location predictor parameter\n",
    "#         self.rho = nn.Parameter(torch.Tensor(vertices_dim))\n",
    "#         nn.init.normal_(self.rho, mean=0, std=0.01) # Standard deviation parameter (rho^2 will be variance)\n",
    "        \n",
    "        \n",
    "#     def generate_edges(self, num_vertices):\n",
    "#         edges = torch.tensor([[i, i+1] for i in range(num_vertices-1)], dtype=torch.long)\n",
    "#         return edges\n",
    "\n",
    "    def forward(self, vertices, h_e1, edges):\n",
    "        print(vertices.shape)\n",
    "        if vertices.shape[0] < num_vertices:\n",
    "            vertices_source = vertices[edges[:-1, 0]]\n",
    "            vertices_target = vertices[edges[:-1, 1]]\n",
    "        else:            \n",
    "            vertices_source = vertices[edges[:, 0]]\n",
    "            vertices_target = vertices[edges[:, 1]]\n",
    "            \n",
    "        h_ij = self.f_ep(torch.cat((vertices_source, vertices_target, h_e1), dim=1))  # Assuming edge features are also input\n",
    "        \n",
    "        # e -> v: \\mu_{j}^{g}=\\mathcal{V}_{j}+f_{v}\\left(\\sum_{i \\neq j} h_{(i, j)}\\right)\n",
    "        # We use mean pooling here to sum over all h_{(i, j)} for each j\n",
    "        pooled = torch.mean(h_ij, dim=0, keepdim=True)\n",
    "        mu_jg = vertices + self.f_v(pooled)\n",
    "\n",
    "        # Return mean predicted locations\n",
    "        return self.f_v_out(mu_jg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90ea42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrifocalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, vertices_pred, vertices_gt):\n",
    "        loss = (vertices_gt - torch.tensor(vertices_pred)).pow(2).mean()  # Changed from sum() to mean()\n",
    "        return loss\n",
    "    \n",
    "# def cross_entropy_loss_func(edges_prob, edges_gt):\n",
    "#     edges_gt_expanded = edges_gt.unsqueeze(-1).float()\n",
    "#     loss_func = nn.BCEWithLogitsLoss()\n",
    "#     loss = loss_func(edges_prob, edges_gt_expanded)\n",
    "#     return loss\n",
    "\n",
    "def edge_loss(edges_prob, edges_gt):\n",
    "    # Expand edges_gt to match the shape of edges_prob\n",
    "    print(edges_prob.shape)\n",
    "    edges_gt_expanded = torch.zeros(edges_prob.shape, dtype=torch.float32)\n",
    "    \n",
    "    for i in range(edges_gt.shape[0]):\n",
    "        u, v = edges_gt[i]\n",
    "        print(\"u:\", u)\n",
    "        print(\"v:\", v)\n",
    "        print(\"edges_gt_expanded.shape:\", edges_gt_expanded.shape)\n",
    "        if i < edges_gt_expanded.shape[0]:\n",
    "            if u < edges_gt_expanded.shape[1]:\n",
    "                edges_gt_expanded[i, u] = 1\n",
    "            if v < edges_gt_expanded.shape[1]:\n",
    "                edges_gt_expanded[i, v] = 1\n",
    "        print('new u', u)\n",
    "        print('new v', v)\n",
    "    # Compute the cross-entropy loss\n",
    "    loss = -torch.sum(edges_gt_expanded.to(device) * torch.log(torch.clamp(edges_prob, min=1e-7)))\n",
    "                      \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a63300d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.gnn_encoder = GNNEncoder()\n",
    "        self.gnn_decoder = GNNDecoder()\n",
    "\n",
    "    def forward(self, img):\n",
    "#         print(\"image in keypoints eval phase\", img.shape)\n",
    "#         img = F.to_tensor(img).to(device)\n",
    "        img.unsqueeze_(0)\n",
    "        img = list(img)\n",
    "        with torch.no_grad():\n",
    "            self.keypoint_model.to(device)\n",
    "            self.keypoint_model.eval()\n",
    "            output = self.keypoint_model(img)\n",
    "            \n",
    "        img = (img[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \\\n",
    "            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "        \n",
    "        keypoints = []\n",
    "        key_points = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append(list(map(int, kps[0,0:2])))\n",
    "            key_points.append([list(map(int, kp[:2])) for kp in kps])\n",
    "            \n",
    "        print(\"keypoints\", keypoints)\n",
    "\n",
    "        labels = []\n",
    "        for label in output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            labels.append(label)\n",
    "#             labels.append('j' + str(int(label)))\n",
    "        \n",
    "        print(\"labels\", labels)\n",
    "        \n",
    "        keypoints_ = [x for _,x in sorted(zip(labels,keypoints))]\n",
    "#         kp_label = [list(x) + [y] for (x, y) in sorted(zip(keypoints, labels))]\n",
    "\n",
    "        # Create a dictionary where the key is the label and the value is the keypoint\n",
    "        label_keypoint_dict = {lbl: kp for kp, lbl in zip(keypoints, labels)}\n",
    "\n",
    "        # Convert the dictionary back to a list and sort it by the label keys\n",
    "        labeled_keypoints = [value + [key] for key, value in sorted(label_keypoint_dict.items())] #,key=lambda item: int(item[0][1:]))]\n",
    "        \n",
    "        print(\"keypoints_\", keypoints_)\n",
    "        print(\"kp_label\", labeled_keypoints)\n",
    "                \n",
    "        keypoints = torch.stack([torch.tensor(kp) for kp in labeled_keypoints]).float().to(device)\n",
    "        print(\"Original Keypoints\", keypoints)\n",
    "        enc_v, enc_e, edges = self.gnn_encoder(keypoints)\n",
    "        \n",
    "        vertices = self.gnn_decoder(enc_v, enc_e, edges)\n",
    "        \n",
    "        return vertices, enc_v, enc_e, edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "945ce7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2662 files [00:00, 19045.54 files/s]\n",
      "Copying files: 2662 files [00:00, 19184.19 files/s]\n",
      "Copying files: 2662 files [00:00, 18585.56 files/s]\n",
      "/tmp/ipykernel_57059/1743336362.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  h2_prob = func.softmax(h2)  # vertices probability\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "keypoints [[111, 258], [196, 258], [203, 178], [223, 180], [295, 112], [309, 127]]\n",
      "labels [1, 2, 3, 4, 5, 6]\n",
      "keypoints_ [[111, 258], [196, 258], [203, 178], [223, 180], [295, 112], [309, 127]]\n",
      "kp_label [[111, 258, 1], [196, 258, 2], [203, 178, 3], [223, 180, 4], [295, 112, 5], [309, 127, 6]]\n",
      "Original Keypoints tensor([[111., 258.,   1.],\n",
      "        [196., 258.,   2.],\n",
      "        [203., 178.,   3.],\n",
      "        [223., 180.,   4.],\n",
      "        [295., 112.,   5.],\n",
      "        [309., 127.,   6.]], device='cuda:0')\n",
      "tensor([[111., 258.,   1.],\n",
      "        [196., 258.,   2.],\n",
      "        [203., 178.,   3.],\n",
      "        [223., 180.,   4.],\n",
      "        [295., 112.,   5.],\n",
      "        [309., 127.,   6.]], device='cuda:0')\n",
      "tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]], device='cuda:0')\n",
      "torch.Size([5, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 4 but got size 5 for tensor number 2 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m vertices_gt[:, :, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, num_vertices\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m vertices_pred, vertices_prob, edges_prob, edges_gt \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Compute the losses\u001b[39;00m\n\u001b[1;32m     48\u001b[0m trifocal_loss \u001b[38;5;241m=\u001b[39m criterion(vertices_pred, vertices_gt)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mKeypointPipeline.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Keypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m, keypoints)\n\u001b[1;32m     53\u001b[0m enc_v, enc_e, edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn_encoder(keypoints)\n\u001b[0;32m---> 55\u001b[0m vertices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vertices, enc_v, enc_e, edges\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mGNNDecoder.forward\u001b[0;34m(self, vertices, h_e1, edges)\u001b[0m\n\u001b[1;32m    100\u001b[0m     vertices_source \u001b[38;5;241m=\u001b[39m vertices[edges[:, \u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    101\u001b[0m     vertices_target \u001b[38;5;241m=\u001b[39m vertices[edges[:, \u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m--> 103\u001b[0m h_ij \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_ep(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertices_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_e1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Assuming edge features are also input\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# e -> v: \\mu_{j}^{g}=\\mathcal{V}_{j}+f_{v}\\left(\\sum_{i \\neq j} h_{(i, j)}\\right)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# We use mean pooling here to sum over all h_{(i, j)} for each j\u001b[39;00m\n\u001b[1;32m    107\u001b[0m pooled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(h_ij, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 4 but got size 5 for tensor number 2 in the list."
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = KeypointPipeline()\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss\n",
    "criterion = TrifocalLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Load the trained model\n",
    "model.keypoint_model = torch.load(weights_path).to(device)\n",
    "\n",
    "num_epochs = 10  # Define your number of epochs\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "model.train()\n",
    "# Epoch loop\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in your training data\n",
    "    for batch in data_loader_train:\n",
    "        img_tuple, target_dict_tuple = batch\n",
    "        img = img_tuple[0]\n",
    "#         print(img.shape)\n",
    "        target = target_dict_tuple[0]\n",
    "        img = img.to(device)\n",
    "        vertices_gt = target['keypoints'].to(device)\n",
    "        num_vertices = vertices_gt.shape[0]\n",
    "        print(num_vertices)\n",
    "        vertices_gt[:, :, 2] = torch.arange(1, num_vertices+1).unsqueeze(1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        vertices_pred, vertices_prob, edges_prob, edges_gt = model(img)\n",
    "        \n",
    "        # Compute the losses\n",
    "        trifocal_loss = criterion(vertices_pred, vertices_gt)\n",
    "        ce_loss = edge_loss(edges_prob, edges_gt)\n",
    "\n",
    "        # Combined loss\n",
    "        loss = trifocal_loss + ce_loss\n",
    "#         loss = trifocal_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for each epoch\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4397900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740f228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6214a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40b993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58949932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

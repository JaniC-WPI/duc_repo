{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "c0fe9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, DataLoader, Data\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from define_path import Def_Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import splitfolders\n",
    "import shutil\n",
    "import albumentations as A # Library for augmentations\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt \n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "9fa9d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jc-merlab/Pictures/Data/2023-06-27/\n"
     ]
    }
   ],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "7b3ea856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotArmDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, pre_transform=None):\n",
    "        super(RobotArmDataset, self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root_dir, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root_dir, \"annotations\")))\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.imgs_files)\n",
    "\n",
    "    def get(self, idx):\n",
    "        image_file = os.path.join(self.root_dir, \"images\", self.imgs_files[idx])\n",
    "        json_file = os.path.join(self.root_dir, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        with open(json_file, 'r') as f:\n",
    "            data_json = json.load(f)\n",
    "        \n",
    "        keypoints = data_json['keypoints']\n",
    "        keypoints = [kp[0] for kp in keypoints]  # Extract keypoints from each list\n",
    "\n",
    "        keypoints = np.array(keypoints).reshape(-1, 3)  # Convert to numpy array and reshape\n",
    "\n",
    "        image = cv2.imread(image_file)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "\n",
    "#         data = {\"image\": image, \"keypoints\": keypoints.tolist()}\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(data)\n",
    "\n",
    "        keypoints = torch.tensor(keypoints, dtype=torch.float).view(-1, 3)  # Convert back to tensor and reshape\n",
    "        edge_index = torch.tensor([[i, i+1] for i in range(len(keypoints)-1)], dtype=torch.long).t().contiguous()\n",
    "        data = Data(x=keypoints, edge_index=edge_index, y=keypoints.clone())\n",
    "    \n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float()  # Convert to PyTorch tensor and rearrange dimensions to (C, H, W)\n",
    "        return image, data\n",
    "#         return transformed\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         image_file = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "#         json_file = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "    \n",
    "#         with open(json_file, 'r') as f:\n",
    "#             data_json = json.load(f)\n",
    "#         keypoints = data_json['keypoints']\n",
    "#         keypoints = np.array(keypoints).reshape(-1, 3)  # Convert to numpy array and reshape\n",
    "\n",
    "#         image = cv2.imread(image_file)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "\n",
    "#         if self.transform:\n",
    "#             # Transform expects keypoints in format [x, y, visibility]\n",
    "#             transformed = self.transform(image=image, keypoints=keypoints)\n",
    "#             image = transformed[\"image\"]\n",
    "#             keypoints = transformed[\"keypoints\"]\n",
    "\n",
    "#         keypoints = torch.tensor(keypoints, dtype=torch.float).view(-1, 3)  # Convert back to tensor and reshape\n",
    "#         edge_index = torch.tensor([[i, i+1] for i in range(len(keypoints)-1)], dtype=torch.long).t().contiguous()\n",
    "#         data = Data(x=keypoints, edge_index=edge_index)\n",
    "\n",
    "#         image = torch.from_numpy(image).permute(2, 0, 1).float()  # Convert to PyTorch tensor and rearrange dimensions to (C, H, W)\n",
    "#         return image, data\n",
    "    \n",
    "#     def len(self):\n",
    "#         return len(self.imgs_files)  # Return the number of data points\n",
    "\n",
    "#     def get(self, idx):\n",
    "#         return self.__getitem__(idx)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "c5f815c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotateKeyPoints(object):\n",
    "    def __init__(self, angle):\n",
    "        self.angle = angle\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, keypoints = sample['image'], sample['keypoints']\n",
    "\n",
    "        # rotation of image\n",
    "        image = image.rotate(self.angle)\n",
    "\n",
    "        # rotation of keypoints\n",
    "        rotation_matrix = torch.tensor([\n",
    "            [torch.cos(self.angle), -torch.sin(self.angle)],\n",
    "            [torch.sin(self.angle),  torch.cos(self.angle)]\n",
    "        ])\n",
    "\n",
    "        keypoints[:, :2] = torch.mm(keypoints[:, :2], rotation_matrix)\n",
    "\n",
    "        return {'image': image, 'keypoints': keypoints}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "937a236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return transforms.Compose([\n",
    "    transforms.Lambda(lambda x: Image.fromarray(x)),\n",
    "    transforms.Resize((640, 480)), \n",
    "    transforms.ToTensor(),\n",
    "    RotateKeyPoints(90),  # Rotate image and keypoints by 90 degrees\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "f8e725dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    print(type(output))\n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "6821c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(image, data, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    # Remove the channel dimension and convert the tensor back to numpy array\n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    ax.imshow(image_np.astype(int))\n",
    "\n",
    "    keypoints = data.x[:, :2]  # Assuming the first two dimensions are x and y\n",
    "    # Create a graph from the edge_index\n",
    "    G = nx.Graph()\n",
    "    for i in range(keypoints.shape[0]):\n",
    "        G.add_node(i, pos=(keypoints[i][0].item(), keypoints[i][1].item()))\n",
    "    for edge in data.edge_index.t():\n",
    "        G.add_edge(edge[0].item(), edge[1].item())\n",
    "    \n",
    "    # Draw the graph\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    nx.draw(G, pos, node_color='r', node_size=50, ax=ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "f8c8b77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 15462 files [00:00, 19137.72 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jc-merlab/Pictures/Data/split_folder_output-2023-06-27/train\n",
      "5411\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'transformed' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [335]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m RobotArmDataset(KEYPOINTS_FOLDER_TRAIN, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,pre_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[0;32m----> 5\u001b[0m image, data \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;66;03m# Get the first image and its associated data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m visualize_data(image, data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch_geometric/data/dataset.py:258\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mpresent).\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03mIn case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03mtuple, or a :obj:`torch.Tensor` or :obj:`np.ndarray` of type long or\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03mbool, will return a subset of the dataset at the specified indices.\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 258\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Input \u001b[0;32mIn [329]\u001b[0m, in \u001b[0;36mRobotArmDataset.get\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m             transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#         keypoints = torch.tensor(keypoints, dtype=torch.float).view(-1, 3)  # Convert back to tensor and reshape\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#         edge_index = torch.tensor([[i, i+1] for i in range(len(keypoints)-1)], dtype=torch.long).t().contiguous()\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#         data = Data(x=keypoints, edge_index=edge_index, y=keypoints.clone())\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#         image = torch.from_numpy(image).permute(2, 0, 1).float()  # Convert to PyTorch tensor and rearrange dimensions to (C, H, W)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#         return image, data\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransformed\u001b[49m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'transformed' referenced before assignment"
     ]
    }
   ],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" \n",
    "print(KEYPOINTS_FOLDER_TRAIN)\n",
    "dataset = RobotArmDataset(KEYPOINTS_FOLDER_TRAIN, transform=None,pre_transform=None)\n",
    "print(len(dataset))\n",
    "image, data = dataset[0]\n",
    "print(image.shape)# Get the first image and its associated data\n",
    "visualize_data(image, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ee939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotArmGCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobotArmGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(3, 16)\n",
    "        self.conv2 = GCNConv(16, 3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f50560c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 15462 files [00:00, 17539.65 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 15462 files [00:00, 15760.65 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 15462 files [00:00, 17253.34 files/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [223]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust loss function as necessary\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:3281\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m   3278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3279\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[1;32m   3280\u001b[0m     )\n\u001b[0;32m-> 3281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3282\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()),\n\u001b[1;32m   3286\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3287\u001b[0m     )\n\u001b[1;32m   3288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = RobotArmDataset(KEYPOINTS_FOLDER_TRAIN, transform=None,pre_transform=None)\n",
    "dataset_val = RobotArmDataset(KEYPOINTS_FOLDER_VAL, transform=None)\n",
    "dataset_test = RobotArmDataset(KEYPOINTS_FOLDER_TEST, transform=None)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=32)\n",
    "\n",
    "model = RobotArmGCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):  # Adjust number of epochs as necessary\n",
    "    for batch in dataloader_train:\n",
    "        image, data = batch\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = torch.nn.functional.mse_loss(out, data.y)  # Adjust loss function as necessary\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3a5d6972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "# from albumentations import Resize, Compose\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load an image\n",
    "image_path = KEYPOINTS_FOLDER_TRAIN + '/images/000000.rgb.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# Define a transformation\n",
    "# Define a transformation\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224)\n",
    "])\n",
    "\n",
    "# Apply the transformation\n",
    "transformed = transform(image=image)\n",
    "transformed_image = transformed[\"image\"]\n",
    "\n",
    "# Display the transformed image\n",
    "plt.imshow(transformed_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e40e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

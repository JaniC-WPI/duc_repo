{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "from torchmetrics.image import TotalVariation\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "# import torchvision.transforms.functional as F\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import random\n",
    "from torchvision.models import resnet50\n",
    "from torch.nn import MSELoss\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "# from utils.inpainting_utils import *\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "PLOT = True\n",
    "imsize = -1\n",
    "dim_div_by = 64\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "f = r-a  # free inside reserved\n",
    "\n",
    "%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ed119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLabelDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, image_transform=None, label_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        self.image_transform = image_transform\n",
    "        self.label_transform = label_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        label_name = os.path.join(self.label_dir, self.image_files[idx])\n",
    "\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = Image.open(label_name).convert('RGB')\n",
    "\n",
    "        seed = np.random.randint(2147483647)  # make a seed with numpy generator \n",
    "        \n",
    "        if self.image_transform is not None:\n",
    "            random.seed(seed)  # Apply this seed to image transforms\n",
    "            torch.manual_seed(seed)\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        if self.label_transform is not None:\n",
    "            random.seed(seed)  # Apply this seed to label transforms\n",
    "            torch.manual_seed(seed)\n",
    "            label = self.label_transform(label)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d455ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def compute_mean_std_for_inpainting(dataset, batch_size=64, num_workers=8):\n",
    "#     '''Compute the mean and std value of the images in the inpainting dataset.'''\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "#     sum_ = torch.tensor([0.0, 0.0, 0.0])\n",
    "#     sum_squared = torch.tensor([0.0, 0.0, 0.0])\n",
    "#     total_pixels = 0\n",
    "    \n",
    "#     for inputs, labels in dataloader:\n",
    "#         inputs_labels_combined = torch.cat((inputs, labels), dim=0)  # Combine inputs and labels for processing\n",
    "#         total_pixels += inputs_labels_combined.numel() / inputs_labels_combined.size(1)  # Exclude the channel dimension\n",
    "        \n",
    "#         sum_ += inputs_labels_combined.sum([0, 2, 3])  # Sum over batch, height, and width, but not channels\n",
    "#         sum_squared += (inputs_labels_combined ** 2).sum([0, 2, 3])\n",
    "    \n",
    "#     mean = sum_ / total_pixels\n",
    "#     std = torch.sqrt((sum_squared / total_pixels) - (mean ** 2))\n",
    "    \n",
    "#     return mean, std\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "image_dir = '/home/jc-merlab/Pictures/panda_data/panda_vit_data/images/'\n",
    "label_dir = '/home/jc-merlab/Pictures/panda_data/panda_vit_data/labels'\n",
    "\n",
    "# dataset = ImageLabelDataset(image_dir=image_dir, label_dir=label_dir, \n",
    "#                             image_transform=transform, \n",
    "#                             label_transform=transform)\n",
    "\n",
    "# mean, std = compute_mean_std_for_inpainting(dataset)\n",
    "# print(mean, std)\n",
    "\n",
    "mean = torch.tensor([0.2367, 0.2567, 0.2429])\n",
    "std = torch.tensor([0.2261, 0.2213, 0.2405])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.1),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),\n",
    "    transforms.RandomEqualize(p=0.01),\n",
    "    transforms.RandomAutocontrast(p=0.01),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "label_transform = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = ImageLabelDataset(image_dir=image_dir, label_dir=label_dir, \n",
    "                            image_transform=image_transform, \n",
    "                            label_transform=label_transform)\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.9 * total_size)\n",
    "val_size = (total_size - train_size) // 2\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean, std):\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "    std = torch.tensor(std).view(-1, 1, 1)\n",
    "    tensor = tensor * std + mean\n",
    "    return tensor\n",
    "\n",
    "def show_image(img_tensor, mean, std, title=\"\", normalize=True, original_size=(480, 640)):\n",
    "    \"\"\"\n",
    "    Display a tensor as an image.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        denorm_img = denormalize(img_tensor, mean, std)\n",
    "    \n",
    "    img_tensor = F.interpolate(denorm_img.unsqueeze(0), size=original_size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "    # Convert tensor to numpy for visualization\n",
    "    img = img_tensor.detach().cpu().numpy().transpose((1, 2, 0))  # Adjust for channel ordering\n",
    "    img = np.clip(img, 0, 1)  # Ensure the image's pixel values are valid after denormalization\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "def visualize_dataset(dataset_loader):\n",
    "    \"\"\"\n",
    "    Visualizes a batch of images and labels from the dataset loader.\n",
    "    \"\"\"\n",
    "    # Fetch a batch of images and labels\n",
    "    images, labels = next(iter(dataset_loader))\n",
    "    \n",
    "    batch_size = len(images)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4 * batch_size))\n",
    "    \n",
    "    for idx in range(batch_size):\n",
    "        plt.subplot(batch_size, 2, 2*idx + 1)\n",
    "        show_image(images[idx], mean, std, title=f\"Image {idx}\")\n",
    "        \n",
    "        plt.subplot(batch_size, 2, 2*idx + 2)\n",
    "        show_image(labels[idx], mean, std, title=f\"Label {idx}\")  # Assuming labels are already normalized\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a batch from the train_loader\n",
    "visualize_dataset(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e71283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision.models import vit_h_14  # Using a pre-trained ViT model\n",
    "\n",
    "# class ViTInpainting(nn.Module):\n",
    "#     def __init__(self, image_size=224, patch_size=16, num_classes=3, channels=3):\n",
    "#         super(ViTInpainting, self).__init__()\n",
    "        \n",
    "#         # Initialize the Vision Transformer\n",
    "#         self.vit = vit_b_16(pretrained=True)\n",
    "        \n",
    "#         # Decoder to reconstruct the image\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(self.vit.head.in_features, image_size * image_size * channels),\n",
    "#             nn.Unflatten(1, (channels, image_size, image_size)),\n",
    "#             nn.ConvTranspose2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.Sigmoid()  # Use sigmoid to ensure output pixel values are between 0 and 1\n",
    "#         )\n",
    "        \n",
    "#         # Replace the classification head of the ViT with a dummy head\n",
    "#         self.vit.heads = nn.Identity()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Pass input through ViT\n",
    "#         features = self.vit(x)\n",
    "        \n",
    "#         # Decode features to reconstruct the image\n",
    "#         reconstructed_img = self.decoder(features)\n",
    "        \n",
    "#         return reconstructed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_h_14, ViT_H_14_Weights\n",
    "\n",
    "vit = weights = ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1\n",
    "vit = vit_h_14(weights=weights)\n",
    "\n",
    "print(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_h_14, ViT_H_14_Weights\n",
    "\n",
    "class ViTInpainting(nn.Module):\n",
    "    def __init__(self, image_size=224, channels=3):\n",
    "        super(ViTInpainting, self).__init__()\n",
    "\n",
    "        # Load the specific pre-trained ViT model\n",
    "        weights = ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1\n",
    "        self.vit = vit_h_14(weights=weights)\n",
    "        \n",
    "        # Assuming image_size and channels are adjusted based on the pre-trained model's input requirements\n",
    "        # Ensure the linear layer matches the feature size of the new ViT model\n",
    "        num_features = self.vit.heads.head.in_features  # Adjust this based on the actual model architecture\n",
    "\n",
    "        # Decoder to reconstruct the image\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1000, image_size * image_size * channels),\n",
    "            nn.Unflatten(1, (channels, image_size, image_size)),\n",
    "            nn.ConvTranspose2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # Ensure output pixel values are between 0 and 1\n",
    "        )\n",
    "        \n",
    "        # Replace the classification head with a dummy, if needed\n",
    "        # Check the specific model structure for how to best do this\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through ViT\n",
    "        features = self.vit(x)\n",
    "        \n",
    "        # Decode features to reconstruct the image\n",
    "        reconstructed_img = self.decoder(features)\n",
    "        \n",
    "        return reconstructed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec3267",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = nn.L1Loss()\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.resnet50 = resnet50(pretrained=True).eval()\n",
    "        # Remove the fully connected layer to get feature representations\n",
    "        self.resnet50 = nn.Sequential(*list(self.resnet50.children())[:-2])\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # Ensure input tensors are detached and do not require gradients\n",
    "        output_features = self.resnet50(output.detach())\n",
    "        target_features = self.resnet50(target.detach())\n",
    "        loss = F.l1_loss(output_features, target_features)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTInpainting().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "reconstruction_criterion = nn.L1Loss()\n",
    "perceptual_criterion = PerceptualLoss().to(device)\n",
    "\n",
    "num_epochs = 2\n",
    "scaler = GradScaler()\n",
    "accumulation_steps = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):  # Assuming 'dataloader' is defined\n",
    "        inputs, targets = data  # 'inputs' are occluded images, 'targets' are full images\n",
    "        inputs, targets = inputs.to(device), targets.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            recon_loss = reconstruction_criterion(outputs, targets)\n",
    "            perceptual_loss = perceptual_criterion(outputs, targets)\n",
    "            loss = (recon_loss + perceptual_loss) / accumulation_steps\n",
    "        # Backward pass with scaled loss\n",
    "        scaler.scale(loss).backward()  \n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:  # Perform the optimization step every `accumulation_steps`\n",
    "            scaler.step(optimizer)  # Update weights\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "       \n",
    "    # Checkpoint saving\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint_path = f'checkpoint_epoch_{epoch+1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f'Saved checkpoint to {checkpoint_path}')\n",
    "        \n",
    "# Save the final model\n",
    "final_model_path = 'final_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f'Saved final model to {final_model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec151a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

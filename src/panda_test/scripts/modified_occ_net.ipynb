{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved\n",
    "\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "# root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "root_dir = parent_path + \"occ_sim_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a6d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast\n",
    "        ], p=1)\n",
    "#         A.Resize(640, 480)  # Resize all images to be 640x480\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(0.95, 0.025, 0.025), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.imgs_files[idx]\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the robot\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_joint')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "            bboxes_labels_original.append('joint6')  \n",
    "\n",
    "        if self.transform:   \n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            \n",
    "            # Apply augmentations\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            # Unflattening list transformed['keypoints']\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "\n",
    "            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "#                 print(\"object\", obj)\n",
    "#                 print(\" obj index\", o_idx)# Iterating over objects\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        \n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n",
    "        \n",
    "        # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]            \n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original, img_file\n",
    "        else:\n",
    "            return img, target, img_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebe0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch_geometric.nn as pyg\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "_EPS = 1e-10\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Two-layer fully-connected ELU net with batch norm.\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def batch_norm(self, inputs):\n",
    "        x = inputs.view(inputs.size(0) * inputs.size(1), -1)\n",
    "        x = self.bn(x)\n",
    "        return x.view(inputs.size(0), inputs.size(1), -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "#         print(\"Input shape before any operations: \", inputs.shape)\n",
    "\n",
    "        # Flatten the last two dimensions for the linear layer input\n",
    "#         x = inputs.view(inputs.size(0), -1)\n",
    "        x = func.elu(self.fc1(inputs))\n",
    "        print(\"shape of x before do\", x.shape)\n",
    "        x = func.dropout(x, self.dropout_prob, training=self.training)\n",
    "        x = func.elu(self.fc2(x))\n",
    "        print(\"batch norm input\", x.shape)\n",
    "        return self.batch_norm(x)\n",
    "\n",
    "        # Assuming you want to maintain the second dimension for some reason\n",
    "        # (like temporal sequence in a RNN), you would reshape the output\n",
    "        # back to the desired shape. If not, this step is unnecessary.\n",
    "        # output = x.view(inputs.size(0), inputs.size(1), -1)\n",
    "        # print(\"Output shape after forward pass: \", output.shape)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_out=4, do_prob=0., factor=True):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "\n",
    "        self.factor = factor\n",
    "\n",
    "        self.mlp1 = MLP(n_in, n_hid, n_hid, do_prob)\n",
    "        self.mlp2 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "        self.mlp3 = MLP(n_hid, n_hid, n_hid, do_prob)\n",
    "        if self.factor:\n",
    "            self.mlp4 = MLP(n_hid * 3, n_hid, n_hid, do_prob)\n",
    "            print(\"Using factor graph MLP encoder.\")\n",
    "        else:\n",
    "            self.mlp4 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "#             print(\"mlp4\", self.mlp4)\n",
    "            print(\"Using MLP graph encoder.\")\n",
    "        self.fc_out = nn.Linear(n_hid, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def edge2node(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        incoming = torch.matmul(rel_rec.t(), x)\n",
    "#         print(\"Nodes in edge2node: \", incoming)\n",
    "#         print(\"final incoming: \", incoming / incoming.size(1))\n",
    "        return incoming / incoming.size(1)\n",
    "\n",
    "    def node2edge(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        receivers = torch.matmul(rel_rec, x)\n",
    "        senders = torch.matmul(rel_send, x)\n",
    "        edges = torch.cat([receivers, senders], dim=2)\n",
    "#         print(\"Edges in node2edge: \", edges)\n",
    "        return edges\n",
    "\n",
    "    def forward(self, inputs, rel_rec, rel_send):\n",
    "        # Input shape: [num_sims, num_atoms, num_timesteps, num_dims]\n",
    "        x = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "#         print(\"x shape:\", x.shape)\n",
    "#         print(\"rel_rec shape:\", rel_rec.shape)\n",
    "#         print(\"rel_send shape:\", rel_send.shape)\n",
    "\n",
    "        # New shape: [num_sims, num_atoms, num_timesteps*num_dims]\n",
    "        x = self.mlp1(x)  # 2-layer ELU net per node\n",
    "\n",
    "        x = self.node2edge(x, rel_rec, rel_send)\n",
    "        x = self.mlp2(x)\n",
    "        x_skip = x    \n",
    "        \n",
    "        if self.factor:\n",
    "            x = self.edge2node(x, rel_rec, rel_send)\n",
    "            x = self.mlp3(x)\n",
    "            x = self.node2edge(x, rel_rec, rel_send)\n",
    "            x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "            x = self.mlp4(x)\n",
    "        else:\n",
    "            x = self.mlp3(x)\n",
    "            x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "            x = self.mlp4(x)\n",
    "            \n",
    "#         print(\"output of graph encoder: \", self.fc_out(x))\n",
    "\n",
    "        return self.fc_out(x)    \n",
    "    \n",
    "class GraphDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in_node, edge_types, msg_hid, msg_out, n_hid,\n",
    "                 do_prob=0., skip_first=False):\n",
    "        super(GraphDecoder, self).__init__()\n",
    "        self.msg_fc1 = nn.ModuleList(\n",
    "            [nn.Linear(2 * n_in_node, msg_hid) for _ in range(edge_types)])\n",
    "        self.msg_fc2 = nn.ModuleList(\n",
    "            [nn.Linear(msg_hid, msg_out) for _ in range(edge_types)])\n",
    "        self.msg_out_shape = msg_out\n",
    "        self.skip_first_edge_type = skip_first\n",
    "\n",
    "        self.out_fc1 = nn.Linear(n_in_node + msg_out, n_hid)\n",
    "        self.out_fc2 = nn.Linear(n_hid, n_hid)\n",
    "        self.out_fc3 = nn.Linear(n_hid, n_in_node)\n",
    "\n",
    "        print('Using learned graph decoder.')\n",
    "\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "    def single_step_forward(self, single_timestep_inputs, rel_rec, rel_send,\n",
    "                            single_timestep_rel_type):\n",
    "\n",
    "        # single_timestep_inputs has shape\n",
    "        # [batch_size, num_timesteps, num_atoms, num_dims]\n",
    "\n",
    "        # single_timestep_rel_type has shape:\n",
    "        # [batch_size, num_timesteps, num_atoms*(num_atoms-1), num_edge_types]\n",
    "\n",
    "        # Node2edge\n",
    "        receivers = torch.matmul(rel_rec, single_timestep_inputs)\n",
    "        senders = torch.matmul(rel_send, single_timestep_inputs)\n",
    "        pre_msg = torch.cat([receivers, senders], dim=-1)\n",
    "\n",
    "        all_msgs = Variable(torch.zeros(pre_msg.size(0), pre_msg.size(1),self.msg_out_shape))\n",
    "        if single_timestep_inputs.is_cuda:\n",
    "            all_msgs = all_msgs.cuda()\n",
    "\n",
    "        if self.skip_first_edge_type:\n",
    "            start_idx = 1\n",
    "        else:\n",
    "            start_idx = 0\n",
    "\n",
    "        # Run separate MLP for every edge type\n",
    "        # NOTE: To exlude one edge type, simply offset range by 1\n",
    "        for i in range(start_idx, len(self.msg_fc2)):\n",
    "            msg = func.relu(self.msg_fc1[i](pre_msg))\n",
    "            msg = func.dropout(msg, p=self.dropout_prob)\n",
    "            msg = func.relu(self.msg_fc2[i](msg))\n",
    "            msg = msg * single_timestep_rel_type[:, :, i:i + 1]\n",
    "            all_msgs += msg\n",
    "\n",
    "        # Aggregate all msgs to receiver\n",
    "        agg_msgs = all_msgs.transpose(-2, -1).matmul(rel_rec).transpose(-2, -1)\n",
    "        agg_msgs = agg_msgs.contiguous()\n",
    "\n",
    "        # Skip connection\n",
    "        aug_inputs = torch.cat([single_timestep_inputs, agg_msgs], dim=-1)\n",
    "\n",
    "        # Output MLP\n",
    "        pred = func.dropout(func.relu(self.out_fc1(aug_inputs)), p=self.dropout_prob)\n",
    "        pred = func.dropout(func.relu(self.out_fc2(pred)), p=self.dropout_prob)\n",
    "        pred = self.out_fc3(pred)\n",
    "#        print(pred.shape,single_timestep_inputs.shape)\n",
    "#         print(\"output for single time steps fwd: \", single_timestep_inputs, pred, single_timestep_inputs + pred)\n",
    "        # Predict position/velocity difference\n",
    "        return single_timestep_inputs + pred\n",
    "\n",
    "    def forward(self, inputs, rel_type, rel_rec, rel_send, pred_steps=4):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "\n",
    "\n",
    "        # Only take n-th timesteps as starting points (n: pred_steps)\n",
    "        last_pred = inputs[:, :, :]\n",
    "        #asa\n",
    "        curr_rel_type = rel_type[:, :, :]\n",
    "        preds=[]\n",
    "        #print(curr_rel_type.shape)\n",
    "        # NOTE: Assumes rel_type is constant (i.e. same across all time steps).\n",
    "\n",
    "        # Run n prediction steps\n",
    "        for step in range(0, pred_steps):\n",
    "            last_pred = self.single_step_forward(last_pred, rel_rec, rel_send,\n",
    "                                                 curr_rel_type)\n",
    "            preds.append(last_pred)\n",
    "\n",
    "        sizes = [preds[0].size(0), preds[0].size(1),\n",
    "                 preds[0].size(2)]\n",
    "\n",
    "        output = Variable(torch.zeros(sizes))\n",
    "        if inputs.is_cuda:\n",
    "            output = output.cuda()\n",
    "\n",
    "        # Re-assemble correct timeline\n",
    "        for i in range(len(preds)):\n",
    "            output[:, :, :] = preds[i]\n",
    "\n",
    "        pred_all = output[:, :, :]\n",
    "\n",
    "        # NOTE: We potentially over-predicted (stored in future_pred). Unused.\n",
    "        # future_pred = output[:, (inputs.size(1) - 1):, :, :]\n",
    "#         print(\"output for pred_all\", pred_all)\n",
    "        return pred_all#.transpose(1, 2).contiguous()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6916a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(input, axis=1):\n",
    "    trans_input = input.transpose(axis, 0).contiguous()\n",
    "    soft_max_1d = func.softmax(trans_input,dim=0)\n",
    "    return soft_max_1d.transpose(axis, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ffa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointPipeline(nn.Module):\n",
    "    def __init__(self, weights_path):\n",
    "        super(KeypointPipeline, self).__init__()  \n",
    "        self.keypoint_model = torch.load(weights_path).to(device)\n",
    "        self.encoder = GraphEncoder(4,512,4,0.5,True)\n",
    "        self.decoder = GraphDecoder(n_in_node=4,\n",
    "                                 edge_types=2,\n",
    "                                 msg_hid=512,\n",
    "                                 msg_out=512,\n",
    "                                 n_hid=512,\n",
    "                                 do_prob=0.5,\n",
    "                                 skip_first=False)\n",
    "        \n",
    "#         self.off_diag = np.ones([6,6]) - np.eye(6)\n",
    "\n",
    "#         self.rel_rec = np.array(encode_onehot(np.where(self.off_diag)[1]), dtype=np.float32)\n",
    "#         self.rel_send = np.array(encode_onehot(np.where(self.off_diag)[0]), dtype=np.float32)\n",
    "#         self.rel_rec = torch.FloatTensor(self.rel_rec)\n",
    "#         self.rel_send = torch.FloatTensor(self.rel_send)\n",
    "\n",
    "        num_nodes = 6\n",
    "        self.off_diag = np.zeros([num_nodes, num_nodes])        \n",
    "#         # Creating a cycle: 1->2, 2->3, ..., 6->1\n",
    "#         for i in range(num_nodes):\n",
    "#             self.off_diag[i, (i + 1) % num_nodes] = 1\n",
    "\n",
    "        # Creating a bidirectional cycle\n",
    "#         for i in range(num_nodes):\n",
    "#             # Forward connection: i -> (i + 1) % num_nodes\n",
    "#             self.off_diag[i, (i + 1) % num_nodes] = 1\n",
    "\n",
    "#             # Backward connection: i -> (i - 1 + num_nodes) % num_nodes\n",
    "#             # The addition of num_nodes before modulo ensures a positive index\n",
    "#             self.off_diag[i, (i - 1 + num_nodes) % num_nodes] = 1\n",
    "\n",
    "        # Creating a bidirectional, non-cyclic graph\n",
    "        for i in range(num_nodes):\n",
    "            # Forward connection: i -> (i + 1), except for the last node\n",
    "            if i < num_nodes - 1:  # This prevents connecting the last node to the first\n",
    "                self.off_diag[i, i + 1] = 1\n",
    "\n",
    "            # Backward connection: i -> (i - 1), except for the first node\n",
    "            if i > 0:  # This prevents connecting the first node to the last\n",
    "                self.off_diag[i, i - 1] = 1\n",
    "\n",
    "        # Update rel_rec and rel_send based on the new off_diag\n",
    "        self.rel_rec = np.array(encode_onehot(np.where(self.off_diag)[1]), dtype=np.float32)\n",
    "        self.rel_send = np.array(encode_onehot(np.where(self.off_diag)[0]), dtype=np.float32)\n",
    "        self.rel_rec = torch.FloatTensor(self.rel_rec).to(device)\n",
    "        self.rel_send = torch.FloatTensor(self.rel_send).to(device)\n",
    "\n",
    "        self.encoder= self.encoder.cuda()\n",
    "        self.decoder = self.decoder.cuda()\n",
    "        self.rel_rec = self.rel_rec.cuda()\n",
    "        self.rel_send = self.rel_send.cuda()\n",
    "    \n",
    "    def process_model_output(self, output):\n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist()\n",
    "\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], \n",
    "                                            output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()\n",
    "\n",
    "        confidence = output[0]['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        labels = output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "        keypoints = []\n",
    "        for idx, kps in enumerate(output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "            keypoints.append(list(map(int, kps[0,0:2])) + [confidence[idx]] + [labels[idx]])\n",
    "        \n",
    "        # Sort keypoints based on label\n",
    "        keypoints.sort(key=lambda x: x[-1])\n",
    "        print(\"Keypoints: \", keypoints)\n",
    "        return keypoints\n",
    "    \n",
    "    def keypoints_to_graph(self, keypoints, image_width, image_height):\n",
    "        # keypoints is expected to be a tensor with shape (num_keypoints, 4),\n",
    "        # where each keypoint is (x, y, score, label).\n",
    "        # Convert all elements in keypoints to tensors if they are not already\n",
    "        keypoints = [torch.tensor(kp, dtype=torch.float32).to(device) if not isinstance(kp, torch.Tensor) else kp for kp in keypoints]\n",
    "\n",
    "        # Then stack them\n",
    "        keypoints = torch.stack(keypoints).to(device)        \n",
    "        \n",
    "        # Remove duplicates: Only keep the keypoint with the highest score for each label\n",
    "        unique_labels, best_keypoint_indices = torch.unique(keypoints[:, 3], return_inverse=True)\n",
    "        best_scores, best_indices = torch.max(keypoints[:, 2].unsqueeze(0) * (best_keypoint_indices == torch.arange(len(unique_labels)).unsqueeze(1).cuda()), dim=1)\n",
    "        keypoints = keypoints[best_indices]\n",
    "        \n",
    "#         print(\"init keypoints in graph features\", keypoints)\n",
    "\n",
    "        # Normalize x and y to be in the range [-1, 1]\n",
    "        keypoints[:, 0] = (keypoints[:, 0] - image_width / 2) / (image_width / 2)\n",
    "        keypoints[:, 1] = (keypoints[:, 1] - image_height / 2) / (image_height / 2)\n",
    "\n",
    "        # Use only x, y, and score for the graph features\n",
    "        graph_features = keypoints[:, :4]  # Now shape is (num_keypoints, 3)\n",
    "        \n",
    "        # Ensure the shape is [num_keypoints, 3] before returning\n",
    "        graph_features = graph_features.view(-1, 4)  # Reshape to ensure it's [num_keypoints, 3]\n",
    "#         print(\"graph features\", graph_features)\n",
    "#         print(\"graph features shape\", graph_features.shape)\n",
    "        print(\"graph feature: \", graph_features)\n",
    "\n",
    "        return graph_features\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        # Temporarily set the keypoint model to evaluation mode\n",
    "        keypoint_model_training = self.keypoint_model.training\n",
    "        self.keypoint_model.eval()\n",
    "\n",
    "        # Process each image in the batch\n",
    "        with torch.no_grad():\n",
    "            batch_outputs = [self.keypoint_model(img.unsqueeze(0).to(device)) for img in imgs]\n",
    "\n",
    "        # Set the keypoint model back to its original training mode\n",
    "        self.keypoint_model.train(mode=keypoint_model_training)\n",
    "\n",
    "        # Process model outputs to get labeled keypoints\n",
    "        batch_labeled_keypoints = [self.process_model_output(output) for output in batch_outputs]\n",
    "        # Generate graph input tensor for each image and handle varying number of keypoints\n",
    "        batch_x = []\n",
    "        for labeled_keypoints in batch_labeled_keypoints:\n",
    "            keypoints = self.keypoints_to_graph(labeled_keypoints, 640, 480)\n",
    "\n",
    "            # Initialize x with zeros for 6 nodes with 4 features each\n",
    "            x = torch.zeros(1, 6, 4, device=device)\n",
    "\n",
    "            # Ensure that keypoints are on the correct device and fill in x\n",
    "            num_keypoints_detected = keypoints.size(0)\n",
    "            if num_keypoints_detected <= 6:\n",
    "                x[0, :num_keypoints_detected, :] = keypoints\n",
    "            else:\n",
    "                raise ValueError(\"Number of keypoints detected exceeds the maximum of 6.\")\n",
    "\n",
    "            batch_x.append(x)\n",
    "\n",
    "        # Stack the batch of x tensors for batch processing\n",
    "        batch_x = torch.cat(batch_x, dim=0)\n",
    "\n",
    "        # Forward pass through the encoder and decoder\n",
    "        logits = self.encoder(batch_x, self.rel_rec, self.rel_send)\n",
    "        edges = my_softmax(logits, -1)\n",
    "        KGNN2D = self.decoder(batch_x, edges, self.rel_rec, self.rel_send)\n",
    "\n",
    "        return logits, KGNN2D, batch_labeled_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_edges(valid_points, edges):\n",
    "#     num_nodes = 6\n",
    "    print(\"valid_points: \", valid_points)\n",
    "    batch_size, num_nodes = valid_points.shape\n",
    "    print(batch_size, num_nodes)\n",
    "    off_diag = np.zeros([num_nodes, num_nodes])    \n",
    "    print(\"valid_points shape: \", valid_points.shape)\n",
    "    print(\"off_diag shape: \", off_diag.shape)\n",
    "\n",
    "    # Creating a bidirectional, cyclic graph\n",
    "#     for i in range(num_nodes):\n",
    "#         next_node = (i + 1) % num_nodes  # Ensures cyclic behavior\n",
    "#         off_diag[i, next_node] = 1  # Connect node i to node i+1 (or to the first node if i is the last node)\n",
    "#         off_diag[next_node, i] = 1  # Connect node i+1 (or the first node if i is the last node) back to node i\n",
    "\n",
    "#     Creating a bidirectional, non-cyclic graph\n",
    "    for i in range(num_nodes):\n",
    "        # Forward connection: i -> (i + 1), except for the last node\n",
    "        if i < num_nodes - 1:  # This prevents connecting the last node to the first\n",
    "            off_diag[i, i + 1] = 1\n",
    "\n",
    "        # Backward connection: i -> (i - 1), except for the first node\n",
    "        if i > 0:  # This prevents connecting the first node to the last\n",
    "            off_diag[i, i - 1] = 1   \n",
    "        \n",
    "#     print(\"off_diag original: \", off_diag)\n",
    "        \n",
    "    # Convert off_diag to tensor for indexing\n",
    "    off_diag = torch.tensor(off_diag, dtype=torch.bool, device='cuda')\n",
    "    \n",
    "#     print(\"off_diag: \", off_diag.shape, off_diag)\n",
    "    # Extracting indices where there is a relationship\n",
    "    idx = torch.where(off_diag)[1].reshape((num_nodes-1), -1).to(device='cuda', dtype=torch.long)\n",
    "#     print(torch.where(off_diag)[1])\n",
    "#     print(\"idx: \", idx)# Adjust relations calculation to accommodate batches and node relationships\n",
    "    relations = torch.zeros((batch_size, (num_nodes-1) * 2), device='cuda')\n",
    "#     print(\"relations: \", relations)\n",
    "    \n",
    "    for count, vis in enumerate(valid_points):\n",
    "        vis = vis.view(-1, 1).float()\n",
    "#         print(\"vis: \", vis)\n",
    "        vis_tran_mat = vis*vis.t() # Matrix multiplication to get visibility matrix\n",
    "#         print(\"vis after vis*vis.t(): \", vis_tran_mat)\n",
    "        vis_selected = torch.gather(vis_tran_mat, 1, idx)  # Gather visible relations based on the bidirectional cyclic graph\n",
    "#         print(\"vis after gather: \", vis_selected)\n",
    "#         print(\"vis with flatten: \", vis_selected.view(-1))\n",
    "        relations[count] = vis_selected.view(-1)  # Flatten and assign\n",
    "#     print(\"relations\", relations.shape)\n",
    "    relations = relations.to(torch.long)  # Ensure correct dtype for loss calculation\n",
    "    # Calculate and return cross-entropy loss\n",
    "#     print(\"edges shape: \", edges.shape, edges)\n",
    "    print(\"relations reshaped\", relations.view(-1).shape)\n",
    "    print(\"edges reshaped\", edges.view(-1,4).shape)\n",
    "    loss_edges = func.cross_entropy(edges.view(-1, 4), relations.view(-1))\n",
    "    return loss_edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nll_gaussian(preds, target, variance, add_const=False):\n",
    "#     neg_log_p = ((preds - target) ** 2 / (2 * variance))\n",
    "#     if add_const:\n",
    "#         const = 0.5 * np.log(2 * np.pi * variance)\n",
    "#         neg_log_p += const\n",
    "#     return neg_log_p.sum() / (target.size(0) * target.size(1))\n",
    "\n",
    "# def kgnn2d_loss(keypoints_gt, valid_points, keypoints_logits):\n",
    "#     # Ensure data types are consistent and move tensors to the appropriate device\n",
    "#     keypoints_gt = keypoints_gt.type(torch.FloatTensor).cuda()\n",
    "#     keypoints_logits = keypoints_logits.type(torch.FloatTensor).cuda()\n",
    "#     valid_points = valid_points.type(torch.FloatTensor).cuda()\n",
    "\n",
    "#     # Print shapes for debugging\n",
    "# #     print(f\"keypoints_gt.shape: {keypoints_gt.shape}\")\n",
    "# #     print(f\"keypoints_logits.shape: {keypoints_logits.shape}\")\n",
    "# #     print(f\"valid_points.shape: {valid_points.shape}\")\n",
    "#     keypoints_gt = keypoints_gt.type(torch.FloatTensor)*valid_points.unsqueeze(2).type(torch.FloatTensor)\n",
    "#     keypoints_logits = keypoints_logits.type(torch.FloatTensor)*valid_points.unsqueeze(2).type(torch.FloatTensor)\n",
    "#     keypoints_gt = keypoints_gt.cuda()\n",
    "#     keypoints_logits = keypoints_logits.cuda()\n",
    "#     loss_occ = nll_gaussian(keypoints_gt[:,:,0:2], keypoints_logits[:,:,0:2] , 0.1)\n",
    "#     return loss_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgnn2d_loss(gt_keypoints, pred_keypoints):\n",
    "    loss = func.mse_loss(pred_keypoints, gt_keypoints)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "# def process_keypoints(keypoints):\n",
    "#     # Assuming keypoints is a list of Nx3 tensors where N is the number of keypoints\n",
    "#     # and each keypoint is represented as [x, y, visibility]\n",
    "#     # Remove the unnecessary middle dimension\n",
    "#     keypoints = [kp.squeeze(1) for kp in keypoints]\n",
    "#     visibilities = [kp[:, 2] for kp in keypoints]  # Extract visibility flags\n",
    "#     valid_vis_all = torch.cat([v == 1 for v in visibilities]).long().cuda()\n",
    "#     valid_invis_all = torch.cat([v == 0 for v in visibilities]).long().cuda()\n",
    "\n",
    "#     keypoints_gt = torch.cat([kp[:, :2] for kp in keypoints]).float().cuda()  # Gather all keypoints and discard visibility flags\n",
    "#     keypoints_gt = keypoints_gt.view(-1, 2).unsqueeze(0)  # Add an extra dimension to match expected shape for loss_edges\n",
    "\n",
    "#     return keypoints_gt, valid_vis_all, valid_invis_all\n",
    "\n",
    "# def process_batch_keypoints(batch_keypoints):\n",
    "#     # Assuming batch_keypoints is a batch of keypoints tensors\n",
    "#     # Each tensor in the batch has shape [N, 3] where N is the number of keypoints\n",
    "#     # and each keypoint is represented as [x, y, visibility]\n",
    "\n",
    "#     # Concatenate all keypoints and visibilities from the batch\n",
    "#     all_keypoints = torch.cat([kp for kp in batch_keypoints])\n",
    "#     visibilities = all_keypoints[:, 2]  # Extract visibility flags\n",
    "\n",
    "#     valid_vis_all = (visibilities == 1).long().cuda()\n",
    "#     valid_invis_all = (visibilities == 0).long().cuda()\n",
    "\n",
    "#     keypoints_gt = all_keypoints[:, :2].float().cuda()  # Discard visibility flags\n",
    "#     keypoints_gt = keypoints_gt.view(-1, 2)  # Reshape for consistency\n",
    "\n",
    "#     return keypoints_gt, valid_vis_all, valid_invis_all\n",
    "\n",
    "def process_batch_keypoints(target_dicts):\n",
    "    # This function now expects target_dicts, a list of dictionaries containing keypoints information\n",
    "    batch_size = len(target_dicts)\n",
    "#     print(batch_size)\n",
    "\n",
    "    # Initialize lists to store keypoints and visibilities for each image in the batch\n",
    "    keypoints_list = []\n",
    "    visibilities_list = []\n",
    "\n",
    "    for dict_ in target_dicts:\n",
    "        # Each keypoints tensor in the dict is expected to have a shape [num_keypoints, 3]\n",
    "        keypoints = dict_['keypoints'].squeeze(1).to(device)\n",
    "        print(f\"Original shape of keypoints in dict: {keypoints.shape}\")\n",
    "\n",
    "        # Extract x, y coordinates and visibility flags\n",
    "        xy_coords = keypoints[:, :2]  # Keep only x, y coordinates\n",
    "        visibilities = keypoints[:, 2]  # Extract visibility flags\n",
    "\n",
    "        keypoints_list.append(xy_coords)\n",
    "        visibilities_list.append(visibilities)\n",
    "\n",
    "    # Concatenate keypoints and visibilities for the entire batch\n",
    "    # The final shape of keypoints_gt should be [batch_size, num_keypoints, 2]\n",
    "    keypoints_gt = torch.stack(keypoints_list).float().cuda()\n",
    "    visibilities = torch.stack(visibilities_list).cuda()\n",
    "\n",
    "    # Create valid visibility masks\n",
    "    valid_vis_all = (visibilities == 1).long().cuda()\n",
    "    valid_invis_all = (visibilities == 0).long().cuda()\n",
    "    return keypoints_gt, valid_vis_all, valid_invis_all\n",
    "\n",
    "def reorder_batch_keypoints(batch_keypoints):\n",
    "    # Assuming batch_keypoints is a tensor of shape [batch_size, num_keypoints, num_features]\n",
    "    batch_size, num_keypoints, num_features = batch_keypoints.shape\n",
    "    reordered_keypoints_batch = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Directly use the normalized keypoints\n",
    "        normalized_keypoints = batch_keypoints[i]\n",
    "\n",
    "        # Initialize a tensor for reordered keypoints with only x, y coordinates\n",
    "        reordered_normalized_keypoints = torch.zeros(num_keypoints, 2, device=batch_keypoints.device)\n",
    "\n",
    "        # Reordering logic\n",
    "        rounded_labels = torch.round(normalized_keypoints[:, -1]).int()\n",
    "        used_indices = []\n",
    "        for label in range(1, 7):\n",
    "            valid_idx = (rounded_labels == label).nonzero(as_tuple=True)[0]\n",
    "            if valid_idx.numel() > 0:\n",
    "                reordered_normalized_keypoints[label - 1] = normalized_keypoints[valid_idx[0], :2]\n",
    "            else:\n",
    "                invalid_idx = ((rounded_labels < 1) | (rounded_labels > 6)).nonzero(as_tuple=True)[0]\n",
    "                invalid_idx = [idx for idx in invalid_idx if idx not in used_indices]\n",
    "                if invalid_idx:\n",
    "                    reordered_normalized_keypoints[label - 1] = normalized_keypoints[invalid_idx[0], :2]\n",
    "                    used_indices.append(invalid_idx[0])\n",
    "\n",
    "        reordered_keypoints_batch.append(reordered_normalized_keypoints)\n",
    "\n",
    "    return torch.stack(reordered_keypoints_batch)\n",
    "\n",
    "def denormalize_keypoints(batch_keypoints, width=640, height=480):\n",
    "    # Assuming batch_keypoints is a batch of normalized keypoints tensors\n",
    "    # Denormalize each keypoint in the batch\n",
    "    denormalized_keypoints = []\n",
    "    for kp in batch_keypoints:\n",
    "        denormalized_x = (kp[:, 0] * (width / 2)) + (width / 2)\n",
    "        denormalized_y = (kp[:, 1] * (height / 2)) + (height / 2)\n",
    "        denormalized_kp = torch.stack((denormalized_x, denormalized_y), dim=1)\n",
    "        denormalized_keypoints.append(denormalized_kp)\n",
    "        \n",
    "    denormalized_keypoints = torch.stack(denormalized_keypoints)\n",
    "#     print(\"denormalized_keypoints.shape\", denormalized_keypoints.shape)\n",
    "    return denormalized_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e60a05b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = KeypointPipeline(weights_path)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1 # Define your number of epochs\n",
    "batch_size = 2\n",
    "\n",
    "split_folder_path = train_test_split(root_dir)\n",
    "KEYPOINTS_FOLDER_TRAIN = split_folder_path +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = split_folder_path +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = split_folder_path +\"/test\"\n",
    "\n",
    "dataset_train = KPDataset(KEYPOINTS_FOLDER_TRAIN, transform=None, demo=False)\n",
    "dataset_val = KPDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = KPDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "v = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, target_dicts, _ in data_loader_train:\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass for batch\n",
    "        logits, KGNN2D, batch_labeled_keypoints = model(imgs)\n",
    "#         print(KGNN2D.shape)\n",
    "        print(\"Normalized Prediction in training\", KGNN2D)\n",
    "\n",
    "        # Process keypoints for the entire batch\n",
    "        keypoints_gt, valid_vis_all, valid_invis_all = process_batch_keypoints(target_dicts)\n",
    "        print(\"gt keypoints for loss\", keypoints_gt)\n",
    "#         print(\"valid_invis_all.shape\", valid_invis_all.shape)\n",
    "        \n",
    "        # Normalize and reorder keypoints as per your existing logic\n",
    "        # Ensure this logic works on the batch level\n",
    "        \n",
    "        reordered_normalized_keypoints = reorder_batch_keypoints(KGNN2D)\n",
    "#         print(\"keypoints_logits.shape\", reordered_normalized_keypoints.shape)\n",
    "        # Denormalize the reordered keypoints for the entire batch\n",
    "        print(\"Normalized Reordered\", reordered_normalized_keypoints)\n",
    "        denormalized_keypoints = denormalize_keypoints(reordered_normalized_keypoints)\n",
    "        print(\"Denormalized Prediction in training\", denormalized_keypoints)\n",
    "        \n",
    "#         print(valid_vis_all.shape)\n",
    "#         print(logits.shape)\n",
    "        \n",
    "#         loss_kgnn2d = kgnn2d_loss(keypoints_gt, valid_invis_all, denormalized_keypoints)\n",
    "        loss_kgnn2d = kgnn2d_loss(keypoints_gt, denormalized_keypoints)\n",
    "\n",
    "        # Compute batch losses\n",
    "        edge_loss = loss_edges(valid_vis_all, logits)\n",
    "        \n",
    "#         loss_kgnn2d = kgnn2d_loss(keypoints_gt, denormalized_keypoints, valid_vis_all)\n",
    "\n",
    "        # Combine the losses\n",
    "        total_batch_loss = edge_loss + loss_kgnn2d\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += total_batch_loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Assuming KeypointPipeline and all necessary classes/functions are defined above or imported\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Path to the trained model weights\n",
    "model_path = '/home/jc-merlab/Pictures/Data/trained_models/occ_ckpt/ckpt_e200.pth'\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(model_path)\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image and prepare it for the model prediction.\n",
    "    This function should replicate the preprocessing applied during training.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Your preprocessing steps here, for example, resizing, normalization, etc.\n",
    "    # This is a placeholder transformation, adjust it according to your training transformations\n",
    "    img_tensor = F.to_tensor(img).to(device)  # Add batch dim and convert to tensor\n",
    "    return img_tensor\n",
    "\n",
    "def predict(model, img_tensor):\n",
    "    # Assuming the model takes a list of tensors as input\n",
    "    with torch.no_grad():\n",
    "        logits, KGNN2D, batch_labeled_keypoints = model([img_tensor])\n",
    "    # Process the output as needed\n",
    "    return KGNN2D\n",
    "\n",
    "def postprocess_keypoints(keypoints, width=640, height=480):\n",
    "    # Adjust this function based on your model's output format\n",
    "    denormalized_keypoints = denormalize_keypoints(keypoints, width, height)\n",
    "    return denormalized_keypoints\n",
    "\n",
    "image_path = '/home/jc-merlab/Pictures/Data/source_folder_occlusion/001471.rgb.jpg'\n",
    "\n",
    "\n",
    "# Example usage\n",
    "img_tensor = prepare_image(image_path).to(device)\n",
    "KGNN2D = predict(model, img_tensor)\n",
    "ordered_keypoints = reorder_batch_keypoints(KGNN2D)\n",
    "denormalized_keypoints = postprocess_keypoints(ordered_keypoints)\n",
    "print(denormalized_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0470fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"keypoints\": [[[257.95220042652915, 366.9198630617724, 1]], [[257.95973939799904, 283.013113744617, 1]], \n",
    "              [[179.70016595863137, 298.244571585954, 1]], [[175.69899307811323, 277.8348649543144, 0]], \n",
    "              [[79.90486225732596, 303.90392338594796, 0]], \n",
    "              [[66.15504343164937, 289.4557892368882, 1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the model\n",
    "model_path = '/home/jc-merlab/Pictures/Data/trained_models/occ_ckpt/ckpt_e200.pth'\n",
    "model = torch.load(model_path)\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image and prepare it for the model prediction.\n",
    "    This function should replicate the preprocessing applied during training.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Your preprocessing steps here, for example, resizing, normalization, etc.\n",
    "    # This is a placeholder transformation, adjust it according to your training transformations\n",
    "    img_tensor = F.to_tensor(img).to(device)  # Add batch dim and convert to tensor\n",
    "    return img_tensor\n",
    "\n",
    "\n",
    "def predict(model, img_tensor):\n",
    "    # Assuming the model takes a list of tensors as input\n",
    "    with torch.no_grad():\n",
    "        logits, KGNN2D, batch_labeled_keypoints = model([img_tensor])\n",
    "    # Process the output as needed\n",
    "    return KGNN2D\n",
    "\n",
    "def postprocess_keypoints(keypoints, width=640, height=480):\n",
    "    # Adjust this function based on your model's output format\n",
    "    denormalized_keypoints = denormalize_keypoints(keypoints, width, height)\n",
    "    return denormalized_keypoints\n",
    "\n",
    "def visualize_keypoints(image_path, keypoints, out_dir):\n",
    "    \"\"\"\n",
    "    Visualize keypoints on the image using cv2.\n",
    "    \"\"\"\n",
    "    # Load the original image\n",
    "    img = cv2.imread(image_path)\n",
    "    # Convert keypoints to a NumPy array if it's a tensor\n",
    "    if torch.is_tensor(keypoints):\n",
    "        keypoints = keypoints.cpu().numpy()\n",
    "    \n",
    "    # Draw keypoints on the image\n",
    "    for kp in keypoints[0]:  # Assuming the keypoints shape is [1, N, 2]\n",
    "        x, y = int(kp[0]), int(kp[1])\n",
    "        cv2.circle(img, (x, y), radius=5, color=(0, 255, 0), thickness=-1)\n",
    "    \n",
    "   # Construct the output path and save the image\n",
    "    filename = os.path.basename(image_path)\n",
    "    output_path = os.path.join(out_dir, filename)\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "def process_folder(folder_path, output_path):\n",
    "    \"\"\"\n",
    "    Process all images in the specified folder, predict and visualize keypoints.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            img_tensor = prepare_image(image_path).to(device)\n",
    "            KGNN2D = predict(model, img_tensor)\n",
    "            ordered_keypoints = reorder_batch_keypoints(KGNN2D)  # Ensure this function is defined\n",
    "            denormalized_keypoints = postprocess_keypoints(ordered_keypoints)\n",
    "            visualize_keypoints(image_path, denormalized_keypoints, output_path)\n",
    "\n",
    "# Example usage\n",
    "folder_path = '/home/jc-merlab/Pictures/Data/occ_test_data/'\n",
    "output_path = '/home/jc-merlab/Pictures/Data/occ_test_data/output/'\n",
    "process_folder(folder_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad751b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, DataLoader, Data\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from define_path import Def_Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import splitfolders\n",
    "import shutil\n",
    "import albumentations as A # Library for augmentations\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt \n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ea856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotArmDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, pre_transform=None):\n",
    "        super(RobotArmDataset, self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root_dir, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root_dir, \"annotations\")))\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.imgs_files)\n",
    "\n",
    "    def get(self, idx):\n",
    "        image_file = os.path.join(self.root_dir, \"images\", self.imgs_files[idx])\n",
    "        json_file = os.path.join(self.root_dir, \"annotations\", self.annotations_files[idx])\n",
    "\n",
    "        with open(json_file, 'r') as f:\n",
    "            data_json = json.load(f)\n",
    "        \n",
    "        keypoints = data_json['keypoints']\n",
    "        keypoints = [kp[0] for kp in keypoints]  # Extract keypoints from each list\n",
    "\n",
    "        keypoints = np.array(keypoints).reshape(-1, 3)  # Convert to numpy array and reshape\n",
    "\n",
    "        image = cv2.imread(image_file)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "\n",
    "#         data = {\"image\": image, \"keypoints\": keypoints.tolist()}\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(data)\n",
    "\n",
    "        keypoints = torch.tensor(keypoints, dtype=torch.float).view(-1, 3)  # Convert back to tensor and reshape\n",
    "        edge_index = torch.tensor([[i, i+1] for i in range(len(keypoints)-1)], dtype=torch.long).t().contiguous()\n",
    "        data = Data(x=keypoints, edge_index=edge_index, y=keypoints.clone())\n",
    "    \n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float()  # Convert to PyTorch tensor and rearrange dimensions to (C, H, W)\n",
    "        return image, data\n",
    "#         return transformed\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         image_file = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "#         json_file = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "    \n",
    "#         with open(json_file, 'r') as f:\n",
    "#             data_json = json.load(f)\n",
    "#         keypoints = data_json['keypoints']\n",
    "#         keypoints = np.array(keypoints).reshape(-1, 3)  # Convert to numpy array and reshape\n",
    "\n",
    "#         image = cv2.imread(image_file)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "\n",
    "#         if self.transform:\n",
    "#             # Transform expects keypoints in format [x, y, visibility]\n",
    "#             transformed = self.transform(image=image, keypoints=keypoints)\n",
    "#             image = transformed[\"image\"]\n",
    "#             keypoints = transformed[\"keypoints\"]\n",
    "\n",
    "#         keypoints = torch.tensor(keypoints, dtype=torch.float).view(-1, 3)  # Convert back to tensor and reshape\n",
    "#         edge_index = torch.tensor([[i, i+1] for i in range(len(keypoints)-1)], dtype=torch.long).t().contiguous()\n",
    "#         data = Data(x=keypoints, edge_index=edge_index)\n",
    "\n",
    "#         image = torch.from_numpy(image).permute(2, 0, 1).float()  # Convert to PyTorch tensor and rearrange dimensions to (C, H, W)\n",
    "#         return image, data\n",
    "    \n",
    "#     def len(self):\n",
    "#         return len(self.imgs_files)  # Return the number of data points\n",
    "\n",
    "#     def get(self, idx):\n",
    "#         return self.__getitem__(idx)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c89c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotateKeyPoints(object):\n",
    "    def __init__(self, angle):\n",
    "        self.angle = angle\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, keypoints = sample['image'], sample['keypoints']\n",
    "\n",
    "        # rotation of image\n",
    "        image = image.rotate(self.angle)\n",
    "\n",
    "        # rotation of keypoints\n",
    "        rotation_matrix = torch.tensor([\n",
    "            [torch.cos(self.angle), -torch.sin(self.angle)],\n",
    "            [torch.sin(self.angle),  torch.cos(self.angle)]\n",
    "        ])\n",
    "\n",
    "        keypoints[:, :2] = torch.mm(keypoints[:, :2], rotation_matrix)\n",
    "\n",
    "        return {'image': image, 'keypoints': keypoints}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return transforms.Compose([\n",
    "    transforms.Lambda(lambda x: Image.fromarray(x)),\n",
    "    transforms.Resize((640, 480)), \n",
    "    transforms.ToTensor(),\n",
    "    RotateKeyPoints(90),  # Rotate image and keypoints by 90 degrees\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e725dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    print(type(output))\n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(image, data, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    # Remove the channel dimension and convert the tensor back to numpy array\n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    ax.imshow(image_np.astype(int))\n",
    "\n",
    "    keypoints = data.x[:, :2]  # Assuming the first two dimensions are x and y\n",
    "    # Create a graph from the edge_index\n",
    "    G = nx.Graph()\n",
    "    for i in range(keypoints.shape[0]):\n",
    "        G.add_node(i, pos=(keypoints[i][0].item(), keypoints[i][1].item()))\n",
    "    for edge in data.edge_index.t():\n",
    "        G.add_edge(edge[0].item(), edge[1].item())\n",
    "    \n",
    "    # Draw the graph\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    nx.draw(G, pos, node_color='r', node_size=50, ax=ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" \n",
    "print(KEYPOINTS_FOLDER_TRAIN)\n",
    "dataset = RobotArmDataset(KEYPOINTS_FOLDER_TRAIN, transform=None,pre_transform=None)\n",
    "print(len(dataset))\n",
    "image, data = dataset[0]\n",
    "print(image.shape)# Get the first image and its associated data\n",
    "visualize_data(image, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ee939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RobotArmGCN(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(RobotArmGCN, self).__init__()\n",
    "#         self.conv1 = GCNConv(3, 16)\n",
    "#         self.conv2 = GCNConv(16, 3)\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = torch.nn.functional.relu(x)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08bf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50560c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "dataset_train = RobotArmDataset(KEYPOINTS_FOLDER_TRAIN, transform=None,pre_transform=None)\n",
    "dataset_val = RobotArmDataset(KEYPOINTS_FOLDER_VAL, transform=None)\n",
    "dataset_test = RobotArmDataset(KEYPOINTS_FOLDER_TEST, transform=None)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=32)\n",
    "\n",
    "model = RobotArmGCN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):  # Adjust number of epochs as necessary\n",
    "    for batch in dataloader_train:\n",
    "        image, data = batch\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = torch.nn.functional.mse_loss(out, data.y)  # Adjust loss function as necessary\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in your_test_data_loader:  # Replace with your DataLoader\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "# from albumentations import Resize, Compose\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load an image\n",
    "image_path = KEYPOINTS_FOLDER_TRAIN + '/images/000000.rgb.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# Define a transformation\n",
    "# Define a transformation\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224)\n",
    "])\n",
    "\n",
    "# Apply the transformation\n",
    "transformed = transform(image=image)\n",
    "transformed_image = transformed[\"image\"]\n",
    "\n",
    "# Display the transformed image\n",
    "plt.imshow(transformed_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e40e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
